{
  "url": "https://docs.crawl4ai.com/advanced/advanced-features",
  "timestamp": "2025-02-07T11:22:29.470619",
  "html": "<!DOCTYPE html><html lang=\"en\" style=\"scroll-padding-top: 50px;\"><head>\n    \n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n    <meta name=\"generator\" content=\"mkdocs-1.6.0, mkdocs-terminal-4.4.0\">\n    \n    <meta name=\"description\" content=\"🚀🤖 Crawl4AI, Open-source LLM-Friendly Web Crawler &amp; Scraper\"> \n     \n    \n    <link rel=\"canonical\" href=\"https://docs.crawl4ai.com/advanced/advanced-features/\"><link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"../../img/android-chrome-192x192.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"512x512\" href=\"../../img/android-chrome-512x512.png\">\n<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"../../img/apple-touch-icon.png\">\n<link rel=\"shortcut icon\" type=\"image/png\" sizes=\"48x48\" href=\"../../img/favicon.ico\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"../../img/favicon-16x16.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"../../img/favicon-32x32.png\">\n\n\n    \n \n<title>Overview - Crawl4AI Documentation (v0.4.3bx)</title>\n\n\n<link href=\"../../css/fontawesome/css/fontawesome.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/fontawesome/css/solid.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/normalize.css\" rel=\"stylesheet\">\n<link href=\"../../css/terminal.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.tile_grid.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.footer.css\" rel=\"stylesheet\">\n<!-- dark color palette -->\n<link href=\"../../css/palettes/dark.css\" rel=\"stylesheet\">\n\n<!-- page layout -->\n<style>\n/* initially set page layout to a one column grid */\n.terminal-mkdocs-main-grid {\n    display: grid;\n    grid-column-gap: 1.4em;\n    grid-template-columns: auto;\n    grid-template-rows: auto;\n}\n\n/*  \n*   when side navigation is not hidden, use a two column grid.  \n*   if the screen is too narrow, fall back to the initial one column grid layout.\n*   in this case the main content will be placed under the navigation panel. \n*/\n@media only screen and (min-width: 70em) {\n    .terminal-mkdocs-main-grid {\n        grid-template-columns: 4fr 9fr;\n    }\n}</style>\n\n\n\n    \n    <link href=\"../../assets/styles.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/highlight.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/dmvendor.css\" rel=\"stylesheet\">  \n    \n    \n\n    \n    <!-- search css support -->\n<link href=\"../../css/search/bootstrap-modal.css\" rel=\"stylesheet\">\n<!-- search scripts -->\n<script>\n    var base_url = \"../..\",\n    shortcuts = \"{}\";\n</script>\n<script src=\"../../js/jquery/jquery-1.10.1.min.js\" defer=\"\"></script>\n<script src=\"../../js/bootstrap/bootstrap.min.js\" defer=\"\"></script>\n<script src=\"../../js/mkdocs/base.js\" defer=\"\"></script>\n    \n    \n    \n    \n    <script src=\"../../assets/highlight.min.js\"></script>\n    \n    <script src=\"../../assets/highlight_init.js\"></script>\n    \n    <script src=\"https://buttons.github.io/buttons.js\"></script>\n    \n    <script src=\"../../search/main.js\"></script>\n    \n\n    \n</head>\n\n<body class=\"terminal\" style=\"\"><div class=\"container\">\n    <div class=\"terminal-nav\">\n        <header class=\"terminal-logo\">\n            <div id=\"mkdocs-terminal-site-name\" class=\"logo terminal-prompt\"><a href=\"https://docs.crawl4ai.com/\" class=\"no-style\">Crawl4AI Documentation (v0.4.3bx)</a></div>\n        </header>\n        \n        <nav class=\"terminal-menu\">\n            \n            <ul vocab=\"https://schema.org/\" typeof=\"BreadcrumbList\">\n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../..\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Home</span>\n                    </a>\n                    <meta property=\"position\" content=\"0\">\n                </li>\n                \n                \n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../../core/quickstart/\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Quick Start</span>\n                    </a>\n                    <meta property=\"position\" content=\"1\">\n                </li>\n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                    \n                    \n\n\n<li property=\"itemListElement\" typeof=\"ListItem\">\n    <a href=\"#\" class=\"menu-item\" data-toggle=\"modal\" data-target=\"#mkdocs_search_modal\" property=\"item\" typeof=\"SearchAction\">\n        <i aria-hidden=\"true\" class=\"fa fa-search\"></i> <span property=\"name\">Search</span>\n    </a>\n    <meta property=\"position\" content=\"2\">\n</li>\n                    \n            </ul>\n            \n        </nav>\n    </div>\n</div>\n        \n    <div class=\"container\">\n        <div class=\"terminal-mkdocs-main-grid\"><aside id=\"terminal-mkdocs-side-panel\"><nav>\n  \n    <ul class=\"terminal-mkdocs-side-nav-items\">\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../..\">Home</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Setup &amp; Installation</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/installation/\">Installation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/docker-deploymeny/\">Docker Deployment</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/quickstart/\">Quick Start</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Blog &amp; Changelog</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../blog/\">Blog Home</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md\">Changelog</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Core</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/simple-crawling/\">Simple Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/crawler-result/\">Crawler Result</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/browser-crawler-config/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/markdown-generation/\">Markdown Generation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/fit-markdown/\">Fit Markdown</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/page-interaction/\">Page Interaction</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/content-selection/\">Content Selection</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/cache-modes/\">Cache Modes</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/local-files/\">Local Files &amp; Raw HTML</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/link-media/\">Link &amp; Media</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item--active terminal-mkdocs-side-nav-section-no-index\">Advanced</span>\n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        <span class=\"\n\n    terminal-mkdocs-side-nav-item--active\">Overview</span>\n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../file-downloading/\">File Downloading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../lazy-loading/\">Lazy Loading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../hooks-auth/\">Hooks &amp; Auth</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../proxy-security/\">Proxy &amp; Security</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../session-management/\">Session Management</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../multi-url-crawling/\">Multi-URL Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../crawl-dispatcher/\">Crawl Dispatcher</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../identity-based-crawling/\">Identity Based Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../ssl-certificate/\">SSL Certificate</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Extraction</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/no-llm-strategies/\">LLM-Free Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/llm-strategies/\">LLM Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/clustring-strategies/\">Clustering Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/chunking/\">Chunking</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">API Reference</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/async-webcrawler/\">AsyncWebCrawler</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun/\">arun()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun_many/\">arun_many()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/parameters/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/crawl-result/\">CrawlResult</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/strategies/\">Strategies</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n    </ul>\n  \n</nav><hr>\n<nav>\n    <ul>\n        <li><a href=\"#overview-of-some-important-advanced-features\">Overview of Some Important Advanced Features</a></li>\n        <li><a href=\"#1-proxy-usage\">1. Proxy Usage</a></li><li><a href=\"#2-capturing-pdfs-screenshots\">2. Capturing PDFs &amp; Screenshots</a></li><li><a href=\"#3-handling-ssl-certificates\">3. Handling SSL Certificates</a></li><li><a href=\"#4-custom-headers\">4. Custom Headers</a></li><li><a href=\"#5-session-persistence-local-storage\">5. Session Persistence &amp; Local Storage</a></li><li><a href=\"#6-robotstxt-compliance\">6. Robots.txt Compliance</a></li><li><a href=\"#putting-it-all-together\">Putting It All Together</a></li><li><a href=\"#conclusion-next-steps\">Conclusion &amp; Next Steps</a></li>\n    </ul>\n</nav>\n</aside>\n            <main id=\"terminal-mkdocs-main-content\">\n<section id=\"mkdocs-terminal-content\">\n    <h1 id=\"overview-of-some-important-advanced-features\">Overview of Some Important Advanced Features</h1>\n<p>(Proxy, PDF, Screenshot, SSL, Headers, &amp; Storage State)</p>\n<p>Crawl4AI offers multiple power-user features that go beyond simple crawling. This tutorial covers:</p>\n<p>1. <strong>Proxy Usage</strong><br>\n2. <strong>Capturing PDFs &amp; Screenshots</strong><br>\n3. <strong>Handling SSL Certificates</strong><br>\n4. <strong>Custom Headers</strong><br>\n5. <strong>Session Persistence &amp; Local Storage</strong>\n6. <strong>Robots.txt Compliance</strong></p>\n<blockquote>\n<p><strong>Prerequisites</strong><br>\n- You have a basic grasp of <a href=\"../../core/simple-crawling/\">AsyncWebCrawler Basics</a><br>\n- You know how to run or configure your Python environment with Playwright installed</p>\n</blockquote>\n<hr>\n<h2 id=\"1-proxy-usage\">1. Proxy Usage</h2>\n<p>If you need to route your crawl traffic through a proxy—whether for IP rotation, geo-testing, or privacy—Crawl4AI supports it via <code>BrowserConfig.proxy_config</code>.</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            <span class=\"hljs-string\">\"server\"</span>: <span class=\"hljs-string\">\"http://proxy.example.com:8080\"</span>,\n            <span class=\"hljs-string\">\"username\"</span>: <span class=\"hljs-string\">\"myuser\"</span>,\n            <span class=\"hljs-string\">\"password\"</span>: <span class=\"hljs-string\">\"mypass\"</span>,\n        },\n        headless=<span class=\"hljs-literal\">True</span>\n    )\n    crawler_cfg = CrawlerRunConfig(\n        verbose=<span class=\"hljs-literal\">True</span>\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_cfg) <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://www.whatismyip.com/\"</span>,\n            config=crawler_cfg\n        )\n        <span class=\"hljs-keyword\">if</span> result.success:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"[OK] Page fetched via proxy.\"</span>)\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Page HTML snippet:\"</span>, result.html[:<span class=\"hljs-number\">200</span>])\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"[ERROR]\"</span>, result.error_message)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Key Points</strong><br>\n- <strong><code>proxy_config</code></strong> expects a dict with <code>server</code> and optional auth credentials.<br>\n- Many commercial proxies provide an HTTP/HTTPS “gateway” server that you specify in <code>server</code>.<br>\n- If your proxy doesn’t need auth, omit <code>username</code>/<code>password</code>.</p>\n<hr>\n<h2 id=\"2-capturing-pdfs-screenshots\">2. Capturing PDFs &amp; Screenshots</h2>\n<p>Sometimes you need a visual record of a page or a PDF “printout.” Crawl4AI can do both in one pass:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> os, asyncio\n<span class=\"hljs-keyword\">from</span> base64 <span class=\"hljs-keyword\">import</span> b64decode\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CacheMode\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\"</span>,\n            cache_mode=CacheMode.BYPASS,\n            pdf=<span class=\"hljs-literal\">True</span>,\n            screenshot=<span class=\"hljs-literal\">True</span>\n        )\n\n        <span class=\"hljs-keyword\">if</span> result.success:\n            <span class=\"hljs-comment\"># Save screenshot</span>\n            <span class=\"hljs-keyword\">if</span> result.screenshot:\n                <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"wikipedia_screenshot.png\"</span>, <span class=\"hljs-string\">\"wb\"</span>) <span class=\"hljs-keyword\">as</span> f:\n                    f.write(b64decode(result.screenshot))\n\n            <span class=\"hljs-comment\"># Save PDF</span>\n            <span class=\"hljs-keyword\">if</span> result.pdf:\n                <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"wikipedia_page.pdf\"</span>, <span class=\"hljs-string\">\"wb\"</span>) <span class=\"hljs-keyword\">as</span> f:\n                    f.write(result.pdf)\n\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"[OK] PDF &amp; screenshot captured.\"</span>)\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"[ERROR]\"</span>, result.error_message)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Why PDF + Screenshot?</strong><br>\n- Large or complex pages can be slow or error-prone with “traditional” full-page screenshots.<br>\n- Exporting a PDF is more reliable for very long pages. Crawl4AI automatically converts the first PDF page into an image if you request both.  </p>\n<p><strong>Relevant Parameters</strong><br>\n- <strong><code>pdf=True</code></strong>: Exports the current page as a PDF (base64-encoded in <code>result.pdf</code>).<br>\n- <strong><code>screenshot=True</code></strong>: Creates a screenshot (base64-encoded in <code>result.screenshot</code>).<br>\n- <strong><code>scan_full_page</code></strong> or advanced hooking can further refine how the crawler captures content.</p>\n<hr>\n<h2 id=\"3-handling-ssl-certificates\">3. Handling SSL Certificates</h2>\n<p>If you need to verify or export a site’s SSL certificate—for compliance, debugging, or data analysis—Crawl4AI can fetch it during the crawl:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio, os\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    tmp_dir = os.path.join(os.getcwd(), <span class=\"hljs-string\">\"tmp\"</span>)\n    os.makedirs(tmp_dir, exist_ok=<span class=\"hljs-literal\">True</span>)\n\n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=<span class=\"hljs-literal\">True</span>,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(url=<span class=\"hljs-string\">\"https://example.com\"</span>, config=config)\n\n        <span class=\"hljs-keyword\">if</span> result.success <span class=\"hljs-keyword\">and</span> result.ssl_certificate:\n            cert = result.ssl_certificate\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\\nCertificate Information:\"</span>)\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Issuer (CN): <span class=\"hljs-subst\">{cert.issuer.get(<span class=\"hljs-string\">'CN'</span>, <span class=\"hljs-string\">''</span>)}</span>\"</span>)\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Valid until: <span class=\"hljs-subst\">{cert.valid_until}</span>\"</span>)\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Fingerprint: <span class=\"hljs-subst\">{cert.fingerprint}</span>\"</span>)\n\n            <span class=\"hljs-comment\"># Export in multiple formats:</span>\n            cert.to_json(os.path.join(tmp_dir, <span class=\"hljs-string\">\"certificate.json\"</span>))\n            cert.to_pem(os.path.join(tmp_dir, <span class=\"hljs-string\">\"certificate.pem\"</span>))\n            cert.to_der(os.path.join(tmp_dir, <span class=\"hljs-string\">\"certificate.der\"</span>))\n\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\\nCertificate exported to JSON/PEM/DER in 'tmp' folder.\"</span>)\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"[ERROR] No certificate or crawl failed.\"</span>)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Key Points</strong><br>\n- <strong><code>fetch_ssl_certificate=True</code></strong> triggers certificate retrieval.<br>\n- <code>result.ssl_certificate</code> includes methods (<code>to_json</code>, <code>to_pem</code>, <code>to_der</code>) for saving in various formats (handy for server config, Java keystores, etc.).</p>\n<hr>\n<h2 id=\"4-custom-headers\">4. Custom Headers</h2>\n<p>Sometimes you need to set custom headers (e.g., language preferences, authentication tokens, or specialized user-agent strings). You can do this in multiple ways:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-comment\"># Option 1: Set headers at the crawler strategy level</span>\n    crawler1 = AsyncWebCrawler(\n        <span class=\"hljs-comment\"># The underlying strategy can accept headers in its constructor</span>\n        crawler_strategy=<span class=\"hljs-literal\">None</span>  <span class=\"hljs-comment\"># We'll override below for clarity</span>\n    )\n    crawler1.crawler_strategy.update_user_agent(<span class=\"hljs-string\">\"MyCustomUA/1.0\"</span>)\n    crawler1.crawler_strategy.set_custom_headers({\n        <span class=\"hljs-string\">\"Accept-Language\"</span>: <span class=\"hljs-string\">\"fr-FR,fr;q=0.9\"</span>\n    })\n    result1 = <span class=\"hljs-keyword\">await</span> crawler1.arun(<span class=\"hljs-string\">\"https://www.example.com\"</span>)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Example 1 result success:\"</span>, result1.success)\n\n    <span class=\"hljs-comment\"># Option 2: Pass headers directly to `arun()`</span>\n    crawler2 = AsyncWebCrawler()\n    result2 = <span class=\"hljs-keyword\">await</span> crawler2.arun(\n        url=<span class=\"hljs-string\">\"https://www.example.com\"</span>,\n        headers={<span class=\"hljs-string\">\"Accept-Language\"</span>: <span class=\"hljs-string\">\"es-ES,es;q=0.9\"</span>}\n    )\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Example 2 result success:\"</span>, result2.success)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Notes</strong><br>\n- Some sites may react differently to certain headers (e.g., <code>Accept-Language</code>).<br>\n- If you need advanced user-agent randomization or client hints, see <a href=\"../identity-based-crawling/\">Identity-Based Crawling (Anti-Bot)</a> or use <code>UserAgentGenerator</code>.</p>\n<hr>\n<h2 id=\"5-session-persistence-local-storage\">5. Session Persistence &amp; Local Storage</h2>\n<p>Crawl4AI can preserve cookies and localStorage so you can continue where you left off—ideal for logging into sites or skipping repeated auth flows.</p>\n<h3 id=\"51-storage_state\">5.1 <code>storage_state</code></h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    storage_dict = {\n        <span class=\"hljs-string\">\"cookies\"</span>: [\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"session\"</span>,\n                <span class=\"hljs-string\">\"value\"</span>: <span class=\"hljs-string\">\"abcd1234\"</span>,\n                <span class=\"hljs-string\">\"domain\"</span>: <span class=\"hljs-string\">\"example.com\"</span>,\n                <span class=\"hljs-string\">\"path\"</span>: <span class=\"hljs-string\">\"/\"</span>,\n                <span class=\"hljs-string\">\"expires\"</span>: <span class=\"hljs-number\">1699999999.0</span>,\n                <span class=\"hljs-string\">\"httpOnly\"</span>: <span class=\"hljs-literal\">False</span>,\n                <span class=\"hljs-string\">\"secure\"</span>: <span class=\"hljs-literal\">False</span>,\n                <span class=\"hljs-string\">\"sameSite\"</span>: <span class=\"hljs-string\">\"None\"</span>\n            }\n        ],\n        <span class=\"hljs-string\">\"origins\"</span>: [\n            {\n                <span class=\"hljs-string\">\"origin\"</span>: <span class=\"hljs-string\">\"https://example.com\"</span>,\n                <span class=\"hljs-string\">\"localStorage\"</span>: [\n                    {<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"token\"</span>, <span class=\"hljs-string\">\"value\"</span>: <span class=\"hljs-string\">\"my_auth_token\"</span>}\n                ]\n            }\n        ]\n    }\n\n    <span class=\"hljs-comment\"># Provide the storage state as a dictionary to start \"already logged in\"</span>\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(\n        headless=<span class=\"hljs-literal\">True</span>,\n        storage_state=storage_dict\n    ) <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(<span class=\"hljs-string\">\"https://example.com/protected\"</span>)\n        <span class=\"hljs-keyword\">if</span> result.success:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Protected page content length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.html))\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Failed to crawl protected page\"</span>)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<h3 id=\"52-exporting-reusing-state\">5.2 Exporting &amp; Reusing State</h3>\n<p>You can sign in once, export the browser context, and reuse it later—without re-entering credentials.</p>\n<ul>\n<li><strong><code>await context.storage_state(path=\"my_storage.json\")</code></strong>: Exports cookies, localStorage, etc. to a file.  </li>\n<li>Provide <code>storage_state=\"my_storage.json\"</code> on subsequent runs to skip the login step.</li>\n</ul>\n<p><strong>See</strong>: <a href=\"../session-management/\">Detailed session management tutorial</a> or <a href=\"../identity-based-crawling/\">Explanations → Browser Context &amp; Managed Browser</a> for more advanced scenarios (like multi-step logins, or capturing after interactive pages).</p>\n<hr>\n<h2 id=\"6-robotstxt-compliance\">6. Robots.txt Compliance</h2>\n<p>Crawl4AI supports respecting robots.txt rules with efficient caching:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-comment\"># Enable robots.txt checking in config</span>\n    config = CrawlerRunConfig(\n        check_robots_txt=<span class=\"hljs-literal\">True</span>  <span class=\"hljs-comment\"># Will check and respect robots.txt rules</span>\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            <span class=\"hljs-string\">\"https://example.com\"</span>,\n            config=config\n        )\n\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> result.success <span class=\"hljs-keyword\">and</span> result.status_code == <span class=\"hljs-number\">403</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Access denied by robots.txt\"</span>)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Key Points</strong>\n- Robots.txt files are cached locally for efficiency\n- Cache is stored in <code>~/.crawl4ai/robots/robots_cache.db</code>\n- Cache has a default TTL of 7 days\n- If robots.txt can't be fetched, crawling is allowed\n- Returns 403 status code if URL is disallowed</p>\n<hr>\n<h2 id=\"putting-it-all-together\">Putting It All Together</h2>\n<p>Here’s a snippet that combines multiple “advanced” features (proxy, PDF, screenshot, SSL, custom headers, and session reuse) into one run. Normally, you’d tailor each setting to your project’s needs.</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> os, asyncio\n<span class=\"hljs-keyword\">from</span> base64 <span class=\"hljs-keyword\">import</span> b64decode\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-comment\"># 1. Browser config with proxy + headless</span>\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            <span class=\"hljs-string\">\"server\"</span>: <span class=\"hljs-string\">\"http://proxy.example.com:8080\"</span>,\n            <span class=\"hljs-string\">\"username\"</span>: <span class=\"hljs-string\">\"myuser\"</span>,\n            <span class=\"hljs-string\">\"password\"</span>: <span class=\"hljs-string\">\"mypass\"</span>,\n        },\n        headless=<span class=\"hljs-literal\">True</span>,\n    )\n\n    <span class=\"hljs-comment\"># 2. Crawler config with PDF, screenshot, SSL, custom headers, and ignoring caches</span>\n    crawler_cfg = CrawlerRunConfig(\n        pdf=<span class=\"hljs-literal\">True</span>,\n        screenshot=<span class=\"hljs-literal\">True</span>,\n        fetch_ssl_certificate=<span class=\"hljs-literal\">True</span>,\n        cache_mode=CacheMode.BYPASS,\n        headers={<span class=\"hljs-string\">\"Accept-Language\"</span>: <span class=\"hljs-string\">\"en-US,en;q=0.8\"</span>},\n        storage_state=<span class=\"hljs-string\">\"my_storage.json\"</span>,  <span class=\"hljs-comment\"># Reuse session from a previous sign-in</span>\n        verbose=<span class=\"hljs-literal\">True</span>,\n    )\n\n    <span class=\"hljs-comment\"># 3. Crawl</span>\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_cfg) <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url = <span class=\"hljs-string\">\"https://secure.example.com/protected\"</span>, \n            config=crawler_cfg\n        )\n\n        <span class=\"hljs-keyword\">if</span> result.success:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"[OK] Crawled the secure page. Links found:\"</span>, <span class=\"hljs-built_in\">len</span>(result.links.get(<span class=\"hljs-string\">\"internal\"</span>, [])))\n\n            <span class=\"hljs-comment\"># Save PDF &amp; screenshot</span>\n            <span class=\"hljs-keyword\">if</span> result.pdf:\n                <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"result.pdf\"</span>, <span class=\"hljs-string\">\"wb\"</span>) <span class=\"hljs-keyword\">as</span> f:\n                    f.write(b64decode(result.pdf))\n            <span class=\"hljs-keyword\">if</span> result.screenshot:\n                <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"result.png\"</span>, <span class=\"hljs-string\">\"wb\"</span>) <span class=\"hljs-keyword\">as</span> f:\n                    f.write(b64decode(result.screenshot))\n\n            <span class=\"hljs-comment\"># Check SSL cert</span>\n            <span class=\"hljs-keyword\">if</span> result.ssl_certificate:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"SSL Issuer CN:\"</span>, result.ssl_certificate.issuer.get(<span class=\"hljs-string\">\"CN\"</span>, <span class=\"hljs-string\">\"\"</span>))\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"[ERROR]\"</span>, result.error_message)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<hr>\n<h2 id=\"conclusion-next-steps\">Conclusion &amp; Next Steps</h2>\n<p>You’ve now explored several <strong>advanced</strong> features:</p>\n<ul>\n<li><strong>Proxy Usage</strong>  </li>\n<li><strong>PDF &amp; Screenshot</strong> capturing for large or critical pages  </li>\n<li><strong>SSL Certificate</strong> retrieval &amp; exporting  </li>\n<li><strong>Custom Headers</strong> for language or specialized requests  </li>\n<li><strong>Session Persistence</strong> via storage state</li>\n<li><strong>Robots.txt Compliance</strong></li>\n</ul>\n<p>With these power tools, you can build robust scraping workflows that mimic real user behavior, handle secure sites, capture detailed snapshots, and manage sessions across multiple runs—streamlining your entire data collection pipeline.</p>\n<p><strong>Last Updated</strong>: 2025-01-01</p>\n</section>\n\n            </main>\n        </div>\n        <hr><footer>\n    <div class=\"terminal-mkdocs-footer-grid\">\n        <div id=\"terminal-mkdocs-footer-copyright-info\">\n             Site built with <a href=\"http://www.mkdocs.org\">MkDocs</a> and <a href=\"https://github.com/ntno/mkdocs-terminal\">Terminal for MkDocs</a>.\n        </div>\n    </div>\n</footer>\n    </div>\n\n    \n    <div class=\"modal\" id=\"mkdocs_search_modal\" tabindex=\"-1\" role=\"alertdialog\" aria-modal=\"true\" aria-labelledby=\"searchModalLabel\">\n    <div class=\"modal-dialog modal-lg\" role=\"search\">\n        <div class=\"modal-content\">\n            <div class=\"modal-header\">\n                <h5 class=\"modal-title\" id=\"searchModalLabel\">Search</h5>\n                <button type=\"button\" class=\"close btn btn-default btn-ghost\" data-dismiss=\"modal\"><span aria-hidden=\"true\">x</span><span class=\"sr-only\">Close</span></button>\n            </div>\n            <div class=\"modal-body\">\n                <p id=\"searchInputLabel\">Type to start searching</p>\n                <form>\n                    <div class=\"form-group\">\n                        <input type=\"search\" class=\"form-control\" aria-labelledby=\"searchInputLabel\" placeholder=\"\" id=\"mkdocs-search-query\" title=\"Please enter search terms here\">\n                    </div>\n                </form>\n                <div id=\"mkdocs-search-results\" data-no-results-text=\"No document matches found\"></div>\n            </div>\n            <div class=\"modal-footer\">\n            </div>\n        </div>\n    </div>\n</div>\n    \n    \n\n\n</body></html>",
  "markdown": "# Overview of Some Important Advanced Features\n(Proxy, PDF, Screenshot, SSL, Headers, & Storage State)\nCrawl4AI offers multiple power-user features that go beyond simple crawling. This tutorial covers:\n1. **Proxy Usage** 2. **Capturing PDFs & Screenshots** 3. **Handling SSL Certificates** 4. **Custom Headers** 5. **Session Persistence & Local Storage** 6. **Robots.txt Compliance**\n> **Prerequisites** - You have a basic grasp of [AsyncWebCrawler Basics](https://docs.crawl4ai.com/advanced/core/simple-crawling/>) - You know how to run or configure your Python environment with Playwright installed\n## 1. Proxy Usage\nIf you need to route your crawl traffic through a proxy—whether for IP rotation, geo-testing, or privacy—Crawl4AI supports it via `BrowserConfig.proxy_config`.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\nasync def main():\n  browser_cfg = BrowserConfig(\n    proxy_config={\n      \"server\": \"http://proxy.example.com:8080\",\n      \"username\": \"myuser\",\n      \"password\": \"mypass\",\n    },\n    headless=True\n  )\n  crawler_cfg = CrawlerRunConfig(\n    verbose=True\n  )\n  async with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\n      url=\"https://www.whatismyip.com/\",\n      config=crawler_cfg\n    )\n    if result.success:\n      print(\"[OK] Page fetched via proxy.\")\n      print(\"Page HTML snippet:\", result.html[:200])\n    else:\n      print(\"[ERROR]\", result.error_message)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Key Points** - **`proxy_config`**expects a dict with`server` and optional auth credentials. - Many commercial proxies provide an HTTP/HTTPS “gateway” server that you specify in `server`. - If your proxy doesn’t need auth, omit `username`/`password`.\n## 2. Capturing PDFs & Screenshots\nSometimes you need a visual record of a page or a PDF “printout.” Crawl4AI can do both in one pass:\n```\nimport os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, CacheMode\nasync def main():\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\n      cache_mode=CacheMode.BYPASS,\n      pdf=True,\n      screenshot=True\n    )\n    if result.success:\n      # Save screenshot\n      if result.screenshot:\n        with open(\"wikipedia_screenshot.png\", \"wb\") as f:\n          f.write(b64decode(result.screenshot))\n      # Save PDF\n      if result.pdf:\n        with open(\"wikipedia_page.pdf\", \"wb\") as f:\n          f.write(result.pdf)\n      print(\"[OK] PDF & screenshot captured.\")\n    else:\n      print(\"[ERROR]\", result.error_message)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Why PDF + Screenshot?** - Large or complex pages can be slow or error-prone with “traditional” full-page screenshots. - Exporting a PDF is more reliable for very long pages. Crawl4AI automatically converts the first PDF page into an image if you request both. \n**Relevant Parameters** - **`pdf=True`**: Exports the current page as a PDF (base64-encoded in`result.pdf`). - **`screenshot=True`**: Creates a screenshot (base64-encoded in`result.screenshot`). - **`scan_full_page`**or advanced hooking can further refine how the crawler captures content.\n## 3. Handling SSL Certificates\nIf you need to verify or export a site’s SSL certificate—for compliance, debugging, or data analysis—Crawl4AI can fetch it during the crawl:\n```\nimport asyncio, os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nasync def main():\n  tmp_dir = os.path.join(os.getcwd(), \"tmp\")\n  os.makedirs(tmp_dir, exist_ok=True)\n  config = CrawlerRunConfig(\n    fetch_ssl_certificate=True,\n    cache_mode=CacheMode.BYPASS\n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(url=\"https://example.com\", config=config)\n    if result.success and result.ssl_certificate:\n      cert = result.ssl_certificate\n      print(\"\\nCertificate Information:\")\n      print(f\"Issuer (CN): {cert.issuer.get('CN', '')}\")\n      print(f\"Valid until: {cert.valid_until}\")\n      print(f\"Fingerprint: {cert.fingerprint}\")\n      # Export in multiple formats:\n      cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))\n      cert.to_pem(os.path.join(tmp_dir, \"certificate.pem\"))\n      cert.to_der(os.path.join(tmp_dir, \"certificate.der\"))\n      print(\"\\nCertificate exported to JSON/PEM/DER in 'tmp' folder.\")\n    else:\n      print(\"[ERROR] No certificate or crawl failed.\")\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Key Points** - **`fetch_ssl_certificate=True`**triggers certificate retrieval. -`result.ssl_certificate` includes methods (`to_json`, `to_pem`, `to_der`) for saving in various formats (handy for server config, Java keystores, etc.).\n## 4. Custom Headers\nSometimes you need to set custom headers (e.g., language preferences, authentication tokens, or specialized user-agent strings). You can do this in multiple ways:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n  # Option 1: Set headers at the crawler strategy level\n  crawler1 = AsyncWebCrawler(\n    # The underlying strategy can accept headers in its constructor\n    crawler_strategy=None # We'll override below for clarity\n  )\n  crawler1.crawler_strategy.update_user_agent(\"MyCustomUA/1.0\")\n  crawler1.crawler_strategy.set_custom_headers({\n    \"Accept-Language\": \"fr-FR,fr;q=0.9\"\n  })\n  result1 = await crawler1.arun(\"https://www.example.com\")\n  print(\"Example 1 result success:\", result1.success)\n  # Option 2: Pass headers directly to `arun()`\n  crawler2 = AsyncWebCrawler()\n  result2 = await crawler2.arun(\n    url=\"https://www.example.com\",\n    headers={\"Accept-Language\": \"es-ES,es;q=0.9\"}\n  )\n  print(\"Example 2 result success:\", result2.success)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Notes** - Some sites may react differently to certain headers (e.g., `Accept-Language`). - If you need advanced user-agent randomization or client hints, see [Identity-Based Crawling (Anti-Bot)](https://docs.crawl4ai.com/advanced/<../identity-based-crawling/>) or use `UserAgentGenerator`.\n## 5. Session Persistence & Local Storage\nCrawl4AI can preserve cookies and localStorage so you can continue where you left off—ideal for logging into sites or skipping repeated auth flows.\n### 5.1 `storage_state`\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n  storage_dict = {\n    \"cookies\": [\n      {\n        \"name\": \"session\",\n        \"value\": \"abcd1234\",\n        \"domain\": \"example.com\",\n        \"path\": \"/\",\n        \"expires\": 1699999999.0,\n        \"httpOnly\": False,\n        \"secure\": False,\n        \"sameSite\": \"None\"\n      }\n    ],\n    \"origins\": [\n      {\n        \"origin\": \"https://example.com\",\n        \"localStorage\": [\n          {\"name\": \"token\", \"value\": \"my_auth_token\"}\n        ]\n      }\n    ]\n  }\n  # Provide the storage state as a dictionary to start \"already logged in\"\n  async with AsyncWebCrawler(\n    headless=True,\n    storage_state=storage_dict\n  ) as crawler:\n    result = await crawler.arun(\"https://example.com/protected\")\n    if result.success:\n      print(\"Protected page content length:\", len(result.html))\n    else:\n      print(\"Failed to crawl protected page\")\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n### 5.2 Exporting & Reusing State\nYou can sign in once, export the browser context, and reuse it later—without re-entering credentials.\n  * **`await context.storage_state(path=\"my_storage.json\")`**: Exports cookies, localStorage, etc. to a file.\n  * Provide `storage_state=\"my_storage.json\"` on subsequent runs to skip the login step.\n\n\n**See** : [Detailed session management tutorial](https://docs.crawl4ai.com/advanced/<../session-management/>) or [Explanations → Browser Context & Managed Browser](https://docs.crawl4ai.com/advanced/<../identity-based-crawling/>) for more advanced scenarios (like multi-step logins, or capturing after interactive pages).\n## 6. Robots.txt Compliance\nCrawl4AI supports respecting robots.txt rules with efficient caching:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n  # Enable robots.txt checking in config\n  config = CrawlerRunConfig(\n    check_robots_txt=True # Will check and respect robots.txt rules\n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      \"https://example.com\",\n      config=config\n    )\n    if not result.success and result.status_code == 403:\n      print(\"Access denied by robots.txt\")\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Key Points** - Robots.txt files are cached locally for efficiency - Cache is stored in `~/.crawl4ai/robots/robots_cache.db` - Cache has a default TTL of 7 days - If robots.txt can't be fetched, crawling is allowed - Returns 403 status code if URL is disallowed\n## Putting It All Together\nHere’s a snippet that combines multiple “advanced” features (proxy, PDF, screenshot, SSL, custom headers, and session reuse) into one run. Normally, you’d tailor each setting to your project’s needs.\n```\nimport os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nasync def main():\n  # 1. Browser config with proxy + headless\n  browser_cfg = BrowserConfig(\n    proxy_config={\n      \"server\": \"http://proxy.example.com:8080\",\n      \"username\": \"myuser\",\n      \"password\": \"mypass\",\n    },\n    headless=True,\n  )\n  # 2. Crawler config with PDF, screenshot, SSL, custom headers, and ignoring caches\n  crawler_cfg = CrawlerRunConfig(\n    pdf=True,\n    screenshot=True,\n    fetch_ssl_certificate=True,\n    cache_mode=CacheMode.BYPASS,\n    headers={\"Accept-Language\": \"en-US,en;q=0.8\"},\n    storage_state=\"my_storage.json\", # Reuse session from a previous sign-in\n    verbose=True,\n  )\n  # 3. Crawl\n  async with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\n      url = \"https://secure.example.com/protected\", \n      config=crawler_cfg\n    )\n    if result.success:\n      print(\"[OK] Crawled the secure page. Links found:\", len(result.links.get(\"internal\", [])))\n      # Save PDF & screenshot\n      if result.pdf:\n        with open(\"result.pdf\", \"wb\") as f:\n          f.write(b64decode(result.pdf))\n      if result.screenshot:\n        with open(\"result.png\", \"wb\") as f:\n          f.write(b64decode(result.screenshot))\n      # Check SSL cert\n      if result.ssl_certificate:\n        print(\"SSL Issuer CN:\", result.ssl_certificate.issuer.get(\"CN\", \"\"))\n    else:\n      print(\"[ERROR]\", result.error_message)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n## Conclusion & Next Steps\nYou’ve now explored several **advanced** features:\n  * **Proxy Usage**\n  * **PDF & Screenshot** capturing for large or critical pages \n  * **SSL Certificate** retrieval & exporting \n  * **Custom Headers** for language or specialized requests \n  * **Session Persistence** via storage state\n  * **Robots.txt Compliance**\n\n\nWith these power tools, you can build robust scraping workflows that mimic real user behavior, handle secure sites, capture detailed snapshots, and manage sessions across multiple runs—streamlining your entire data collection pipeline.\n**Last Updated** : 2025-01-01\n##### Search\nxClose\nType to start searching\n",
  "links": [
    "https://docs.crawl4ai.com",
    "https://docs.crawl4ai.com/api/arun",
    "https://docs.crawl4ai.com/api/arun_many",
    "https://docs.crawl4ai.com/api/async-webcrawler",
    "https://docs.crawl4ai.com/api/crawl-result",
    "https://docs.crawl4ai.com/api/parameters",
    "https://docs.crawl4ai.com/api/strategies",
    "https://docs.crawl4ai.com/blog",
    "https://docs.crawl4ai.com/core/browser-crawler-config",
    "https://docs.crawl4ai.com/core/cache-modes",
    "https://docs.crawl4ai.com/core/content-selection",
    "https://docs.crawl4ai.com/core/crawler-result",
    "https://docs.crawl4ai.com/core/docker-deploymeny",
    "https://docs.crawl4ai.com/core/fit-markdown",
    "https://docs.crawl4ai.com/core/installation",
    "https://docs.crawl4ai.com/core/link-media",
    "https://docs.crawl4ai.com/core/local-files",
    "https://docs.crawl4ai.com/core/markdown-generation",
    "https://docs.crawl4ai.com/core/page-interaction",
    "https://docs.crawl4ai.com/core/quickstart",
    "https://docs.crawl4ai.com/core/simple-crawling",
    "https://docs.crawl4ai.com/crawl-dispatcher",
    "https://docs.crawl4ai.com/extraction/chunking",
    "https://docs.crawl4ai.com/extraction/clustring-strategies",
    "https://docs.crawl4ai.com/extraction/llm-strategies",
    "https://docs.crawl4ai.com/extraction/no-llm-strategies",
    "https://docs.crawl4ai.com/file-downloading",
    "https://docs.crawl4ai.com/hooks-auth",
    "https://docs.crawl4ai.com/identity-based-crawling",
    "https://docs.crawl4ai.com/lazy-loading",
    "https://docs.crawl4ai.com/multi-url-crawling",
    "https://docs.crawl4ai.com/proxy-security",
    "https://docs.crawl4ai.com/session-management",
    "https://docs.crawl4ai.com/ssl-certificate"
  ],
  "title": "Overview - Crawl4AI Documentation (v0.4.3bx)",
  "keywords": [],
  "last_modified": null,
  "stats": {
    "processed": 1,
    "total": 0,
    "depth": 0,
    "elapsed": "0:00:02",
    "page_limit": null
  }
}