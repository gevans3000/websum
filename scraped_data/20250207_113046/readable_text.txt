Browser & Crawler Configuration (Quick Overview)
Crawl4AI’s flexibility stems from two key classes:
• *– Dictateshow the browser is launched and behaves (e.g., headless or visible, proxy, user agent). 2. – Dictateshow each crawl operates (e.g., caching, extraction, timeouts, JavaScript code to run, etc.).
In most examples, you create one for the entire crawler session, then pass a fresh or re-used whenever you call . This tutorial shows the most commonly used parameters. If you need advanced or rarely used fields, see the Configuration Parameters [1].
• BrowserConfig Essentials

---

Key Fields to Note
• - Options: , , or . - Defaults to . - If you need a different engine, specify it here.
• - : Runs the browser in headless mode (invisible browser). - : Runs the browser in visible mode, which helps with debugging.
• - A dictionary with fields like:
• Leave as if a proxy is not required. 
• & : - The initial window size. - Some sites behave differently with smaller or bigger viewports.
• : - If , prints extra logs. - Handy for debugging.
• : - If , uses a persistent browser profile, storing cookies/local storage across runs. - Typically also set to point to a folder.
• & : - If you want to start with specific cookies or add universal HTTP headers, set them here. - E.g..
• : - Custom User-Agent string. If , a default is used. - You can also set for randomization (if you want to fight bot detection).
• & : - disables images, possibly speeding up text-only crawls. - turns off certain background features for performance. 
• : - Additional flags for the underlying browser. - E.g..
Helper Methods
Both configuration classes provide a method to create modified copies:

---

Minimal Example :
• CrawlerRunConfig Essentials

---

Key Fields to Note
• : - The minimum word count before a block is considered. - If your site has lots of short paragraphs or items, you can lower it.
• : - Where you plug in JSON-based extraction (CSS, LLM, etc.). - If , no structured extraction is done (only raw/cleaned HTML + markdown).
• : - E.g., , controlling how HTML→Markdown conversion is done. - If , a default approach is used.
• : - Controls caching behavior ( , , , etc.). - If , defaults to some level of caching or you can specify .
• : - A string or list of JS strings to execute. - Great for “Load More” buttons or user interactions.
• : - A CSS or JS expression to wait for before extracting content. - Common usage: or .
• & : - If , captures a screenshot or PDF after the page is fully loaded. - The results go to (base64) or (bytes).
• : - Logs additional runtime details. - Overlaps with the browser’s verbosity if also set to in .
• : - If , enables rate limiting for batch processing. - Requires to be set.
• : - A object controlling rate limiting behavior. - See below for details.
• : - The memory threshold (as a percentage) to monitor. - If exceeded, the crawler will pause or slow down.
• : - The interval (in seconds) to check system resources. - Affects how often memory and CPU usage are monitored.
• : - The maximum number of concurrent crawl sessions. - Helps prevent overwhelming the system.
• : - The display mode for progress information ( , , etc.). - Affects how much information is printed during the crawl.
Helper Methods
The method is particularly useful for creating variations of your crawler configuration:

---

The method: - Creates a new instance with all the same settings - Updates only the specified parameters - Leaves the original configuration unchanged - Perfect for creating variations without repeating all parameters
Rate Limiting & Resource Management
For batch processing with , you can enable intelligent rate limiting:

---

This configuration: - Implements intelligent rate limiting per domain - Monitors system resources - Provides detailed progress information - Manages concurrent crawls efficiently
Minimal Example :
• Putting It All Together
In a typical scenario, you define one for your crawler session, then create one or more depending on each call’s needs:
• Next Steps
For a detailed list of available parameters (including advanced ones), see:
 BrowserConfig and CrawlerRunConfig Reference [2]

---

You can explore topics like:
 Custom Hooks & Auth (Inject JavaScript or handle login forms). 
 Session Management (Re-use pages, preserve state across multiple calls). 
 Magic Mode or Identity-based Crawling (Fight bot detection by simulating user behavior). 
 Advanced Caching (Fine-tune read/write cache modes). 
• Conclusion
BrowserConfig and CrawlerRunConfig give you straightforward ways to define:
 Which browser to launch, how it should run, and any proxy or user agent needs. 
 How each crawl should behave—caching, timeouts, JavaScript code, extraction strategies, etc.

---

Use them together for clear, maintainable code, and when you need more specialized behavior, check out the advanced parameters in the reference docs [3]. Happy crawling!
Search
xClose
Type to start searching


References:
[1] https://docs.crawl4ai.com/core/browser-crawler-config/api/parameters/>
[2] https://docs.crawl4ai.com/core/browser-crawler-config/api/parameters/>
[3] https://docs.crawl4ai.com/core/browser-crawler-config/api/parameters/>