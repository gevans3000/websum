Overview of Some Important Advanced Features
(Proxy, PDF, Screenshot, SSL, Headers, & Storage State)
Crawl4AI offers multiple power-user features that go beyond simple crawling. This tutorial covers:
• Proxy Usage 2. Capturing PDFs & Screenshots 3. Handling SSL Certificates 4. Custom Headers 5. Session Persistence & Local Storage 6. Robots.txt Compliance
> Prerequisites - You have a basic grasp of AsyncWebCrawler Basics [1] - You know how to run or configure your Python environment with Playwright installed
• Proxy Usage
If you need to route your crawl traffic through a proxy—whether for IP rotation, geo-testing, or privacy—Crawl4AI supports it via .

---

Key Points - *expects a dict with and optional auth credentials. - Many commercial proxies provide an HTTP/HTTPS “gateway” server that you specify in . - If your proxy doesn’t need auth, omit /.
• Capturing PDFs & Screenshots
Sometimes you need a visual record of a page or a PDF “printout.” Crawl4AI can do both in one pass:

---

Why PDF + Screenshot? - Large or complex pages can be slow or error-prone with “traditional” full-page screenshots. - Exporting a PDF is more reliable for very long pages. Crawl4AI automatically converts the first PDF page into an image if you request both. 
Relevant Parameters - : Exports the current page as a PDF (base64-encoded in). - : Creates a screenshot (base64-encoded in). - or advanced hooking can further refine how the crawler captures content.
• Handling SSL Certificates
If you need to verify or export a site’s SSL certificate—for compliance, debugging, or data analysis—Crawl4AI can fetch it during the crawl:

---

Key Points - triggers certificate retrieval. - includes methods (, , ) for saving in various formats (handy for server config, Java keystores, etc.).
• Custom Headers
Sometimes you need to set custom headers (e.g., language preferences, authentication tokens, or specialized user-agent strings). You can do this in multiple ways:
``arun()`Accept-LanguageUserAgentGeneratorstoragestateawait context.storagestate(path="mystorage.json")storagestate="mystorage.json"~/.crawl4ai/robots/robotscache.db` - Cache has a default TTL of 7 days - If robots.txt can't be fetched, crawling is allowed - Returns 403 status code if URL is disallowed
Putting It All Together
Here’s a snippet that combines multiple “advanced” features (proxy, PDF, screenshot, SSL, custom headers, and session reuse) into one run. Normally, you’d tailor each setting to your project’s needs.

---

Conclusion & Next Steps
You’ve now explored several advanced features:
 Proxy Usage
 PDF & Screenshot capturing for large or critical pages 
 SSL Certificate retrieval & exporting 
 Custom Headers for language or specialized requests 
 Session Persistence via storage state
• Robots.txt Compliance

---

With these power tools, you can build robust scraping workflows that mimic real user behavior, handle secure sites, capture detailed snapshots, and manage sessions across multiple runs—streamlining your entire data collection pipeline.
Last Updated : 2025-01-01
Search
xClose
Type to start searching


References:
[1] https://docs.crawl4ai.com/advanced/core/simple-crawling/>