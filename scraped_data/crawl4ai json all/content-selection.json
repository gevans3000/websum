{
  "url": "https://docs.crawl4ai.com/core/content-selection",
  "timestamp": "2025-02-06T13:23:36.609656",
  "html": "<!DOCTYPE html><html lang=\"en\" style=\"scroll-padding-top: 50px;\"><head>\n    \n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n    <meta name=\"generator\" content=\"mkdocs-1.6.0, mkdocs-terminal-4.4.0\">\n    \n    <meta name=\"description\" content=\"ðŸš€ðŸ¤– Crawl4AI, Open-source LLM-Friendly Web Crawler &amp; Scraper\"> \n     \n    \n    <link rel=\"canonical\" href=\"https://docs.crawl4ai.com/core/content-selection/\"><link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"../../img/android-chrome-192x192.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"512x512\" href=\"../../img/android-chrome-512x512.png\">\n<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"../../img/apple-touch-icon.png\">\n<link rel=\"shortcut icon\" type=\"image/png\" sizes=\"48x48\" href=\"../../img/favicon.ico\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"../../img/favicon-16x16.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"../../img/favicon-32x32.png\">\n\n\n    \n \n<title>Content Selection - Crawl4AI Documentation (v0.4.3bx)</title>\n\n\n<link href=\"../../css/fontawesome/css/fontawesome.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/fontawesome/css/solid.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/normalize.css\" rel=\"stylesheet\">\n<link href=\"../../css/terminal.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.tile_grid.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.footer.css\" rel=\"stylesheet\">\n<!-- dark color palette -->\n<link href=\"../../css/palettes/dark.css\" rel=\"stylesheet\">\n\n<!-- page layout -->\n<style>\n/* initially set page layout to a one column grid */\n.terminal-mkdocs-main-grid {\n    display: grid;\n    grid-column-gap: 1.4em;\n    grid-template-columns: auto;\n    grid-template-rows: auto;\n}\n\n/*  \n*   when side navigation is not hidden, use a two column grid.  \n*   if the screen is too narrow, fall back to the initial one column grid layout.\n*   in this case the main content will be placed under the navigation panel. \n*/\n@media only screen and (min-width: 70em) {\n    .terminal-mkdocs-main-grid {\n        grid-template-columns: 4fr 9fr;\n    }\n}</style>\n\n\n\n    \n    <link href=\"../../assets/styles.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/highlight.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/dmvendor.css\" rel=\"stylesheet\">  \n    \n    \n\n    \n    <!-- search css support -->\n<link href=\"../../css/search/bootstrap-modal.css\" rel=\"stylesheet\">\n<!-- search scripts -->\n<script>\n    var base_url = \"../..\",\n    shortcuts = \"{}\";\n</script>\n<script src=\"../../js/jquery/jquery-1.10.1.min.js\" defer=\"\"></script>\n<script src=\"../../js/bootstrap/bootstrap.min.js\" defer=\"\"></script>\n<script src=\"../../js/mkdocs/base.js\" defer=\"\"></script>\n    \n    \n    \n    \n    <script src=\"../../assets/highlight.min.js\"></script>\n    \n    <script src=\"../../assets/highlight_init.js\"></script>\n    \n    <script src=\"https://buttons.github.io/buttons.js\"></script>\n    \n    <script src=\"../../search/main.js\"></script>\n    \n\n    \n</head>\n\n<body class=\"terminal\" style=\"\"><div class=\"container\">\n    <div class=\"terminal-nav\">\n        <header class=\"terminal-logo\">\n            <div id=\"mkdocs-terminal-site-name\" class=\"logo terminal-prompt\"><a href=\"https://docs.crawl4ai.com/\" class=\"no-style\">Crawl4AI Documentation (v0.4.3bx)</a></div>\n        </header>\n        \n        <nav class=\"terminal-menu\">\n            \n            <ul vocab=\"https://schema.org/\" typeof=\"BreadcrumbList\">\n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../..\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Home</span>\n                    </a>\n                    <meta property=\"position\" content=\"0\">\n                </li>\n                \n                \n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../quickstart/\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Quick Start</span>\n                    </a>\n                    <meta property=\"position\" content=\"1\">\n                </li>\n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                    \n                    \n\n\n<li property=\"itemListElement\" typeof=\"ListItem\">\n    <a href=\"#\" class=\"menu-item\" data-toggle=\"modal\" data-target=\"#mkdocs_search_modal\" property=\"item\" typeof=\"SearchAction\">\n        <i aria-hidden=\"true\" class=\"fa fa-search\"></i> <span property=\"name\">Search</span>\n    </a>\n    <meta property=\"position\" content=\"2\">\n</li>\n                    \n            </ul>\n            \n        </nav>\n    </div>\n</div>\n        \n    <div class=\"container\">\n        <div class=\"terminal-mkdocs-main-grid\"><aside id=\"terminal-mkdocs-side-panel\"><nav>\n  \n    <ul class=\"terminal-mkdocs-side-nav-items\">\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../..\">Home</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Setup &amp; Installation</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../installation/\">Installation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../docker-deploymeny/\">Docker Deployment</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../quickstart/\">Quick Start</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Blog &amp; Changelog</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../blog/\">Blog Home</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md\">Changelog</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item--active terminal-mkdocs-side-nav-section-no-index\">Core</span>\n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../simple-crawling/\">Simple Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../crawler-result/\">Crawler Result</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../browser-crawler-config/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../markdown-generation/\">Markdown Generation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../fit-markdown/\">Fit Markdown</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../page-interaction/\">Page Interaction</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        <span class=\"\n\n    terminal-mkdocs-side-nav-item--active\">Content Selection</span>\n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../cache-modes/\">Cache Modes</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../local-files/\">Local Files &amp; Raw HTML</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../link-media/\">Link &amp; Media</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Advanced</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/advanced-features/\">Overview</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/file-downloading/\">File Downloading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/lazy-loading/\">Lazy Loading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/hooks-auth/\">Hooks &amp; Auth</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/proxy-security/\">Proxy &amp; Security</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/session-management/\">Session Management</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/multi-url-crawling/\">Multi-URL Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/crawl-dispatcher/\">Crawl Dispatcher</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/identity-based-crawling/\">Identity Based Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/ssl-certificate/\">SSL Certificate</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Extraction</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/no-llm-strategies/\">LLM-Free Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/llm-strategies/\">LLM Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/clustring-strategies/\">Clustering Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/chunking/\">Chunking</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">API Reference</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/async-webcrawler/\">AsyncWebCrawler</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun/\">arun()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun_many/\">arun_many()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/parameters/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/crawl-result/\">CrawlResult</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/strategies/\">Strategies</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n    </ul>\n  \n</nav><hr>\n<nav>\n    <ul>\n        <li><a href=\"#content-selection\">Content Selection</a></li>\n        <li><a href=\"#1-css-based-selection\">1. CSS-Based Selection</a></li><li><a href=\"#2-content-filtering-exclusions\">2. Content Filtering &amp; Exclusions</a></li><li><a href=\"#3-handling-iframes\">3. Handling Iframes</a></li><li><a href=\"#4-structured-extraction-examples\">4. Structured Extraction Examples</a></li><li><a href=\"#5-comprehensive-example\">5. Comprehensive Example</a></li><li><a href=\"#6-scraping-modes\">6. Scraping Modes</a></li><li><a href=\"#7-conclusion\">7. Conclusion</a></li>\n    </ul>\n</nav>\n</aside>\n            <main id=\"terminal-mkdocs-main-content\">\n<section id=\"mkdocs-terminal-content\">\n    <h1 id=\"content-selection\">Content Selection</h1>\n<p>Crawl4AI provides multiple ways to <strong>select</strong>, <strong>filter</strong>, and <strong>refine</strong> the content from your crawls. Whether you need to target a specific CSS region, exclude entire tags, filter out external links, or remove certain domains and images, <strong><code>CrawlerRunConfig</code></strong> offers a wide range of parameters.</p>\n<p>Below, we show how to configure these parameters and combine them for precise control.</p>\n<hr>\n<h2 id=\"1-css-based-selection\">1. CSS-Based Selection</h2>\n<p>A straightforward way to <strong>limit</strong> your crawl results to a certain region of the page is <strong><code>css_selector</code></strong> in <strong><code>CrawlerRunConfig</code></strong>:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    config = CrawlerRunConfig(\n        <span class=\"hljs-comment\"># e.g., first 30 items from Hacker News</span>\n        css_selector=<span class=\"hljs-string\">\".athing:nth-child(-n+30)\"</span>  \n    )\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://news.ycombinator.com/newest\"</span>, \n            config=config\n        )\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Partial HTML length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.cleaned_html))\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Result</strong>: Only elements matching that selector remain in <code>result.cleaned_html</code>.</p>\n<hr>\n<h2 id=\"2-content-filtering-exclusions\">2. Content Filtering &amp; Exclusions</h2>\n<h3 id=\"21-basic-overview\">2.1 Basic Overview</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\">config = CrawlerRunConfig(\n    <span class=\"hljs-comment\"># Content thresholds</span>\n    word_count_threshold=<span class=\"hljs-number\">10</span>,        <span class=\"hljs-comment\"># Minimum words per block</span>\n\n    <span class=\"hljs-comment\"># Tag exclusions</span>\n    excluded_tags=[<span class=\"hljs-string\">'form'</span>, <span class=\"hljs-string\">'header'</span>, <span class=\"hljs-string\">'footer'</span>, <span class=\"hljs-string\">'nav'</span>],\n\n    <span class=\"hljs-comment\"># Link filtering</span>\n    exclude_external_links=<span class=\"hljs-literal\">True</span>,    \n    exclude_social_media_links=<span class=\"hljs-literal\">True</span>,\n    <span class=\"hljs-comment\"># Block entire domains</span>\n    exclude_domains=[<span class=\"hljs-string\">\"adtrackers.com\"</span>, <span class=\"hljs-string\">\"spammynews.org\"</span>],    \n    exclude_social_media_domains=[<span class=\"hljs-string\">\"facebook.com\"</span>, <span class=\"hljs-string\">\"twitter.com\"</span>],\n\n    <span class=\"hljs-comment\"># Media filtering</span>\n    exclude_external_images=<span class=\"hljs-literal\">True</span>\n)\n</code></pre></div>\n<p><strong>Explanation</strong>:</p>\n<ul>\n<li><strong><code>word_count_threshold</code></strong>: Ignores text blocks under X words. Helps skip trivial blocks like short nav or disclaimers.  </li>\n<li><strong><code>excluded_tags</code></strong>: Removes entire tags (<code>&lt;form&gt;</code>, <code>&lt;header&gt;</code>, <code>&lt;footer&gt;</code>, etc.).  </li>\n<li><strong>Link Filtering</strong>:  </li>\n<li><code>exclude_external_links</code>: Strips out external links and may remove them from <code>result.links</code>.  </li>\n<li><code>exclude_social_media_links</code>: Removes links pointing to known social media domains.  </li>\n<li><code>exclude_domains</code>: A custom list of domains to block if discovered in links.  </li>\n<li><code>exclude_social_media_domains</code>: A curated list (override or add to it) for social media sites.  </li>\n<li><strong>Media Filtering</strong>:  </li>\n<li><code>exclude_external_images</code>: Discards images not hosted on the same domain as the main page (or its subdomains).</li>\n</ul>\n<p>By default in case you set <code>exclude_social_media_links=True</code>, the following social media domains are excluded:\n</p><div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-bash\">[\n    <span class=\"hljs-string\">'facebook.com'</span>,\n    <span class=\"hljs-string\">'twitter.com'</span>,\n    <span class=\"hljs-string\">'x.com'</span>,\n    <span class=\"hljs-string\">'linkedin.com'</span>,\n    <span class=\"hljs-string\">'instagram.com'</span>,\n    <span class=\"hljs-string\">'pinterest.com'</span>,\n    <span class=\"hljs-string\">'tiktok.com'</span>,\n    <span class=\"hljs-string\">'snapchat.com'</span>,\n    <span class=\"hljs-string\">'reddit.com'</span>,\n]\n</code></pre></div><p></p>\n<h3 id=\"22-example-usage\">2.2 Example Usage</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    config = CrawlerRunConfig(\n        css_selector=<span class=\"hljs-string\">\"main.content\"</span>, \n        word_count_threshold=<span class=\"hljs-number\">10</span>,\n        excluded_tags=[<span class=\"hljs-string\">\"nav\"</span>, <span class=\"hljs-string\">\"footer\"</span>],\n        exclude_external_links=<span class=\"hljs-literal\">True</span>,\n        exclude_social_media_links=<span class=\"hljs-literal\">True</span>,\n        exclude_domains=[<span class=\"hljs-string\">\"ads.com\"</span>, <span class=\"hljs-string\">\"spammytrackers.net\"</span>],\n        exclude_external_images=<span class=\"hljs-literal\">True</span>,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(url=<span class=\"hljs-string\">\"https://news.ycombinator.com\"</span>, config=config)\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Cleaned HTML length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.cleaned_html))\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Note</strong>: If these parameters remove too much, reduce or disable them accordingly.</p>\n<hr>\n<h2 id=\"3-handling-iframes\">3. Handling Iframes</h2>\n<p>Some sites embed content in <code>&lt;iframe&gt;</code> tags. If you want that inline:\n</p><div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-sql\">config <span class=\"hljs-operator\">=</span> CrawlerRunConfig(\n    # <span class=\"hljs-keyword\">Merge</span> iframe content <span class=\"hljs-keyword\">into</span> the <span class=\"hljs-keyword\">final</span> output\n    process_iframes<span class=\"hljs-operator\">=</span><span class=\"hljs-literal\">True</span>,    \n    remove_overlay_elements<span class=\"hljs-operator\">=</span><span class=\"hljs-literal\">True</span>\n)\n</code></pre></div><p></p>\n<p><strong>Usage</strong>:\n</p><div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    config = CrawlerRunConfig(\n        process_iframes=<span class=\"hljs-literal\">True</span>,\n        remove_overlay_elements=<span class=\"hljs-literal\">True</span>\n    )\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://example.org/iframe-demo\"</span>, \n            config=config\n        )\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Iframe-merged length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.cleaned_html))\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div><p></p>\n<hr>\n<h2 id=\"4-structured-extraction-examples\">4. Structured Extraction Examples</h2>\n<p>You can combine content selection with a more advanced extraction strategy. For instance, a <strong>CSS-based</strong> or <strong>LLM-based</strong> extraction strategy can run on the filtered HTML.</p>\n<h3 id=\"41-pattern-based-with-jsoncssextractionstrategy\">4.1 Pattern-Based with <code>JsonCssExtractionStrategy</code></h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> JsonCssExtractionStrategy\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-comment\"># Minimal schema for repeated items</span>\n    schema = {\n        <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"News Items\"</span>,\n        <span class=\"hljs-string\">\"baseSelector\"</span>: <span class=\"hljs-string\">\"tr.athing\"</span>,\n        <span class=\"hljs-string\">\"fields\"</span>: [\n            {<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"title\"</span>, <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\"a.storylink\"</span>, <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>},\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"link\"</span>, \n                <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\"a.storylink\"</span>, \n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"attribute\"</span>, \n                <span class=\"hljs-string\">\"attribute\"</span>: <span class=\"hljs-string\">\"href\"</span>\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        <span class=\"hljs-comment\"># Content filtering</span>\n        excluded_tags=[<span class=\"hljs-string\">\"form\"</span>, <span class=\"hljs-string\">\"header\"</span>],\n        exclude_domains=[<span class=\"hljs-string\">\"adsite.com\"</span>],\n\n        <span class=\"hljs-comment\"># CSS selection or entire page</span>\n        css_selector=<span class=\"hljs-string\">\"table.itemlist\"</span>,\n\n        <span class=\"hljs-comment\"># No caching for demonstration</span>\n        cache_mode=CacheMode.BYPASS,\n\n        <span class=\"hljs-comment\"># Extraction strategy</span>\n        extraction_strategy=JsonCssExtractionStrategy(schema)\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://news.ycombinator.com/newest\"</span>, \n            config=config\n        )\n        data = json.loads(result.extracted_content)\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Sample extracted item:\"</span>, data[:<span class=\"hljs-number\">1</span>])  <span class=\"hljs-comment\"># Show first item</span>\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<h3 id=\"42-llm-based-extraction\">4.2 LLM-Based Extraction</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">from</span> pydantic <span class=\"hljs-keyword\">import</span> BaseModel, Field\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> LLMExtractionStrategy\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">ArticleData</span>(<span class=\"hljs-title class_ inherited__\">BaseModel</span>):\n    headline: <span class=\"hljs-built_in\">str</span>\n    summary: <span class=\"hljs-built_in\">str</span>\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    llm_strategy = LLMExtractionStrategy(\n        provider=<span class=\"hljs-string\">\"openai/gpt-4\"</span>,\n        api_token=<span class=\"hljs-string\">\"sk-YOUR_API_KEY\"</span>,\n        schema=ArticleData.schema(),\n        extraction_type=<span class=\"hljs-string\">\"schema\"</span>,\n        instruction=<span class=\"hljs-string\">\"Extract 'headline' and a short 'summary' from the content.\"</span>\n    )\n\n    config = CrawlerRunConfig(\n        exclude_external_links=<span class=\"hljs-literal\">True</span>,\n        word_count_threshold=<span class=\"hljs-number\">20</span>,\n        extraction_strategy=llm_strategy\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(url=<span class=\"hljs-string\">\"https://news.ycombinator.com\"</span>, config=config)\n        article = json.loads(result.extracted_content)\n        <span class=\"hljs-built_in\">print</span>(article)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p>Here, the crawler:</p>\n<ul>\n<li>Filters out external links (<code>exclude_external_links=True</code>).  </li>\n<li>Ignores very short text blocks (<code>word_count_threshold=20</code>).  </li>\n<li>Passes the final HTML to your LLM strategy for an AI-driven parse.</li>\n</ul>\n<hr>\n<h2 id=\"5-comprehensive-example\">5. Comprehensive Example</h2>\n<p>Below is a short function that unifies <strong>CSS selection</strong>, <strong>exclusion</strong> logic, and a pattern-based extraction, demonstrating how you can fine-tune your final data:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> JsonCssExtractionStrategy\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">extract_main_articles</span>(<span class=\"hljs-params\">url: <span class=\"hljs-built_in\">str</span></span>):\n    schema = {\n        <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"ArticleBlock\"</span>,\n        <span class=\"hljs-string\">\"baseSelector\"</span>: <span class=\"hljs-string\">\"div.article-block\"</span>,\n        <span class=\"hljs-string\">\"fields\"</span>: [\n            {<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"headline\"</span>, <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\"h2\"</span>, <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>},\n            {<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"summary\"</span>, <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\".summary\"</span>, <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>},\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"metadata\"</span>,\n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"nested\"</span>,\n                <span class=\"hljs-string\">\"fields\"</span>: [\n                    {<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"author\"</span>, <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\".author\"</span>, <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>},\n                    {<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"date\"</span>, <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\".date\"</span>, <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>}\n                ]\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        <span class=\"hljs-comment\"># Keep only #main-content</span>\n        css_selector=<span class=\"hljs-string\">\"#main-content\"</span>,\n\n        <span class=\"hljs-comment\"># Filtering</span>\n        word_count_threshold=<span class=\"hljs-number\">10</span>,\n        excluded_tags=[<span class=\"hljs-string\">\"nav\"</span>, <span class=\"hljs-string\">\"footer\"</span>],  \n        exclude_external_links=<span class=\"hljs-literal\">True</span>,\n        exclude_domains=[<span class=\"hljs-string\">\"somebadsite.com\"</span>],\n        exclude_external_images=<span class=\"hljs-literal\">True</span>,\n\n        <span class=\"hljs-comment\"># Extraction</span>\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n\n        cache_mode=CacheMode.BYPASS\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(url=url, config=config)\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> result.success:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Error: <span class=\"hljs-subst\">{result.error_message}</span>\"</span>)\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">None</span>\n        <span class=\"hljs-keyword\">return</span> json.loads(result.extracted_content)\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    articles = <span class=\"hljs-keyword\">await</span> extract_main_articles(<span class=\"hljs-string\">\"https://news.ycombinator.com/newest\"</span>)\n    <span class=\"hljs-keyword\">if</span> articles:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Extracted Articles:\"</span>, articles[:<span class=\"hljs-number\">2</span>])  <span class=\"hljs-comment\"># Show first 2</span>\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Why This Works</strong>:\n- <strong>CSS</strong> scoping with <code>#main-content</code>.<br>\n- Multiple <strong>exclude_</strong> parameters to remove domains, external images, etc.<br>\n- A <strong>JsonCssExtractionStrategy</strong> to parse repeated article blocks.</p>\n<hr>\n<h2 id=\"6-scraping-modes\">6. Scraping Modes</h2>\n<p>Crawl4AI provides two different scraping strategies for HTML content processing: <code>WebScrapingStrategy</code> (BeautifulSoup-based, default) and <code>LXMLWebScrapingStrategy</code> (LXML-based). The LXML strategy offers significantly better performance, especially for large HTML documents.</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-csharp\"><span class=\"hljs-keyword\">from</span> crawl4ai import AsyncWebCrawler, CrawlerRunConfig, <span class=\"hljs-function\">LXMLWebScrapingStrategy\n\n<span class=\"hljs-keyword\">async</span> def <span class=\"hljs-title\">main</span>():\n    config</span> = CrawlerRunConfig(\n        scraping_strategy=LXMLWebScrapingStrategy()  <span class=\"hljs-meta\"># Faster alternative to default BeautifulSoup</span>\n    )\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> <span class=\"hljs-title\">AsyncWebCrawler</span>() <span class=\"hljs-keyword\">as</span> crawler:\n        result</span> = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://example.com\"</span>, \n            config=config\n        )\n</code></pre></div>\n<p>You can also create your own custom scraping strategy by inheriting from <code>ContentScrapingStrategy</code>. The strategy must return a <code>ScrapingResult</code> object with the following structure:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CustomScrapingStrategy</span>(<span class=\"hljs-title class_ inherited__\">ContentScrapingStrategy</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">scrap</span>(<span class=\"hljs-params\">self, url: <span class=\"hljs-built_in\">str</span>, html: <span class=\"hljs-built_in\">str</span>, **kwargs</span>) -&gt; ScrapingResult:\n        <span class=\"hljs-comment\"># Implement your custom scraping logic here</span>\n        <span class=\"hljs-keyword\">return</span> ScrapingResult(\n            cleaned_html=<span class=\"hljs-string\">\"&lt;html&gt;...&lt;/html&gt;\"</span>,  <span class=\"hljs-comment\"># Cleaned HTML content</span>\n            success=<span class=\"hljs-literal\">True</span>,                     <span class=\"hljs-comment\"># Whether scraping was successful</span>\n            media=Media(\n                images=[                      <span class=\"hljs-comment\"># List of images found</span>\n                    MediaItem(\n                        src=<span class=\"hljs-string\">\"https://example.com/image.jpg\"</span>,\n                        alt=<span class=\"hljs-string\">\"Image description\"</span>,\n                        desc=<span class=\"hljs-string\">\"Surrounding text\"</span>,\n                        score=<span class=\"hljs-number\">1</span>,\n                        <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-string\">\"image\"</span>,\n                        group_id=<span class=\"hljs-number\">1</span>,\n                        <span class=\"hljs-built_in\">format</span>=<span class=\"hljs-string\">\"jpg\"</span>,\n                        width=<span class=\"hljs-number\">800</span>\n                    )\n                ],\n                videos=[],                    <span class=\"hljs-comment\"># List of videos (same structure as images)</span>\n                audios=[]                     <span class=\"hljs-comment\"># List of audio files (same structure as images)</span>\n            ),\n            links=Links(\n                internal=[                    <span class=\"hljs-comment\"># List of internal links</span>\n                    Link(\n                        href=<span class=\"hljs-string\">\"https://example.com/page\"</span>,\n                        text=<span class=\"hljs-string\">\"Link text\"</span>,\n                        title=<span class=\"hljs-string\">\"Link title\"</span>,\n                        base_domain=<span class=\"hljs-string\">\"example.com\"</span>\n                    )\n                ],\n                external=[]                   <span class=\"hljs-comment\"># List of external links (same structure)</span>\n            ),\n            metadata={                        <span class=\"hljs-comment\"># Additional metadata</span>\n                <span class=\"hljs-string\">\"title\"</span>: <span class=\"hljs-string\">\"Page Title\"</span>,\n                <span class=\"hljs-string\">\"description\"</span>: <span class=\"hljs-string\">\"Page description\"</span>\n            }\n        )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">ascrap</span>(<span class=\"hljs-params\">self, url: <span class=\"hljs-built_in\">str</span>, html: <span class=\"hljs-built_in\">str</span>, **kwargs</span>) -&gt; ScrapingResult:\n        <span class=\"hljs-comment\"># For simple cases, you can use the sync version</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-keyword\">await</span> asyncio.to_thread(self.scrap, url, html, **kwargs)\n</code></pre></div>\n<h3 id=\"performance-considerations\">Performance Considerations</h3>\n<p>The LXML strategy can be up to 10-20x faster than BeautifulSoup strategy, particularly when processing large HTML documents. However, please note:</p>\n<ol>\n<li>LXML strategy is currently experimental</li>\n<li>In some edge cases, the parsing results might differ slightly from BeautifulSoup</li>\n<li>If you encounter any inconsistencies between LXML and BeautifulSoup results, please <a href=\"https://github.com/codeium/crawl4ai/issues\">raise an issue</a> with a reproducible example</li>\n</ol>\n<p>Choose LXML strategy when:\n- Processing large HTML documents (recommended for &gt;100KB)\n- Performance is critical\n- Working with well-formed HTML</p>\n<p>Stick to BeautifulSoup strategy (default) when:\n- Maximum compatibility is needed\n- Working with malformed HTML\n- Exact parsing behavior is critical</p>\n<hr>\n<h2 id=\"7-conclusion\">7. Conclusion</h2>\n<p>By mixing <strong>css_selector</strong> scoping, <strong>content filtering</strong> parameters, and advanced <strong>extraction strategies</strong>, you can precisely <strong>choose</strong> which data to keep. Key parameters in <strong><code>CrawlerRunConfig</code></strong> for content selection include:</p>\n<p>1.â€€<strong><code>css_selector</code></strong> â€“ Basic scoping to an element or region.<br>\n2.â€€<strong><code>word_count_threshold</code></strong> â€“ Skip short blocks.<br>\n3.â€€<strong><code>excluded_tags</code></strong> â€“ Remove entire HTML tags.<br>\n4.â€€<strong><code>exclude_external_links</code></strong>, <strong><code>exclude_social_media_links</code></strong>, <strong><code>exclude_domains</code></strong> â€“ Filter out unwanted links or domains.<br>\n5.â€€<strong><code>exclude_external_images</code></strong> â€“ Remove images from external sources.<br>\n6.â€€<strong><code>process_iframes</code></strong> â€“ Merge iframe content if needed.  </p>\n<p>Combine these with structured extraction (CSS, LLM-based, or others) to build powerful crawls that yield exactly the content you want, from raw or cleaned HTML up to sophisticated JSON structures. For more detail, see <a href=\"../../api/parameters/\">Configuration Reference</a>. Enjoy curating your data to the max!</p>\n</section>\n\n            </main>\n        </div>\n        <hr><footer>\n    <div class=\"terminal-mkdocs-footer-grid\">\n        <div id=\"terminal-mkdocs-footer-copyright-info\">\n             Site built with <a href=\"http://www.mkdocs.org\">MkDocs</a> and <a href=\"https://github.com/ntno/mkdocs-terminal\">Terminal for MkDocs</a>.\n        </div>\n    </div>\n</footer>\n    </div>\n\n    \n    <div class=\"modal\" id=\"mkdocs_search_modal\" tabindex=\"-1\" role=\"alertdialog\" aria-modal=\"true\" aria-labelledby=\"searchModalLabel\">\n    <div class=\"modal-dialog modal-lg\" role=\"search\">\n        <div class=\"modal-content\">\n            <div class=\"modal-header\">\n                <h5 class=\"modal-title\" id=\"searchModalLabel\">Search</h5>\n                <button type=\"button\" class=\"close btn btn-default btn-ghost\" data-dismiss=\"modal\"><span aria-hidden=\"true\">x</span><span class=\"sr-only\">Close</span></button>\n            </div>\n            <div class=\"modal-body\">\n                <p id=\"searchInputLabel\">Type to start searching</p>\n                <form>\n                    <div class=\"form-group\">\n                        <input type=\"search\" class=\"form-control\" aria-labelledby=\"searchInputLabel\" placeholder=\"\" id=\"mkdocs-search-query\" title=\"Please enter search terms here\">\n                    </div>\n                </form>\n                <div id=\"mkdocs-search-results\" data-no-results-text=\"No document matches found\"></div>\n            </div>\n            <div class=\"modal-footer\">\n            </div>\n        </div>\n    </div>\n</div>\n    \n    \n\n\n</body></html>",
  "markdown": "# Content Selection\nCrawl4AI provides multiple ways to **select** , **filter** , and **refine** the content from your crawls. Whether you need to target a specific CSS region, exclude entire tags, filter out external links, or remove certain domains and images, **`CrawlerRunConfig`**offers a wide range of parameters.\nBelow, we show how to configure these parameters and combine them for precise control.\n## 1. CSS-Based Selection\nA straightforward way to **limit** your crawl results to a certain region of the page is **`css_selector`**in**`CrawlerRunConfig`**:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n  config = CrawlerRunConfig(\n    # e.g., first 30 items from Hacker News\n    css_selector=\".athing:nth-child(-n+30)\" \n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      url=\"https://news.ycombinator.com/newest\", \n      config=config\n    )\n    print(\"Partial HTML length:\", len(result.cleaned_html))\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Result** : Only elements matching that selector remain in `result.cleaned_html`.\n## 2. Content Filtering & Exclusions\n### 2.1 Basic Overview\n```\nconfig = CrawlerRunConfig(\n  # Content thresholds\n  word_count_threshold=10,    # Minimum words per block\n  # Tag exclusions\n  excluded_tags=['form', 'header', 'footer', 'nav'],\n  # Link filtering\n  exclude_external_links=True,  \n  exclude_social_media_links=True,\n  # Block entire domains\n  exclude_domains=[\"adtrackers.com\", \"spammynews.org\"],  \n  exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],\n  # Media filtering\n  exclude_external_images=True\n)\n\n```\n\n**Explanation** :\n  * **`word_count_threshold`**: Ignores text blocks under X words. Helps skip trivial blocks like short nav or disclaimers.\n  * **`excluded_tags`**: Removes entire tags (`<form>` , `<header>`, `<footer>`, etc.). \n  * **Link Filtering** : \n  * `exclude_external_links`: Strips out external links and may remove them from `result.links`. \n  * `exclude_social_media_links`: Removes links pointing to known social media domains. \n  * `exclude_domains`: A custom list of domains to block if discovered in links. \n  * `exclude_social_media_domains`: A curated list (override or add to it) for social media sites. \n  * **Media Filtering** : \n  * `exclude_external_images`: Discards images not hosted on the same domain as the main page (or its subdomains).\n\n\nBy default in case you set `exclude_social_media_links=True`, the following social media domains are excluded: \n```\n[\n  'facebook.com',\n  'twitter.com',\n  'x.com',\n  'linkedin.com',\n  'instagram.com',\n  'pinterest.com',\n  'tiktok.com',\n  'snapchat.com',\n  'reddit.com',\n]\n\n```\n\n### 2.2 Example Usage\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nasync def main():\n  config = CrawlerRunConfig(\n    css_selector=\"main.content\", \n    word_count_threshold=10,\n    excluded_tags=[\"nav\", \"footer\"],\n    exclude_external_links=True,\n    exclude_social_media_links=True,\n    exclude_domains=[\"ads.com\", \"spammytrackers.net\"],\n    exclude_external_images=True,\n    cache_mode=CacheMode.BYPASS\n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n    print(\"Cleaned HTML length:\", len(result.cleaned_html))\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Note** : If these parameters remove too much, reduce or disable them accordingly.\n## 3. Handling Iframes\nSome sites embed content in `<iframe>` tags. If you want that inline: \n```\nconfig = CrawlerRunConfig(\n  # Merge iframe content into the final output\n  process_iframes=True,  \n  remove_overlay_elements=True\n)\n\n```\n\n**Usage** : \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nasync def main():\n  config = CrawlerRunConfig(\n    process_iframes=True,\n    remove_overlay_elements=True\n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      url=\"https://example.org/iframe-demo\", \n      config=config\n    )\n    print(\"Iframe-merged length:\", len(result.cleaned_html))\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n## 4. Structured Extraction Examples\nYou can combine content selection with a more advanced extraction strategy. For instance, a **CSS-based** or **LLM-based** extraction strategy can run on the filtered HTML.\n### 4.1 Pattern-Based with `JsonCssExtractionStrategy`\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nasync def main():\n  # Minimal schema for repeated items\n  schema = {\n    \"name\": \"News Items\",\n    \"baseSelector\": \"tr.athing\",\n    \"fields\": [\n      {\"name\": \"title\", \"selector\": \"a.storylink\", \"type\": \"text\"},\n      {\n        \"name\": \"link\", \n        \"selector\": \"a.storylink\", \n        \"type\": \"attribute\", \n        \"attribute\": \"href\"\n      }\n    ]\n  }\n  config = CrawlerRunConfig(\n    # Content filtering\n    excluded_tags=[\"form\", \"header\"],\n    exclude_domains=[\"adsite.com\"],\n    # CSS selection or entire page\n    css_selector=\"table.itemlist\",\n    # No caching for demonstration\n    cache_mode=CacheMode.BYPASS,\n    # Extraction strategy\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      url=\"https://news.ycombinator.com/newest\", \n      config=config\n    )\n    data = json.loads(result.extracted_content)\n    print(\"Sample extracted item:\", data[:1]) # Show first item\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n### 4.2 LLM-Based Extraction\n```\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nclass ArticleData(BaseModel):\n  headline: str\n  summary: str\nasync def main():\n  llm_strategy = LLMExtractionStrategy(\n    provider=\"openai/gpt-4\",\n    api_token=\"sk-YOUR_API_KEY\",\n    schema=ArticleData.schema(),\n    extraction_type=\"schema\",\n    instruction=\"Extract 'headline' and a short 'summary' from the content.\"\n  )\n  config = CrawlerRunConfig(\n    exclude_external_links=True,\n    word_count_threshold=20,\n    extraction_strategy=llm_strategy\n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n    article = json.loads(result.extracted_content)\n    print(article)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\nHere, the crawler:\n  * Filters out external links (`exclude_external_links=True`). \n  * Ignores very short text blocks (`word_count_threshold=20`). \n  * Passes the final HTML to your LLM strategy for an AI-driven parse.\n\n\n## 5. Comprehensive Example\nBelow is a short function that unifies **CSS selection** , **exclusion** logic, and a pattern-based extraction, demonstrating how you can fine-tune your final data:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nasync def extract_main_articles(url: str):\n  schema = {\n    \"name\": \"ArticleBlock\",\n    \"baseSelector\": \"div.article-block\",\n    \"fields\": [\n      {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n      {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n      {\n        \"name\": \"metadata\",\n        \"type\": \"nested\",\n        \"fields\": [\n          {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n          {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n        ]\n      }\n    ]\n  }\n  config = CrawlerRunConfig(\n    # Keep only #main-content\n    css_selector=\"#main-content\",\n    # Filtering\n    word_count_threshold=10,\n    excluded_tags=[\"nav\", \"footer\"], \n    exclude_external_links=True,\n    exclude_domains=[\"somebadsite.com\"],\n    exclude_external_images=True,\n    # Extraction\n    extraction_strategy=JsonCssExtractionStrategy(schema),\n    cache_mode=CacheMode.BYPASS\n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(url=url, config=config)\n    if not result.success:\n      print(f\"Error: {result.error_message}\")\n      return None\n    return json.loads(result.extracted_content)\nasync def main():\n  articles = await extract_main_articles(\"https://news.ycombinator.com/newest\")\n  if articles:\n    print(\"Extracted Articles:\", articles[:2]) # Show first 2\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Why This Works** : - **CSS** scoping with `#main-content`. - Multiple **exclude_** parameters to remove domains, external images, etc. - A **JsonCssExtractionStrategy** to parse repeated article blocks.\n## 6. Scraping Modes\nCrawl4AI provides two different scraping strategies for HTML content processing: `WebScrapingStrategy` (BeautifulSoup-based, default) and `LXMLWebScrapingStrategy` (LXML-based). The LXML strategy offers significantly better performance, especially for large HTML documents.\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LXMLWebScrapingStrategy\nasync def main():\n  config = CrawlerRunConfig(\n    scraping_strategy=LXMLWebScrapingStrategy() # Faster alternative to default BeautifulSoup\n  )\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      url=\"https://example.com\", \n      config=config\n    )\n\n```\n\nYou can also create your own custom scraping strategy by inheriting from `ContentScrapingStrategy`. The strategy must return a `ScrapingResult` object with the following structure:\n```\nfrom crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links\nclass CustomScrapingStrategy(ContentScrapingStrategy):\n  def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n    # Implement your custom scraping logic here\n    return ScrapingResult(\n      cleaned_html=\"<html>...</html>\", # Cleaned HTML content\n      success=True,           # Whether scraping was successful\n      media=Media(\n        images=[           # List of images found\n          MediaItem(\n            src=\"https://example.com/image.jpg\",\n            alt=\"Image description\",\n            desc=\"Surrounding text\",\n            score=1,\n            type=\"image\",\n            group_id=1,\n            format=\"jpg\",\n            width=800\n          )\n        ],\n        videos=[],          # List of videos (same structure as images)\n        audios=[]           # List of audio files (same structure as images)\n      ),\n      links=Links(\n        internal=[          # List of internal links\n          Link(\n            href=\"https://example.com/page\",\n            text=\"Link text\",\n            title=\"Link title\",\n            base_domain=\"example.com\"\n          )\n        ],\n        external=[]          # List of external links (same structure)\n      ),\n      metadata={            # Additional metadata\n        \"title\": \"Page Title\",\n        \"description\": \"Page description\"\n      }\n    )\n  async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n    # For simple cases, you can use the sync version\n    return await asyncio.to_thread(self.scrap, url, html, **kwargs)\n\n```\n\n### Performance Considerations\nThe LXML strategy can be up to 10-20x faster than BeautifulSoup strategy, particularly when processing large HTML documents. However, please note:\n  1. LXML strategy is currently experimental\n  2. In some edge cases, the parsing results might differ slightly from BeautifulSoup\n  3. If you encounter any inconsistencies between LXML and BeautifulSoup results, please with a reproducible example\n\n\nChoose LXML strategy when: - Processing large HTML documents (recommended for >100KB) - Performance is critical - Working with well-formed HTML\nStick to BeautifulSoup strategy (default) when: - Maximum compatibility is needed - Working with malformed HTML - Exact parsing behavior is critical\n## 7. Conclusion\nBy mixing **css_selector** scoping, **content filtering** parameters, and advanced **extraction strategies** , you can precisely **choose** which data to keep. Key parameters in **`CrawlerRunConfig`**for content selection include:\n1. **`css_selector`**â€“ Basic scoping to an element or region. 2.**`word_count_threshold`**â€“ Skip short blocks. 3.**`excluded_tags`**â€“ Remove entire HTML tags. 4.**`exclude_external_links`**,**`exclude_social_media_links`**,**`exclude_domains`**â€“ Filter out unwanted links or domains. 5.**`exclude_external_images`**â€“ Remove images from external sources. 6.**`process_iframes`**â€“ Merge iframe content if needed.\nCombine these with structured extraction (CSS, LLM-based, or others) to build powerful crawls that yield exactly the content you want, from raw or cleaned HTML up to sophisticated JSON structures. For more detail, see [Configuration Reference](https://docs.crawl4ai.com/core/api/parameters/>). Enjoy curating your data to the max!\n##### Search\nxClose\nType to start searching\n",
  "links": [
    "https://docs.crawl4ai.com",
    "https://docs.crawl4ai.com/advanced/advanced-features",
    "https://docs.crawl4ai.com/advanced/crawl-dispatcher",
    "https://docs.crawl4ai.com/advanced/file-downloading",
    "https://docs.crawl4ai.com/advanced/hooks-auth",
    "https://docs.crawl4ai.com/advanced/identity-based-crawling",
    "https://docs.crawl4ai.com/advanced/lazy-loading",
    "https://docs.crawl4ai.com/advanced/multi-url-crawling",
    "https://docs.crawl4ai.com/advanced/proxy-security",
    "https://docs.crawl4ai.com/advanced/session-management",
    "https://docs.crawl4ai.com/advanced/ssl-certificate",
    "https://docs.crawl4ai.com/api/arun",
    "https://docs.crawl4ai.com/api/arun_many",
    "https://docs.crawl4ai.com/api/async-webcrawler",
    "https://docs.crawl4ai.com/api/crawl-result",
    "https://docs.crawl4ai.com/api/parameters",
    "https://docs.crawl4ai.com/api/strategies",
    "https://docs.crawl4ai.com/blog",
    "https://docs.crawl4ai.com/browser-crawler-config",
    "https://docs.crawl4ai.com/cache-modes",
    "https://docs.crawl4ai.com/crawler-result",
    "https://docs.crawl4ai.com/docker-deploymeny",
    "https://docs.crawl4ai.com/extraction/chunking",
    "https://docs.crawl4ai.com/extraction/clustring-strategies",
    "https://docs.crawl4ai.com/extraction/llm-strategies",
    "https://docs.crawl4ai.com/extraction/no-llm-strategies",
    "https://docs.crawl4ai.com/fit-markdown",
    "https://docs.crawl4ai.com/installation",
    "https://docs.crawl4ai.com/link-media",
    "https://docs.crawl4ai.com/local-files",
    "https://docs.crawl4ai.com/markdown-generation",
    "https://docs.crawl4ai.com/page-interaction",
    "https://docs.crawl4ai.com/quickstart",
    "https://docs.crawl4ai.com/simple-crawling"
  ],
  "depth": 1,
  "stats": {
    "processed": 16,
    "total": 0,
    "depth": 1,
    "elapsed": "0:00:20",
    "page_limit": 34
  }
}