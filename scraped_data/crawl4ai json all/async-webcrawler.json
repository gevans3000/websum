{
  "url": "https://docs.crawl4ai.com/api/async-webcrawler",
  "timestamp": "2025-02-06T13:23:28.275614",
  "html": "<!DOCTYPE html><html lang=\"en\" style=\"scroll-padding-top: 50px;\"><head>\n    \n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n    <meta name=\"generator\" content=\"mkdocs-1.6.0, mkdocs-terminal-4.4.0\">\n    \n    <meta name=\"description\" content=\"ðŸš€ðŸ¤– Crawl4AI, Open-source LLM-Friendly Web Crawler &amp; Scraper\"> \n     \n    \n    <link rel=\"canonical\" href=\"https://docs.crawl4ai.com/api/async-webcrawler/\"><link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"../../img/android-chrome-192x192.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"512x512\" href=\"../../img/android-chrome-512x512.png\">\n<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"../../img/apple-touch-icon.png\">\n<link rel=\"shortcut icon\" type=\"image/png\" sizes=\"48x48\" href=\"../../img/favicon.ico\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"../../img/favicon-16x16.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"../../img/favicon-32x32.png\">\n\n\n    \n \n<title>AsyncWebCrawler - Crawl4AI Documentation (v0.4.3bx)</title>\n\n\n<link href=\"../../css/fontawesome/css/fontawesome.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/fontawesome/css/solid.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/normalize.css\" rel=\"stylesheet\">\n<link href=\"../../css/terminal.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.tile_grid.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.footer.css\" rel=\"stylesheet\">\n<!-- dark color palette -->\n<link href=\"../../css/palettes/dark.css\" rel=\"stylesheet\">\n\n<!-- page layout -->\n<style>\n/* initially set page layout to a one column grid */\n.terminal-mkdocs-main-grid {\n    display: grid;\n    grid-column-gap: 1.4em;\n    grid-template-columns: auto;\n    grid-template-rows: auto;\n}\n\n/*  \n*   when side navigation is not hidden, use a two column grid.  \n*   if the screen is too narrow, fall back to the initial one column grid layout.\n*   in this case the main content will be placed under the navigation panel. \n*/\n@media only screen and (min-width: 70em) {\n    .terminal-mkdocs-main-grid {\n        grid-template-columns: 4fr 9fr;\n    }\n}</style>\n\n\n\n    \n    <link href=\"../../assets/styles.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/highlight.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/dmvendor.css\" rel=\"stylesheet\">  \n    \n    \n\n    \n    <!-- search css support -->\n<link href=\"../../css/search/bootstrap-modal.css\" rel=\"stylesheet\">\n<!-- search scripts -->\n<script>\n    var base_url = \"../..\",\n    shortcuts = \"{}\";\n</script>\n<script src=\"../../js/jquery/jquery-1.10.1.min.js\" defer=\"\"></script>\n<script src=\"../../js/bootstrap/bootstrap.min.js\" defer=\"\"></script>\n<script src=\"../../js/mkdocs/base.js\" defer=\"\"></script>\n    \n    \n    \n    \n    <script src=\"../../assets/highlight.min.js\"></script>\n    \n    <script src=\"../../assets/highlight_init.js\"></script>\n    \n    <script src=\"https://buttons.github.io/buttons.js\"></script>\n    \n    <script src=\"../../search/main.js\"></script>\n    \n\n    \n</head>\n\n<body class=\"terminal\" style=\"\"><div class=\"container\">\n    <div class=\"terminal-nav\">\n        <header class=\"terminal-logo\">\n            <div id=\"mkdocs-terminal-site-name\" class=\"logo terminal-prompt\"><a href=\"https://docs.crawl4ai.com/\" class=\"no-style\">Crawl4AI Documentation (v0.4.3bx)</a></div>\n        </header>\n        \n        <nav class=\"terminal-menu\">\n            \n            <ul vocab=\"https://schema.org/\" typeof=\"BreadcrumbList\">\n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../..\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Home</span>\n                    </a>\n                    <meta property=\"position\" content=\"0\">\n                </li>\n                \n                \n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../../core/quickstart/\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Quick Start</span>\n                    </a>\n                    <meta property=\"position\" content=\"1\">\n                </li>\n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                    \n                    \n\n\n<li property=\"itemListElement\" typeof=\"ListItem\">\n    <a href=\"#\" class=\"menu-item\" data-toggle=\"modal\" data-target=\"#mkdocs_search_modal\" property=\"item\" typeof=\"SearchAction\">\n        <i aria-hidden=\"true\" class=\"fa fa-search\"></i> <span property=\"name\">Search</span>\n    </a>\n    <meta property=\"position\" content=\"2\">\n</li>\n                    \n            </ul>\n            \n        </nav>\n    </div>\n</div>\n        \n    <div class=\"container\">\n        <div class=\"terminal-mkdocs-main-grid\"><aside id=\"terminal-mkdocs-side-panel\"><nav>\n  \n    <ul class=\"terminal-mkdocs-side-nav-items\">\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../..\">Home</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Setup &amp; Installation</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/installation/\">Installation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/docker-deploymeny/\">Docker Deployment</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/quickstart/\">Quick Start</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Blog &amp; Changelog</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../blog/\">Blog Home</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md\">Changelog</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Core</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/simple-crawling/\">Simple Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/crawler-result/\">Crawler Result</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/browser-crawler-config/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/markdown-generation/\">Markdown Generation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/fit-markdown/\">Fit Markdown</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/page-interaction/\">Page Interaction</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/content-selection/\">Content Selection</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/cache-modes/\">Cache Modes</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/local-files/\">Local Files &amp; Raw HTML</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/link-media/\">Link &amp; Media</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Advanced</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/advanced-features/\">Overview</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/file-downloading/\">File Downloading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/lazy-loading/\">Lazy Loading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/hooks-auth/\">Hooks &amp; Auth</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/proxy-security/\">Proxy &amp; Security</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/session-management/\">Session Management</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/multi-url-crawling/\">Multi-URL Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/crawl-dispatcher/\">Crawl Dispatcher</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/identity-based-crawling/\">Identity Based Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/ssl-certificate/\">SSL Certificate</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Extraction</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/no-llm-strategies/\">LLM-Free Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/llm-strategies/\">LLM Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/clustring-strategies/\">Clustering Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/chunking/\">Chunking</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item--active terminal-mkdocs-side-nav-section-no-index\">API Reference</span>\n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        <span class=\"\n\n    terminal-mkdocs-side-nav-item--active\">AsyncWebCrawler</span>\n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../arun/\">arun()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../arun_many/\">arun_many()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../parameters/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../crawl-result/\">CrawlResult</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../strategies/\">Strategies</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n    </ul>\n  \n</nav><hr>\n<nav>\n    <ul>\n        <li><a href=\"#asyncwebcrawler\">AsyncWebCrawler</a></li>\n        <li><a href=\"#1-constructor-overview\">1. Constructor Overview</a></li><li><a href=\"#2-lifecycle-startclose-or-context-manager\">2. Lifecycle: Start/Close or Context Manager</a></li><li><a href=\"#3-primary-method-arun\">3. Primary Method: arun()</a></li><li><a href=\"#4-batch-processing-arun_many\">4. Batch Processing: arun_many()</a></li><li><a href=\"#5-crawlresult-output\">5. CrawlResult Output</a></li><li><a href=\"#6-quick-example\">6. Quick Example</a></li><li><a href=\"#7-best-practices-migration-notes\">7. Best Practices &amp; Migration Notes</a></li><li><a href=\"#8-summary\">8. Summary</a></li>\n    </ul>\n</nav>\n</aside>\n            <main id=\"terminal-mkdocs-main-content\">\n<section id=\"mkdocs-terminal-content\">\n    <h1 id=\"asyncwebcrawler\">AsyncWebCrawler</h1>\n<p>The <strong><code>AsyncWebCrawler</code></strong> is the core class for asynchronous web crawling in Crawl4AI.â€€You typically create it <strong>once</strong>, optionally customize it with a <strong><code>BrowserConfig</code></strong> (e.g., headless, user agent), then <strong>run</strong> multiple <strong><code>arun()</code></strong> calls with different <strong><code>CrawlerRunConfig</code></strong> objects.</p>\n<p><strong>Recommended usage</strong>:</p>\n<p>1.â€€<strong>Create</strong> a <code>BrowserConfig</code> for global browser settings.â€€ </p>\n<p>2.â€€<strong>Instantiate</strong> <code>AsyncWebCrawler(config=browser_config)</code>.â€€ </p>\n<p>3.â€€<strong>Use</strong> the crawler in an async context manager (<code>async with</code>) or manage start/close manually.â€€ </p>\n<p>4.â€€<strong>Call</strong> <code>arun(url, config=crawler_run_config)</code> for each page you want.</p>\n<hr>\n<h2 id=\"1-constructor-overview\">1.â€€Constructor Overview</h2>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">AsyncWebCrawler</span>:\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">\n        self,\n        crawler_strategy: <span class=\"hljs-type\">Optional</span>[AsyncCrawlerStrategy] = <span class=\"hljs-literal\">None</span>,\n        config: <span class=\"hljs-type\">Optional</span>[BrowserConfig] = <span class=\"hljs-literal\">None</span>,\n        always_bypass_cache: <span class=\"hljs-built_in\">bool</span> = <span class=\"hljs-literal\">False</span>,           <span class=\"hljs-comment\"># deprecated</span>\n        always_by_pass_cache: <span class=\"hljs-type\">Optional</span>[<span class=\"hljs-built_in\">bool</span>] = <span class=\"hljs-literal\">None</span>, <span class=\"hljs-comment\"># also deprecated</span>\n        base_directory: <span class=\"hljs-built_in\">str</span> = ...,\n        thread_safe: <span class=\"hljs-built_in\">bool</span> = <span class=\"hljs-literal\">False</span>,\n        **kwargs,\n    </span>):\n        <span class=\"hljs-string\">\"\"\"\n        Create an AsyncWebCrawler instance.\n\n        Args:\n            crawler_strategy: \n                (Advanced) Provide a custom crawler strategy if needed.\n            config: \n                A BrowserConfig object specifying how the browser is set up.\n            always_bypass_cache: \n                (Deprecated) Use CrawlerRunConfig.cache_mode instead.\n            base_directory:     \n                Folder for storing caches/logs (if relevant).\n            thread_safe: \n                If True, attempts some concurrency safeguards.â€€Usually False.\n            **kwargs: \n                Additional legacy or debugging parameters.\n        \"\"\"</span>\n    )\n\n<span class=\"hljs-comment\">### Typical Initialization</span>\n\n```python\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig\n\nbrowser_cfg = BrowserConfig(\n    browser_type=<span class=\"hljs-string\">\"chromium\"</span>,\n    headless=<span class=\"hljs-literal\">True</span>,\n    verbose=<span class=\"hljs-literal\">True</span>\n)\n\ncrawler = AsyncWebCrawler(config=browser_cfg)\n</code></pre></div>\n<p><strong>Notes</strong>:</p>\n<ul>\n<li><strong>Legacy</strong> parameters like <code>always_bypass_cache</code> remain for backward compatibility, but prefer to set <strong>caching</strong> in <code>CrawlerRunConfig</code>.</li>\n</ul>\n<hr>\n<h2 id=\"2-lifecycle-startclose-or-context-manager\">2.â€€Lifecycle: Start/Close or Context Manager</h2>\n<h3 id=\"21-context-manager-recommended\">2.1 Context Manager (Recommended)</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-csharp\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> <span class=\"hljs-title\">AsyncWebCrawler</span>(<span class=\"hljs-params\">config=browser_cfg</span>) <span class=\"hljs-keyword\">as</span> crawler:\n    result</span> = <span class=\"hljs-keyword\">await</span> crawler.arun(<span class=\"hljs-string\">\"https://example.com\"</span>)\n    <span class=\"hljs-meta\"># The crawler automatically starts/closes resources</span>\n</code></pre></div>\n<p>When the <code>async with</code> block ends, the crawler cleans up (closes the browser, etc.).</p>\n<h3 id=\"22-manual-start-close\">2.2 Manual Start &amp; Close</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-csharp\">crawler = AsyncWebCrawler(config=browser_cfg)\n<span class=\"hljs-keyword\">await</span> crawler.start()\n\nresult1 = <span class=\"hljs-keyword\">await</span> crawler.arun(<span class=\"hljs-string\">\"https://example.com\"</span>)\nresult2 = <span class=\"hljs-keyword\">await</span> crawler.arun(<span class=\"hljs-string\">\"https://another.com\"</span>)\n\n<span class=\"hljs-keyword\">await</span> crawler.close()\n</code></pre></div>\n<p>Use this style if you have a <strong>long-running</strong> application or need full control of the crawlerâ€™s lifecycle.</p>\n<hr>\n<h2 id=\"3-primary-method-arun\">3.â€€Primary Method: <code>arun()</code></h2>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">arun</span>(<span class=\"hljs-params\">\n    self,\n    url: <span class=\"hljs-built_in\">str</span>,\n    config: <span class=\"hljs-type\">Optional</span>[CrawlerRunConfig] = <span class=\"hljs-literal\">None</span>,\n    <span class=\"hljs-comment\"># Legacy parameters for backward compatibility...</span>\n</span>) -&gt; CrawlResult:\n    ...\n</code></pre></div>\n<h3 id=\"31-new-approach\">3.1 New Approach</h3>\n<p>You pass a <code>CrawlerRunConfig</code> object that sets up everything about a crawlâ€”content filtering, caching, session reuse, JS code, screenshots, etc.</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> CrawlerRunConfig, CacheMode\n\nrun_cfg = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    css_selector=<span class=\"hljs-string\">\"main.article\"</span>,\n    word_count_threshold=<span class=\"hljs-number\">10</span>,\n    screenshot=<span class=\"hljs-literal\">True</span>\n)\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_cfg) <span class=\"hljs-keyword\">as</span> crawler:\n    result = <span class=\"hljs-keyword\">await</span> crawler.arun(<span class=\"hljs-string\">\"https://example.com/news\"</span>, config=run_cfg)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Crawled HTML length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.cleaned_html))\n    <span class=\"hljs-keyword\">if</span> result.screenshot:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Screenshot base64 length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.screenshot))\n</code></pre></div>\n<h3 id=\"32-legacy-parameters-still-accepted\">3.2 Legacy Parameters Still Accepted</h3>\n<p>For <strong>backward</strong> compatibility, <code>arun()</code> can still accept direct arguments like <code>css_selector=...</code>, <code>word_count_threshold=...</code>, etc., but we strongly advise migrating them into a <strong><code>CrawlerRunConfig</code></strong>.</p>\n<hr>\n<h2 id=\"4-batch-processing-arun_many\">4.â€€Batch Processing: <code>arun_many()</code></h2>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">arun_many</span>(<span class=\"hljs-params\">\n    self,\n    urls: <span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\">str</span>],\n    config: <span class=\"hljs-type\">Optional</span>[CrawlerRunConfig] = <span class=\"hljs-literal\">None</span>,\n    <span class=\"hljs-comment\"># Legacy parameters maintained for backwards compatibility...</span>\n</span>) -&gt; <span class=\"hljs-type\">List</span>[CrawlResult]:\n    <span class=\"hljs-string\">\"\"\"\n    Process multiple URLs with intelligent rate limiting and resource monitoring.\n    \"\"\"</span>\n</code></pre></div>\n<h3 id=\"41-resource-aware-crawling\">4.1 Resource-Aware Crawling</h3>\n<p>The <code>arun_many()</code> method now uses an intelligent dispatcher that:</p>\n<ul>\n<li>Monitors system memory usage</li>\n<li>Implements adaptive rate limiting</li>\n<li>Provides detailed progress monitoring</li>\n<li>Manages concurrent crawls efficiently</li>\n</ul>\n<h3 id=\"42-example-usage\">4.2 Example Usage</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig\n<span class=\"hljs-keyword\">from</span> crawl4ai.dispatcher <span class=\"hljs-keyword\">import</span> DisplayMode\n\n<span class=\"hljs-comment\"># Configure browser</span>\nbrowser_cfg = BrowserConfig(headless=<span class=\"hljs-literal\">True</span>)\n\n<span class=\"hljs-comment\"># Configure crawler with rate limiting</span>\nrun_cfg = CrawlerRunConfig(\n    <span class=\"hljs-comment\"># Enable rate limiting</span>\n    enable_rate_limiting=<span class=\"hljs-literal\">True</span>,\n    rate_limit_config=RateLimitConfig(\n        base_delay=(<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">2.0</span>),  <span class=\"hljs-comment\"># Random delay between 1-2 seconds</span>\n        max_delay=<span class=\"hljs-number\">30.0</span>,         <span class=\"hljs-comment\"># Maximum delay after rate limit hits</span>\n        max_retries=<span class=\"hljs-number\">2</span>,          <span class=\"hljs-comment\"># Number of retries before giving up</span>\n        rate_limit_codes=[<span class=\"hljs-number\">429</span>, <span class=\"hljs-number\">503</span>]  <span class=\"hljs-comment\"># Status codes that trigger rate limiting</span>\n    ),\n    <span class=\"hljs-comment\"># Resource monitoring</span>\n    memory_threshold_percent=<span class=\"hljs-number\">70.0</span>,  <span class=\"hljs-comment\"># Pause if memory exceeds this</span>\n    check_interval=<span class=\"hljs-number\">0.5</span>,            <span class=\"hljs-comment\"># How often to check resources</span>\n    max_session_permit=<span class=\"hljs-number\">3</span>,          <span class=\"hljs-comment\"># Maximum concurrent crawls</span>\n    display_mode=DisplayMode.DETAILED.value  <span class=\"hljs-comment\"># Show detailed progress</span>\n)\n\nurls = [\n    <span class=\"hljs-string\">\"https://example.com/page1\"</span>,\n    <span class=\"hljs-string\">\"https://example.com/page2\"</span>,\n    <span class=\"hljs-string\">\"https://example.com/page3\"</span>\n]\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_cfg) <span class=\"hljs-keyword\">as</span> crawler:\n    results = <span class=\"hljs-keyword\">await</span> crawler.arun_many(urls, config=run_cfg)\n    <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> results:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"URL: <span class=\"hljs-subst\">{result.url}</span>, Success: <span class=\"hljs-subst\">{result.success}</span>\"</span>)\n</code></pre></div>\n<h3 id=\"43-key-features\">4.3 Key Features</h3>\n<p>1.â€€<strong>Rate Limiting</strong></p>\n<ul>\n<li>Automatic delay between requests</li>\n<li>Exponential backoff on rate limit detection</li>\n<li>Domain-specific rate limiting</li>\n<li>Configurable retry strategy</li>\n</ul>\n<p>2.â€€<strong>Resource Monitoring</strong></p>\n<ul>\n<li>Memory usage tracking</li>\n<li>Adaptive concurrency based on system load</li>\n<li>Automatic pausing when resources are constrained</li>\n</ul>\n<p>3.â€€<strong>Progress Monitoring</strong></p>\n<ul>\n<li>Detailed or aggregated progress display</li>\n<li>Real-time status updates</li>\n<li>Memory usage statistics</li>\n</ul>\n<p>4.â€€<strong>Error Handling</strong></p>\n<ul>\n<li>Graceful handling of rate limits</li>\n<li>Automatic retries with backoff</li>\n<li>Detailed error reporting</li>\n</ul>\n<hr>\n<h2 id=\"5-crawlresult-output\">5.â€€<code>CrawlResult</code> Output</h2>\n<p>Each <code>arun()</code> returns a <strong><code>CrawlResult</code></strong> containing:</p>\n<ul>\n<li><code>url</code>: Final URL (if redirected).</li>\n<li><code>html</code>: Original HTML.</li>\n<li><code>cleaned_html</code>: Sanitized HTML.</li>\n<li><code>markdown_v2</code> (or future <code>markdown</code>): Markdown outputs (raw, fit, etc.).</li>\n<li><code>extracted_content</code>: If an extraction strategy was used (JSON for CSS/LLM strategies).</li>\n<li><code>screenshot</code>, <code>pdf</code>: If screenshots/PDF requested.</li>\n<li><code>media</code>, <code>links</code>: Information about discovered images/links.</li>\n<li><code>success</code>, <code>error_message</code>: Status info.</li>\n</ul>\n<p>For details, see <a href=\"../crawl-result/\">CrawlResult doc</a>.</p>\n<hr>\n<h2 id=\"6-quick-example\">6.â€€Quick Example</h2>\n<p>Below is an example hooking it all together:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> JsonCssExtractionStrategy\n<span class=\"hljs-keyword\">import</span> json\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-comment\"># 1.â€€Browser config</span>\n    browser_cfg = BrowserConfig(\n        browser_type=<span class=\"hljs-string\">\"firefox\"</span>,\n        headless=<span class=\"hljs-literal\">False</span>,\n        verbose=<span class=\"hljs-literal\">True</span>\n    )\n\n    <span class=\"hljs-comment\"># 2.â€€Run config</span>\n    schema = {\n        <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"Articles\"</span>,\n        <span class=\"hljs-string\">\"baseSelector\"</span>: <span class=\"hljs-string\">\"article.post\"</span>,\n        <span class=\"hljs-string\">\"fields\"</span>: [\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"title\"</span>, \n                <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\"h2\"</span>, \n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>\n            },\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"url\"</span>, \n                <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\"a\"</span>, \n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"attribute\"</span>, \n                <span class=\"hljs-string\">\"attribute\"</span>: <span class=\"hljs-string\">\"href\"</span>\n            }\n        ]\n    }\n\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        word_count_threshold=<span class=\"hljs-number\">15</span>,\n        remove_overlay_elements=<span class=\"hljs-literal\">True</span>,\n        wait_for=<span class=\"hljs-string\">\"css:.post\"</span>  <span class=\"hljs-comment\"># Wait for posts to appear</span>\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_cfg) <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://example.com/blog\"</span>,\n            config=run_cfg\n        )\n\n        <span class=\"hljs-keyword\">if</span> result.success:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Cleaned HTML length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.cleaned_html))\n            <span class=\"hljs-keyword\">if</span> result.extracted_content:\n                articles = json.loads(result.extracted_content)\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Extracted articles:\"</span>, articles[:<span class=\"hljs-number\">2</span>])\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Error:\"</span>, result.error_message)\n\nasyncio.run(main())\n</code></pre></div>\n<p><strong>Explanation</strong>:</p>\n<ul>\n<li>We define a <strong><code>BrowserConfig</code></strong> with Firefox, no headless, and <code>verbose=True</code>.â€€ </li>\n<li>We define a <strong><code>CrawlerRunConfig</code></strong> that <strong>bypasses cache</strong>, uses a <strong>CSS</strong> extraction schema, has a <code>word_count_threshold=15</code>, etc.â€€ </li>\n<li>We pass them to <code>AsyncWebCrawler(config=...)</code> and <code>arun(url=..., config=...)</code>.</li>\n</ul>\n<hr>\n<h2 id=\"7-best-practices-migration-notes\">7.â€€Best Practices &amp; Migration Notes</h2>\n<p>1.â€€<strong>Use</strong> <code>BrowserConfig</code> for <strong>global</strong> settings about the browserâ€™s environment.â€€ \n2.â€€<strong>Use</strong> <code>CrawlerRunConfig</code> for <strong>per-crawl</strong> logic (caching, content filtering, extraction strategies, wait conditions).â€€ \n3.â€€<strong>Avoid</strong> legacy parameters like <code>css_selector</code> or <code>word_count_threshold</code> directly in <code>arun()</code>.â€€Instead:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-ini\"><span class=\"hljs-attr\">run_cfg</span> = CrawlerRunConfig(css_selector=<span class=\"hljs-string\">\".main-content\"</span>, word_count_threshold=<span class=\"hljs-number\">20</span>)\n<span class=\"hljs-attr\">result</span> = await crawler.arun(url=<span class=\"hljs-string\">\"...\"</span>, config=run_cfg)\n</code></pre></div>\n<p>4.â€€<strong>Context Manager</strong> usage is simplest unless you want a persistent crawler across many calls.</p>\n<hr>\n<h2 id=\"8-summary\">8.â€€Summary</h2>\n<p><strong>AsyncWebCrawler</strong> is your entry point to asynchronous crawling:</p>\n<ul>\n<li><strong>Constructor</strong> accepts <strong><code>BrowserConfig</code></strong> (or defaults).â€€ </li>\n<li><strong><code>arun(url, config=CrawlerRunConfig)</code></strong> is the main method for single-page crawls.â€€ </li>\n<li><strong><code>arun_many(urls, config=CrawlerRunConfig)</code></strong> handles concurrency across multiple URLs.â€€ </li>\n<li>For advanced lifecycle control, use <code>start()</code> and <code>close()</code> explicitly.â€€ </li>\n</ul>\n<p><strong>Migration</strong>:  </p>\n<ul>\n<li>If you used <code>AsyncWebCrawler(browser_type=\"chromium\", css_selector=\"...\")</code>, move browser settings to <code>BrowserConfig(...)</code> and content/crawl logic to <code>CrawlerRunConfig(...)</code>.</li>\n</ul>\n<p>This modular approach ensures your code is <strong>clean</strong>, <strong>scalable</strong>, and <strong>easy to maintain</strong>.â€€For any advanced or rarely used parameters, see the <a href=\"../parameters/\">BrowserConfig docs</a>.</p>\n</section>\n\n            </main>\n        </div>\n        <hr><footer>\n    <div class=\"terminal-mkdocs-footer-grid\">\n        <div id=\"terminal-mkdocs-footer-copyright-info\">\n             Site built with <a href=\"http://www.mkdocs.org\">MkDocs</a> and <a href=\"https://github.com/ntno/mkdocs-terminal\">Terminal for MkDocs</a>.\n        </div>\n    </div>\n</footer>\n    </div>\n\n    \n    <div class=\"modal\" id=\"mkdocs_search_modal\" tabindex=\"-1\" role=\"alertdialog\" aria-modal=\"true\" aria-labelledby=\"searchModalLabel\">\n    <div class=\"modal-dialog modal-lg\" role=\"search\">\n        <div class=\"modal-content\">\n            <div class=\"modal-header\">\n                <h5 class=\"modal-title\" id=\"searchModalLabel\">Search</h5>\n                <button type=\"button\" class=\"close btn btn-default btn-ghost\" data-dismiss=\"modal\"><span aria-hidden=\"true\">x</span><span class=\"sr-only\">Close</span></button>\n            </div>\n            <div class=\"modal-body\">\n                <p id=\"searchInputLabel\">Type to start searching</p>\n                <form>\n                    <div class=\"form-group\">\n                        <input type=\"search\" class=\"form-control\" aria-labelledby=\"searchInputLabel\" placeholder=\"\" id=\"mkdocs-search-query\" title=\"Please enter search terms here\">\n                    </div>\n                </form>\n                <div id=\"mkdocs-search-results\" data-no-results-text=\"No document matches found\"></div>\n            </div>\n            <div class=\"modal-footer\">\n            </div>\n        </div>\n    </div>\n</div>\n    \n    \n\n\n</body></html>",
  "markdown": "# AsyncWebCrawler\nThe **`AsyncWebCrawler`**is the core class for asynchronous web crawling in Crawl4AI. You typically create it**once** , optionally customize it with a **`BrowserConfig`**(e.g., headless, user agent), then**run** multiple **`arun()`**calls with different**`CrawlerRunConfig`**objects.\n**Recommended usage** :\n1. **Create** a `BrowserConfig` for global browser settings. \n2. **Instantiate** `AsyncWebCrawler(config=browser_config)`. \n3. **Use** the crawler in an async context manager (`async with`) or manage start/close manually. \n4. **Call** `arun(url, config=crawler_run_config)` for each page you want.\n## 1. Constructor Overview\n```\nclass AsyncWebCrawler:\n  def __init__(\n    self,\n    crawler_strategy: Optional[AsyncCrawlerStrategy] = None,\n    config: Optional[BrowserConfig] = None,\n    always_bypass_cache: bool = False,      # deprecated\n    always_by_pass_cache: Optional[bool] = None, # also deprecated\n    base_directory: str = ...,\n    thread_safe: bool = False,\n    **kwargs,\n  ):\n    \"\"\"\n    Create an AsyncWebCrawler instance.\n    Args:\n      crawler_strategy: \n        (Advanced) Provide a custom crawler strategy if needed.\n      config: \n        A BrowserConfig object specifying how the browser is set up.\n      always_bypass_cache: \n        (Deprecated) Use CrawlerRunConfig.cache_mode instead.\n      base_directory:   \n        Folder for storing caches/logs (if relevant).\n      thread_safe: \n        If True, attempts some concurrency safeguards.â€€Usually False.\n      **kwargs: \n        Additional legacy or debugging parameters.\n    \"\"\"\n  )\n### Typical Initialization\n```python\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\nbrowser_cfg = BrowserConfig(\n  browser_type=\"chromium\",\n  headless=True,\n  verbose=True\n)\ncrawler = AsyncWebCrawler(config=browser_cfg)\n\n```\n\n**Notes** :\n  * **Legacy** parameters like `always_bypass_cache` remain for backward compatibility, but prefer to set **caching** in `CrawlerRunConfig`.\n\n\n## 2. Lifecycle: Start/Close or Context Manager\n### 2.1 Context Manager (Recommended)\n```\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n  result = await crawler.arun(\"https://example.com\")\n  # The crawler automatically starts/closes resources\n\n```\n\nWhen the `async with` block ends, the crawler cleans up (closes the browser, etc.).\n### 2.2 Manual Start & Close\n```\ncrawler = AsyncWebCrawler(config=browser_cfg)\nawait crawler.start()\nresult1 = await crawler.arun(\"https://example.com\")\nresult2 = await crawler.arun(\"https://another.com\")\nawait crawler.close()\n\n```\n\nUse this style if you have a **long-running** application or need full control of the crawlerâ€™s lifecycle.\n## 3. Primary Method: `arun()`\n```\nasync def arun(\n  self,\n  url: str,\n  config: Optional[CrawlerRunConfig] = None,\n  # Legacy parameters for backward compatibility...\n) -> CrawlResult:\n  ...\n\n```\n\n### 3.1 New Approach\nYou pass a `CrawlerRunConfig` object that sets up everything about a crawlâ€”content filtering, caching, session reuse, JS code, screenshots, etc.\n```\nimport asyncio\nfrom crawl4ai import CrawlerRunConfig, CacheMode\nrun_cfg = CrawlerRunConfig(\n  cache_mode=CacheMode.BYPASS,\n  css_selector=\"main.article\",\n  word_count_threshold=10,\n  screenshot=True\n)\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n  result = await crawler.arun(\"https://example.com/news\", config=run_cfg)\n  print(\"Crawled HTML length:\", len(result.cleaned_html))\n  if result.screenshot:\n    print(\"Screenshot base64 length:\", len(result.screenshot))\n\n```\n\n### 3.2 Legacy Parameters Still Accepted\nFor **backward** compatibility, `arun()` can still accept direct arguments like `css_selector=...`, `word_count_threshold=...`, etc., but we strongly advise migrating them into a **`CrawlerRunConfig`**.\n## 4. Batch Processing: `arun_many()`\n```\nasync def arun_many(\n  self,\n  urls: List[str],\n  config: Optional[CrawlerRunConfig] = None,\n  # Legacy parameters maintained for backwards compatibility...\n) -> List[CrawlResult]:\n  \"\"\"\n  Process multiple URLs with intelligent rate limiting and resource monitoring.\n  \"\"\"\n\n```\n\n### 4.1 Resource-Aware Crawling\nThe `arun_many()` method now uses an intelligent dispatcher that:\n  * Monitors system memory usage\n  * Implements adaptive rate limiting\n  * Provides detailed progress monitoring\n  * Manages concurrent crawls efficiently\n\n\n### 4.2 Example Usage\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig\nfrom crawl4ai.dispatcher import DisplayMode\n# Configure browser\nbrowser_cfg = BrowserConfig(headless=True)\n# Configure crawler with rate limiting\nrun_cfg = CrawlerRunConfig(\n  # Enable rate limiting\n  enable_rate_limiting=True,\n  rate_limit_config=RateLimitConfig(\n    base_delay=(1.0, 2.0), # Random delay between 1-2 seconds\n    max_delay=30.0,     # Maximum delay after rate limit hits\n    max_retries=2,     # Number of retries before giving up\n    rate_limit_codes=[429, 503] # Status codes that trigger rate limiting\n  ),\n  # Resource monitoring\n  memory_threshold_percent=70.0, # Pause if memory exceeds this\n  check_interval=0.5,      # How often to check resources\n  max_session_permit=3,     # Maximum concurrent crawls\n  display_mode=DisplayMode.DETAILED.value # Show detailed progress\n)\nurls = [\n  \"https://example.com/page1\",\n  \"https://example.com/page2\",\n  \"https://example.com/page3\"\n]\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n  results = await crawler.arun_many(urls, config=run_cfg)\n  for result in results:\n    print(f\"URL: {result.url}, Success: {result.success}\")\n\n```\n\n### 4.3 Key Features\n1. **Rate Limiting**\n  * Automatic delay between requests\n  * Exponential backoff on rate limit detection\n  * Domain-specific rate limiting\n  * Configurable retry strategy\n\n\n2. **Resource Monitoring**\n  * Memory usage tracking\n  * Adaptive concurrency based on system load\n  * Automatic pausing when resources are constrained\n\n\n3. **Progress Monitoring**\n  * Detailed or aggregated progress display\n  * Real-time status updates\n  * Memory usage statistics\n\n\n4. **Error Handling**\n  * Graceful handling of rate limits\n  * Automatic retries with backoff\n  * Detailed error reporting\n\n\n## 5. `CrawlResult` Output\nEach `arun()` returns a **`CrawlResult`**containing:\n  * `url`: Final URL (if redirected).\n  * `html`: Original HTML.\n  * `cleaned_html`: Sanitized HTML.\n  * `markdown_v2` (or future `markdown`): Markdown outputs (raw, fit, etc.).\n  * `extracted_content`: If an extraction strategy was used (JSON for CSS/LLM strategies).\n  * `screenshot`, `pdf`: If screenshots/PDF requested.\n  * `media`, `links`: Information about discovered images/links.\n  * `success`, `error_message`: Status info.\n\n\nFor details, see [CrawlResult doc](https://docs.crawl4ai.com/api/<../crawl-result/>).\n## 6. Quick Example\nBelow is an example hooking it all together:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nimport json\nasync def main():\n  # 1.â€€Browser config\n  browser_cfg = BrowserConfig(\n    browser_type=\"firefox\",\n    headless=False,\n    verbose=True\n  )\n  # 2.â€€Run config\n  schema = {\n    \"name\": \"Articles\",\n    \"baseSelector\": \"article.post\",\n    \"fields\": [\n      {\n        \"name\": \"title\", \n        \"selector\": \"h2\", \n        \"type\": \"text\"\n      },\n      {\n        \"name\": \"url\", \n        \"selector\": \"a\", \n        \"type\": \"attribute\", \n        \"attribute\": \"href\"\n      }\n    ]\n  }\n  run_cfg = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    extraction_strategy=JsonCssExtractionStrategy(schema),\n    word_count_threshold=15,\n    remove_overlay_elements=True,\n    wait_for=\"css:.post\" # Wait for posts to appear\n  )\n  async with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\n      url=\"https://example.com/blog\",\n      config=run_cfg\n    )\n    if result.success:\n      print(\"Cleaned HTML length:\", len(result.cleaned_html))\n      if result.extracted_content:\n        articles = json.loads(result.extracted_content)\n        print(\"Extracted articles:\", articles[:2])\n    else:\n      print(\"Error:\", result.error_message)\nasyncio.run(main())\n\n```\n\n**Explanation** :\n  * We define a **`BrowserConfig`**with Firefox, no headless, and`verbose=True`. \n  * We define a **`CrawlerRunConfig`**that**bypasses cache** , uses a **CSS** extraction schema, has a `word_count_threshold=15`, etc. \n  * We pass them to `AsyncWebCrawler(config=...)` and `arun(url=..., config=...)`.\n\n\n## 7. Best Practices & Migration Notes\n1. **Use** `BrowserConfig` for **global** settings about the browserâ€™s environment. 2. **Use** `CrawlerRunConfig` for **per-crawl** logic (caching, content filtering, extraction strategies, wait conditions). 3. **Avoid** legacy parameters like `css_selector` or `word_count_threshold` directly in `arun()`. Instead:\n```\nrun_cfg = CrawlerRunConfig(css_selector=\".main-content\", word_count_threshold=20)\nresult = await crawler.arun(url=\"...\", config=run_cfg)\n\n```\n\n4. **Context Manager** usage is simplest unless you want a persistent crawler across many calls.\n## 8. Summary\n**AsyncWebCrawler** is your entry point to asynchronous crawling:\n  * **Constructor** accepts **`BrowserConfig`**(or defaults).\n  * **`arun(url, config=CrawlerRunConfig)`**is the main method for single-page crawls.\n  * **`arun_many(urls, config=CrawlerRunConfig)`**handles concurrency across multiple URLs.\n  * For advanced lifecycle control, use `start()` and `close()` explicitly. \n\n\n**Migration** : \n  * If you used `AsyncWebCrawler(browser_type=\"chromium\", css_selector=\"...\")`, move browser settings to `BrowserConfig(...)` and content/crawl logic to `CrawlerRunConfig(...)`.\n\n\nThis modular approach ensures your code is **clean** , **scalable** , and **easy to maintain**. For any advanced or rarely used parameters, see the [BrowserConfig docs](https://docs.crawl4ai.com/api/<../parameters/>).\n##### Search\nxClose\nType to start searching\n",
  "links": [
    "https://docs.crawl4ai.com",
    "https://docs.crawl4ai.com/advanced/advanced-features",
    "https://docs.crawl4ai.com/advanced/crawl-dispatcher",
    "https://docs.crawl4ai.com/advanced/file-downloading",
    "https://docs.crawl4ai.com/advanced/hooks-auth",
    "https://docs.crawl4ai.com/advanced/identity-based-crawling",
    "https://docs.crawl4ai.com/advanced/lazy-loading",
    "https://docs.crawl4ai.com/advanced/multi-url-crawling",
    "https://docs.crawl4ai.com/advanced/proxy-security",
    "https://docs.crawl4ai.com/advanced/session-management",
    "https://docs.crawl4ai.com/advanced/ssl-certificate",
    "https://docs.crawl4ai.com/arun",
    "https://docs.crawl4ai.com/arun_many",
    "https://docs.crawl4ai.com/blog",
    "https://docs.crawl4ai.com/core/browser-crawler-config",
    "https://docs.crawl4ai.com/core/cache-modes",
    "https://docs.crawl4ai.com/core/content-selection",
    "https://docs.crawl4ai.com/core/crawler-result",
    "https://docs.crawl4ai.com/core/docker-deploymeny",
    "https://docs.crawl4ai.com/core/fit-markdown",
    "https://docs.crawl4ai.com/core/installation",
    "https://docs.crawl4ai.com/core/link-media",
    "https://docs.crawl4ai.com/core/local-files",
    "https://docs.crawl4ai.com/core/markdown-generation",
    "https://docs.crawl4ai.com/core/page-interaction",
    "https://docs.crawl4ai.com/core/quickstart",
    "https://docs.crawl4ai.com/core/simple-crawling",
    "https://docs.crawl4ai.com/crawl-result",
    "https://docs.crawl4ai.com/extraction/chunking",
    "https://docs.crawl4ai.com/extraction/clustring-strategies",
    "https://docs.crawl4ai.com/extraction/llm-strategies",
    "https://docs.crawl4ai.com/extraction/no-llm-strategies",
    "https://docs.crawl4ai.com/parameters",
    "https://docs.crawl4ai.com/strategies"
  ],
  "depth": 1,
  "stats": {
    "processed": 9,
    "total": 0,
    "depth": 1,
    "elapsed": "0:00:12",
    "page_limit": 34
  }
}