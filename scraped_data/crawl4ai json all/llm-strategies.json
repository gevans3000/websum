{
  "url": "https://docs.crawl4ai.com/extraction/llm-strategies",
  "timestamp": "2025-02-06T13:23:52.553522",
  "html": "<!DOCTYPE html><html lang=\"en\" style=\"scroll-padding-top: 50px;\"><head>\n    \n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n    <meta name=\"generator\" content=\"mkdocs-1.6.0, mkdocs-terminal-4.4.0\">\n    \n    <meta name=\"description\" content=\"🚀🤖 Crawl4AI, Open-source LLM-Friendly Web Crawler &amp; Scraper\"> \n     \n    \n    <link rel=\"canonical\" href=\"https://docs.crawl4ai.com/extraction/llm-strategies/\"><link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"../../img/android-chrome-192x192.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"512x512\" href=\"../../img/android-chrome-512x512.png\">\n<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"../../img/apple-touch-icon.png\">\n<link rel=\"shortcut icon\" type=\"image/png\" sizes=\"48x48\" href=\"../../img/favicon.ico\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"../../img/favicon-16x16.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"../../img/favicon-32x32.png\">\n\n\n    \n \n<title>LLM Strategies - Crawl4AI Documentation (v0.4.3bx)</title>\n\n\n<link href=\"../../css/fontawesome/css/fontawesome.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/fontawesome/css/solid.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/normalize.css\" rel=\"stylesheet\">\n<link href=\"../../css/terminal.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.tile_grid.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.footer.css\" rel=\"stylesheet\">\n<!-- dark color palette -->\n<link href=\"../../css/palettes/dark.css\" rel=\"stylesheet\">\n\n<!-- page layout -->\n<style>\n/* initially set page layout to a one column grid */\n.terminal-mkdocs-main-grid {\n    display: grid;\n    grid-column-gap: 1.4em;\n    grid-template-columns: auto;\n    grid-template-rows: auto;\n}\n\n/*  \n*   when side navigation is not hidden, use a two column grid.  \n*   if the screen is too narrow, fall back to the initial one column grid layout.\n*   in this case the main content will be placed under the navigation panel. \n*/\n@media only screen and (min-width: 70em) {\n    .terminal-mkdocs-main-grid {\n        grid-template-columns: 4fr 9fr;\n    }\n}</style>\n\n\n\n    \n    <link href=\"../../assets/styles.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/highlight.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/dmvendor.css\" rel=\"stylesheet\">  \n    \n    \n\n    \n    <!-- search css support -->\n<link href=\"../../css/search/bootstrap-modal.css\" rel=\"stylesheet\">\n<!-- search scripts -->\n<script>\n    var base_url = \"../..\",\n    shortcuts = \"{}\";\n</script>\n<script src=\"../../js/jquery/jquery-1.10.1.min.js\" defer=\"\"></script>\n<script src=\"../../js/bootstrap/bootstrap.min.js\" defer=\"\"></script>\n<script src=\"../../js/mkdocs/base.js\" defer=\"\"></script>\n    \n    \n    \n    \n    <script src=\"../../assets/highlight.min.js\"></script>\n    \n    <script src=\"../../assets/highlight_init.js\"></script>\n    \n    <script src=\"https://buttons.github.io/buttons.js\"></script>\n    \n    <script src=\"../../search/main.js\"></script>\n    \n\n    \n</head>\n\n<body class=\"terminal\" style=\"\"><div class=\"container\">\n    <div class=\"terminal-nav\">\n        <header class=\"terminal-logo\">\n            <div id=\"mkdocs-terminal-site-name\" class=\"logo terminal-prompt\"><a href=\"https://docs.crawl4ai.com/\" class=\"no-style\">Crawl4AI Documentation (v0.4.3bx)</a></div>\n        </header>\n        \n        <nav class=\"terminal-menu\">\n            \n            <ul vocab=\"https://schema.org/\" typeof=\"BreadcrumbList\">\n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../..\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Home</span>\n                    </a>\n                    <meta property=\"position\" content=\"0\">\n                </li>\n                \n                \n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../../core/quickstart/\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Quick Start</span>\n                    </a>\n                    <meta property=\"position\" content=\"1\">\n                </li>\n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                    \n                    \n\n\n<li property=\"itemListElement\" typeof=\"ListItem\">\n    <a href=\"#\" class=\"menu-item\" data-toggle=\"modal\" data-target=\"#mkdocs_search_modal\" property=\"item\" typeof=\"SearchAction\">\n        <i aria-hidden=\"true\" class=\"fa fa-search\"></i> <span property=\"name\">Search</span>\n    </a>\n    <meta property=\"position\" content=\"2\">\n</li>\n                    \n            </ul>\n            \n        </nav>\n    </div>\n</div>\n        \n    <div class=\"container\">\n        <div class=\"terminal-mkdocs-main-grid\"><aside id=\"terminal-mkdocs-side-panel\"><nav>\n  \n    <ul class=\"terminal-mkdocs-side-nav-items\">\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../..\">Home</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Setup &amp; Installation</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/installation/\">Installation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/docker-deploymeny/\">Docker Deployment</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/quickstart/\">Quick Start</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Blog &amp; Changelog</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../blog/\">Blog Home</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md\">Changelog</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Core</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/simple-crawling/\">Simple Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/crawler-result/\">Crawler Result</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/browser-crawler-config/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/markdown-generation/\">Markdown Generation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/fit-markdown/\">Fit Markdown</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/page-interaction/\">Page Interaction</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/content-selection/\">Content Selection</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/cache-modes/\">Cache Modes</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/local-files/\">Local Files &amp; Raw HTML</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/link-media/\">Link &amp; Media</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Advanced</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/advanced-features/\">Overview</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/file-downloading/\">File Downloading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/lazy-loading/\">Lazy Loading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/hooks-auth/\">Hooks &amp; Auth</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/proxy-security/\">Proxy &amp; Security</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/session-management/\">Session Management</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/multi-url-crawling/\">Multi-URL Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/crawl-dispatcher/\">Crawl Dispatcher</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/identity-based-crawling/\">Identity Based Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/ssl-certificate/\">SSL Certificate</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item--active terminal-mkdocs-side-nav-section-no-index\">Extraction</span>\n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../no-llm-strategies/\">LLM-Free Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        <span class=\"\n\n    terminal-mkdocs-side-nav-item--active\">LLM Strategies</span>\n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../clustring-strategies/\">Clustering Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../chunking/\">Chunking</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">API Reference</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/async-webcrawler/\">AsyncWebCrawler</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun/\">arun()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun_many/\">arun_many()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/parameters/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/crawl-result/\">CrawlResult</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/strategies/\">Strategies</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n    </ul>\n  \n</nav><hr>\n<nav>\n    <ul>\n        <li><a href=\"#extracting-json-llm\">Extracting JSON (LLM)</a></li>\n        <li><a href=\"#1-why-use-an-llm\">1. Why Use an LLM?</a></li><li><a href=\"#2-provider-agnostic-via-lightllm\">2. Provider-Agnostic via LightLLM</a></li><li><a href=\"#3-how-llm-extraction-works\">3. How LLM Extraction Works</a></li><li><a href=\"#4-key-parameters\">4. Key Parameters</a></li><li><a href=\"#5-putting-it-in-crawlerrunconfig\">5. Putting It in CrawlerRunConfig</a></li><li><a href=\"#6-chunking-details\">6. Chunking Details</a></li><li><a href=\"#7-input-format\">7. Input Format</a></li><li><a href=\"#8-token-usage-show-usage\">8. Token Usage &amp; Show Usage</a></li><li><a href=\"#9-example-building-a-knowledge-graph\">9. Example: Building a Knowledge Graph</a></li><li><a href=\"#10-best-practices-caveats\">10. Best Practices &amp; Caveats</a></li><li><a href=\"#11-conclusion\">11. Conclusion</a></li>\n    </ul>\n</nav>\n</aside>\n            <main id=\"terminal-mkdocs-main-content\">\n<section id=\"mkdocs-terminal-content\">\n    <h1 id=\"extracting-json-llm\">Extracting JSON (LLM)</h1>\n<p>In some cases, you need to extract <strong>complex or unstructured</strong> information from a webpage that a simple CSS/XPath schema cannot easily parse. Or you want <strong>AI</strong>-driven insights, classification, or summarization. For these scenarios, Crawl4AI provides an <strong>LLM-based extraction strategy</strong> that:</p>\n<ol>\n<li>Works with <strong>any</strong> large language model supported by <a href=\"https://github.com/LightLLM\">LightLLM</a> (Ollama, OpenAI, Claude, and more).  </li>\n<li>Automatically splits content into chunks (if desired) to handle token limits, then combines results.  </li>\n<li>Lets you define a <strong>schema</strong> (like a Pydantic model) or a simpler “block” extraction approach.</li>\n</ol>\n<p><strong>Important</strong>: LLM-based extraction can be slower and costlier than schema-based approaches. If your page data is highly structured, consider using <a href=\"../no-llm-strategies/\"><code>JsonCssExtractionStrategy</code></a> or <a href=\"../no-llm-strategies/\"><code>JsonXPathExtractionStrategy</code></a> first. But if you need AI to interpret or reorganize content, read on!</p>\n<hr>\n<h2 id=\"1-why-use-an-llm\">1. Why Use an LLM?</h2>\n<ul>\n<li><strong>Complex Reasoning</strong>: If the site’s data is unstructured, scattered, or full of natural language context.  </li>\n<li><strong>Semantic Extraction</strong>: Summaries, knowledge graphs, or relational data that require comprehension.  </li>\n<li><strong>Flexible</strong>: You can pass instructions to the model to do more advanced transformations or classification.</li>\n</ul>\n<hr>\n<h2 id=\"2-provider-agnostic-via-lightllm\">2. Provider-Agnostic via LightLLM</h2>\n<p>Crawl4AI uses a “provider string” (e.g., <code>\"openai/gpt-4o\"</code>, <code>\"ollama/llama2.0\"</code>, <code>\"aws/titan\"</code>) to identify your LLM. <strong>Any</strong> model that LightLLM supports is fair game. You just provide:</p>\n<ul>\n<li><strong><code>provider</code></strong>: The <code>&lt;provider&gt;/&lt;model_name&gt;</code> identifier (e.g., <code>\"openai/gpt-4\"</code>, <code>\"ollama/llama2\"</code>, <code>\"huggingface/google-flan\"</code>, etc.).  </li>\n<li><strong><code>api_token</code></strong>: If needed (for OpenAI, HuggingFace, etc.); local models or Ollama might not require it.  </li>\n<li><strong><code>api_base</code></strong> (optional): If your provider has a custom endpoint.  </li>\n</ul>\n<p>This means you <strong>aren’t locked</strong> into a single LLM vendor. Switch or experiment easily.</p>\n<hr>\n<h2 id=\"3-how-llm-extraction-works\">3. How LLM Extraction Works</h2>\n<h3 id=\"31-flow\">3.1 Flow</h3>\n<p>1. <strong>Chunking</strong> (optional): The HTML or markdown is split into smaller segments if it’s very long (based on <code>chunk_token_threshold</code>, overlap, etc.).<br>\n2. <strong>Prompt Construction</strong>: For each chunk, the library forms a prompt that includes your <strong><code>instruction</code></strong> (and possibly schema or examples).<br>\n3. <strong>LLM Inference</strong>: Each chunk is sent to the model in parallel or sequentially (depending on your concurrency).<br>\n4. <strong>Combining</strong>: The results from each chunk are merged and parsed into JSON.</p>\n<h3 id=\"32-extraction_type\">3.2 <code>extraction_type</code></h3>\n<ul>\n<li><strong><code>\"schema\"</code></strong>: The model tries to return JSON conforming to your Pydantic-based schema.  </li>\n<li><strong><code>\"block\"</code></strong>: The model returns freeform text, or smaller JSON structures, which the library collects.  </li>\n</ul>\n<p>For structured data, <code>\"schema\"</code> is recommended. You provide <code>schema=YourPydanticModel.model_json_schema()</code>.</p>\n<hr>\n<h2 id=\"4-key-parameters\">4. Key Parameters</h2>\n<p>Below is an overview of important LLM extraction parameters. All are typically set inside <code>LLMExtractionStrategy(...)</code>. You then put that strategy in your <code>CrawlerRunConfig(..., extraction_strategy=...)</code>.</p>\n<p>1. <strong><code>provider</code></strong> (str): e.g., <code>\"openai/gpt-4\"</code>, <code>\"ollama/llama2\"</code>.<br>\n2. <strong><code>api_token</code></strong> (str): The API key or token for that model. May not be needed for local models.<br>\n3. <strong><code>schema</code></strong> (dict): A JSON schema describing the fields you want. Usually generated by <code>YourModel.model_json_schema()</code>.<br>\n4. <strong><code>extraction_type</code></strong> (str): <code>\"schema\"</code> or <code>\"block\"</code>.<br>\n5. <strong><code>instruction</code></strong> (str): Prompt text telling the LLM what you want extracted. E.g., “Extract these fields as a JSON array.”<br>\n6. <strong><code>chunk_token_threshold</code></strong> (int): Maximum tokens per chunk. If your content is huge, you can break it up for the LLM.<br>\n7. <strong><code>overlap_rate</code></strong> (float): Overlap ratio between adjacent chunks. E.g., <code>0.1</code> means 10% of each chunk is repeated to preserve context continuity.<br>\n8. <strong><code>apply_chunking</code></strong> (bool): Set <code>True</code> to chunk automatically. If you want a single pass, set <code>False</code>.<br>\n9. <strong><code>input_format</code></strong> (str): Determines <strong>which</strong> crawler result is passed to the LLM. Options include:<br>\n   - <code>\"markdown\"</code>: The raw markdown (default).<br>\n   - <code>\"fit_markdown\"</code>: The filtered “fit” markdown if you used a content filter.<br>\n   - <code>\"html\"</code>: The cleaned or raw HTML.<br>\n10. <strong><code>extra_args</code></strong> (dict): Additional LLM parameters like <code>temperature</code>, <code>max_tokens</code>, <code>top_p</code>, etc.<br>\n11. <strong><code>show_usage()</code></strong>: A method you can call to print out usage info (token usage per chunk, total cost if known).  </p>\n<p><strong>Example</strong>:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-graphql\">extraction_strategy <span class=\"hljs-punctuation\">=</span> LLMExtractionStrategy<span class=\"hljs-punctuation\">(</span>\n    provider<span class=\"hljs-punctuation\">=</span><span class=\"hljs-string\">\"openai/gpt-4\"</span>,\n    api_token<span class=\"hljs-punctuation\">=</span><span class=\"hljs-string\">\"YOUR_OPENAI_KEY\"</span>,\n    <span class=\"hljs-keyword\">schema</span><span class=\"hljs-punctuation\">=</span>MyModel.model_json_schema<span class=\"hljs-punctuation\">(</span><span class=\"hljs-punctuation\">)</span>,\n    extraction_type<span class=\"hljs-punctuation\">=</span><span class=\"hljs-string\">\"schema\"</span>,\n    instruction<span class=\"hljs-punctuation\">=</span><span class=\"hljs-string\">\"Extract a list of items from the text with 'name' and 'price' fields.\"</span>,\n    chunk_token_threshold<span class=\"hljs-punctuation\">=</span><span class=\"hljs-number\">1200</span>,\n    overlap_rate<span class=\"hljs-punctuation\">=</span><span class=\"hljs-number\">0.1</span>,\n    apply_chunking<span class=\"hljs-punctuation\">=</span><span class=\"hljs-literal\">True</span>,\n    input_format<span class=\"hljs-punctuation\">=</span><span class=\"hljs-string\">\"html\"</span>,\n    extra_args<span class=\"hljs-punctuation\">=</span><span class=\"hljs-punctuation\">{</span><span class=\"hljs-string\">\"temperature\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">0.1</span>, <span class=\"hljs-string\">\"max_tokens\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">1000</span><span class=\"hljs-punctuation\">}</span>,\n    verbose<span class=\"hljs-punctuation\">=</span><span class=\"hljs-literal\">True</span>\n<span class=\"hljs-punctuation\">)</span>\n</code></pre></div>\n<hr>\n<h2 id=\"5-putting-it-in-crawlerrunconfig\">5. Putting It in <code>CrawlerRunConfig</code></h2>\n<p><strong>Important</strong>: In Crawl4AI, all strategy definitions should go inside the <code>CrawlerRunConfig</code>, not directly as a param in <code>arun()</code>. Here’s a full example:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">from</span> pydantic <span class=\"hljs-keyword\">import</span> BaseModel, Field\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">List</span>\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> LLMExtractionStrategy\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Product</span>(<span class=\"hljs-title class_ inherited__\">BaseModel</span>):\n    name: <span class=\"hljs-built_in\">str</span>\n    price: <span class=\"hljs-built_in\">str</span>\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-comment\"># 1. Define the LLM extraction strategy</span>\n    llm_strategy = LLMExtractionStrategy(\n        provider=<span class=\"hljs-string\">\"openai/gpt-4o-mini\"</span>,            <span class=\"hljs-comment\"># e.g. \"ollama/llama2\"</span>\n        api_token=os.getenv(<span class=\"hljs-string\">'OPENAI_API_KEY'</span>),\n        schema=Product.schema_json(),            <span class=\"hljs-comment\"># Or use model_json_schema()</span>\n        extraction_type=<span class=\"hljs-string\">\"schema\"</span>,\n        instruction=<span class=\"hljs-string\">\"Extract all product objects with 'name' and 'price' from the content.\"</span>,\n        chunk_token_threshold=<span class=\"hljs-number\">1000</span>,\n        overlap_rate=<span class=\"hljs-number\">0.0</span>,\n        apply_chunking=<span class=\"hljs-literal\">True</span>,\n        input_format=<span class=\"hljs-string\">\"markdown\"</span>,   <span class=\"hljs-comment\"># or \"html\", \"fit_markdown\"</span>\n        extra_args={<span class=\"hljs-string\">\"temperature\"</span>: <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-string\">\"max_tokens\"</span>: <span class=\"hljs-number\">800</span>}\n    )\n\n    <span class=\"hljs-comment\"># 2. Build the crawler config</span>\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strategy,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    <span class=\"hljs-comment\"># 3. Create a browser config if needed</span>\n    browser_cfg = BrowserConfig(headless=<span class=\"hljs-literal\">True</span>)\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_cfg) <span class=\"hljs-keyword\">as</span> crawler:\n        <span class=\"hljs-comment\"># 4. Let's say we want to crawl a single page</span>\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://example.com/products\"</span>,\n            config=crawl_config\n        )\n\n        <span class=\"hljs-keyword\">if</span> result.success:\n            <span class=\"hljs-comment\"># 5. The extracted content is presumably JSON</span>\n            data = json.loads(result.extracted_content)\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Extracted items:\"</span>, data)\n\n            <span class=\"hljs-comment\"># 6. Show usage stats</span>\n            llm_strategy.show_usage()  <span class=\"hljs-comment\"># prints token usage</span>\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Error:\"</span>, result.error_message)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<hr>\n<h2 id=\"6-chunking-details\">6. Chunking Details</h2>\n<h3 id=\"61-chunk_token_threshold\">6.1 <code>chunk_token_threshold</code></h3>\n<p>If your page is large, you might exceed your LLM’s context window. <strong><code>chunk_token_threshold</code></strong> sets the approximate max tokens per chunk. The library calculates word→token ratio using <code>word_token_rate</code> (often ~0.75 by default). If chunking is enabled (<code>apply_chunking=True</code>), the text is split into segments.</p>\n<h3 id=\"62-overlap_rate\">6.2 <code>overlap_rate</code></h3>\n<p>To keep context continuous across chunks, we can overlap them. E.g., <code>overlap_rate=0.1</code> means each subsequent chunk includes 10% of the previous chunk’s text. This is helpful if your needed info might straddle chunk boundaries.</p>\n<h3 id=\"63-performance-parallelism\">6.3 Performance &amp; Parallelism</h3>\n<p>By chunking, you can potentially process multiple chunks in parallel (depending on your concurrency settings and the LLM provider). This reduces total time if the site is huge or has many sections.</p>\n<hr>\n<h2 id=\"7-input-format\">7. Input Format</h2>\n<p>By default, <strong>LLMExtractionStrategy</strong> uses <code>input_format=\"markdown\"</code>, meaning the <strong>crawler’s final markdown</strong> is fed to the LLM. You can change to:</p>\n<ul>\n<li><strong><code>html</code></strong>: The cleaned HTML or raw HTML (depending on your crawler config) goes into the LLM.  </li>\n<li><strong><code>fit_markdown</code></strong>: If you used, for instance, <code>PruningContentFilter</code>, the “fit” version of the markdown is used. This can drastically reduce tokens if you trust the filter.  </li>\n<li><strong><code>markdown</code></strong>: Standard markdown output from the crawler’s <code>markdown_generator</code>.</li>\n</ul>\n<p>This setting is crucial: if the LLM instructions rely on HTML tags, pick <code>\"html\"</code>. If you prefer a text-based approach, pick <code>\"markdown\"</code>.</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-bash\">LLMExtractionStrategy(\n    <span class=\"hljs-comment\"># ...</span>\n    input_format=<span class=\"hljs-string\">\"html\"</span>,  <span class=\"hljs-comment\"># Instead of \"markdown\" or \"fit_markdown\"</span>\n)\n</code></pre></div>\n<hr>\n<h2 id=\"8-token-usage-show-usage\">8. Token Usage &amp; Show Usage</h2>\n<p>To keep track of tokens and cost, each chunk is processed with an LLM call. We record usage in:</p>\n<ul>\n<li><strong><code>usages</code></strong> (list): token usage per chunk or call.  </li>\n<li><strong><code>total_usage</code></strong>: sum of all chunk calls.  </li>\n<li><strong><code>show_usage()</code></strong>: prints a usage report (if the provider returns usage data).</li>\n</ul>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-makefile\">llm_strategy = LLMExtractionStrategy(...)\n<span class=\"hljs-comment\"># ...</span>\nllm_strategy.show_usage()\n<span class=\"hljs-comment\"># e.g. “Total usage: 1241 tokens across 2 chunk calls”</span>\n</code></pre></div>\n<p>If your model provider doesn’t return usage info, these fields might be partial or empty.</p>\n<hr>\n<h2 id=\"9-example-building-a-knowledge-graph\">9. Example: Building a Knowledge Graph</h2>\n<p>Below is a snippet combining <strong><code>LLMExtractionStrategy</code></strong> with a Pydantic schema for a knowledge graph. Notice how we pass an <strong><code>instruction</code></strong> telling the model what to parse.</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">List</span>\n<span class=\"hljs-keyword\">from</span> pydantic <span class=\"hljs-keyword\">import</span> BaseModel, Field\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> LLMExtractionStrategy\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Entity</span>(<span class=\"hljs-title class_ inherited__\">BaseModel</span>):\n    name: <span class=\"hljs-built_in\">str</span>\n    description: <span class=\"hljs-built_in\">str</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Relationship</span>(<span class=\"hljs-title class_ inherited__\">BaseModel</span>):\n    entity1: Entity\n    entity2: Entity\n    description: <span class=\"hljs-built_in\">str</span>\n    relation_type: <span class=\"hljs-built_in\">str</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">KnowledgeGraph</span>(<span class=\"hljs-title class_ inherited__\">BaseModel</span>):\n    entities: <span class=\"hljs-type\">List</span>[Entity]\n    relationships: <span class=\"hljs-type\">List</span>[Relationship]\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-comment\"># LLM extraction strategy</span>\n    llm_strat = LLMExtractionStrategy(\n        provider=<span class=\"hljs-string\">\"openai/gpt-4\"</span>,\n        api_token=os.getenv(<span class=\"hljs-string\">'OPENAI_API_KEY'</span>),\n        schema=KnowledgeGraph.schema_json(),\n        extraction_type=<span class=\"hljs-string\">\"schema\"</span>,\n        instruction=<span class=\"hljs-string\">\"Extract entities and relationships from the content. Return valid JSON.\"</span>,\n        chunk_token_threshold=<span class=\"hljs-number\">1400</span>,\n        apply_chunking=<span class=\"hljs-literal\">True</span>,\n        input_format=<span class=\"hljs-string\">\"html\"</span>,\n        extra_args={<span class=\"hljs-string\">\"temperature\"</span>: <span class=\"hljs-number\">0.1</span>, <span class=\"hljs-string\">\"max_tokens\"</span>: <span class=\"hljs-number\">1500</span>}\n    )\n\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strat,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=BrowserConfig(headless=<span class=\"hljs-literal\">True</span>)) <span class=\"hljs-keyword\">as</span> crawler:\n        <span class=\"hljs-comment\"># Example page</span>\n        url = <span class=\"hljs-string\">\"https://www.nbcnews.com/business\"</span>\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(url=url, config=crawl_config)\n\n        <span class=\"hljs-keyword\">if</span> result.success:\n            <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"kb_result.json\"</span>, <span class=\"hljs-string\">\"w\"</span>, encoding=<span class=\"hljs-string\">\"utf-8\"</span>) <span class=\"hljs-keyword\">as</span> f:\n                f.write(result.extracted_content)\n            llm_strat.show_usage()\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Crawl failed:\"</span>, result.error_message)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Key Observations</strong>:</p>\n<ul>\n<li><strong><code>extraction_type=\"schema\"</code></strong> ensures we get JSON fitting our <code>KnowledgeGraph</code>.  </li>\n<li><strong><code>input_format=\"html\"</code></strong> means we feed HTML to the model.  </li>\n<li><strong><code>instruction</code></strong> guides the model to output a structured knowledge graph.  </li>\n</ul>\n<hr>\n<h2 id=\"10-best-practices-caveats\">10. Best Practices &amp; Caveats</h2>\n<p>1. <strong>Cost &amp; Latency</strong>: LLM calls can be slow or expensive. Consider chunking or smaller coverage if you only need partial data.<br>\n2. <strong>Model Token Limits</strong>: If your page + instruction exceed the context window, chunking is essential.<br>\n3. <strong>Instruction Engineering</strong>: Well-crafted instructions can drastically improve output reliability.<br>\n4. <strong>Schema Strictness</strong>: <code>\"schema\"</code> extraction tries to parse the model output as JSON. If the model returns invalid JSON, partial extraction might happen, or you might get an error.<br>\n5. <strong>Parallel vs. Serial</strong>: The library can process multiple chunks in parallel, but you must watch out for rate limits on certain providers.<br>\n6. <strong>Check Output</strong>: Sometimes, an LLM might omit fields or produce extraneous text. You may want to post-validate with Pydantic or do additional cleanup.</p>\n<hr>\n<h2 id=\"11-conclusion\">11. Conclusion</h2>\n<p><strong>LLM-based extraction</strong> in Crawl4AI is <strong>provider-agnostic</strong>, letting you choose from hundreds of models via LightLLM. It’s perfect for <strong>semantically complex</strong> tasks or generating advanced structures like knowledge graphs. However, it’s <strong>slower</strong> and potentially costlier than schema-based approaches. Keep these tips in mind:</p>\n<ul>\n<li>Put your LLM strategy <strong>in <code>CrawlerRunConfig</code></strong>.  </li>\n<li>Use <strong><code>input_format</code></strong> to pick which form (markdown, HTML, fit_markdown) the LLM sees.  </li>\n<li>Tweak <strong><code>chunk_token_threshold</code></strong>, <strong><code>overlap_rate</code></strong>, and <strong><code>apply_chunking</code></strong> to handle large content efficiently.  </li>\n<li>Monitor token usage with <code>show_usage()</code>.</li>\n</ul>\n<p>If your site’s data is consistent or repetitive, consider <a href=\"../no-llm-strategies/\"><code>JsonCssExtractionStrategy</code></a> first for speed and simplicity. But if you need an <strong>AI-driven</strong> approach, <code>LLMExtractionStrategy</code> offers a flexible, multi-provider solution for extracting structured JSON from any website.</p>\n<p><strong>Next Steps</strong>:</p>\n<p>1. <strong>Experiment with Different Providers</strong><br>\n   - Try switching the <code>provider</code> (e.g., <code>\"ollama/llama2\"</code>, <code>\"openai/gpt-4o\"</code>, etc.) to see differences in speed, accuracy, or cost.<br>\n   - Pass different <code>extra_args</code> like <code>temperature</code>, <code>top_p</code>, and <code>max_tokens</code> to fine-tune your results.</p>\n<p>2. <strong>Performance Tuning</strong><br>\n   - If pages are large, tweak <code>chunk_token_threshold</code>, <code>overlap_rate</code>, or <code>apply_chunking</code> to optimize throughput.<br>\n   - Check the usage logs with <code>show_usage()</code> to keep an eye on token consumption and identify potential bottlenecks.</p>\n<p>3. <strong>Validate Outputs</strong><br>\n   - If using <code>extraction_type=\"schema\"</code>, parse the LLM’s JSON with a Pydantic model for a final validation step.<br>\n   - Log or handle any parse errors gracefully, especially if the model occasionally returns malformed JSON.</p>\n<p>4. <strong>Explore Hooks &amp; Automation</strong><br>\n   - Integrate LLM extraction with <a href=\"../../advanced/hooks-auth/\">hooks</a> for complex pre/post-processing.<br>\n   - Use a multi-step pipeline: crawl, filter, LLM-extract, then store or index results for further analysis.</p>\n<p><strong>Last Updated</strong>: 2025-01-01</p>\n<hr>\n<p>That’s it for <strong>Extracting JSON (LLM)</strong>—now you can harness AI to parse, classify, or reorganize data on the web. Happy crawling!</p>\n</section>\n\n            </main>\n        </div>\n        <hr><footer>\n    <div class=\"terminal-mkdocs-footer-grid\">\n        <div id=\"terminal-mkdocs-footer-copyright-info\">\n             Site built with <a href=\"http://www.mkdocs.org\">MkDocs</a> and <a href=\"https://github.com/ntno/mkdocs-terminal\">Terminal for MkDocs</a>.\n        </div>\n    </div>\n</footer>\n    </div>\n\n    \n    <div class=\"modal\" id=\"mkdocs_search_modal\" tabindex=\"-1\" role=\"alertdialog\" aria-modal=\"true\" aria-labelledby=\"searchModalLabel\">\n    <div class=\"modal-dialog modal-lg\" role=\"search\">\n        <div class=\"modal-content\">\n            <div class=\"modal-header\">\n                <h5 class=\"modal-title\" id=\"searchModalLabel\">Search</h5>\n                <button type=\"button\" class=\"close btn btn-default btn-ghost\" data-dismiss=\"modal\"><span aria-hidden=\"true\">x</span><span class=\"sr-only\">Close</span></button>\n            </div>\n            <div class=\"modal-body\">\n                <p id=\"searchInputLabel\">Type to start searching</p>\n                <form>\n                    <div class=\"form-group\">\n                        <input type=\"search\" class=\"form-control\" aria-labelledby=\"searchInputLabel\" placeholder=\"\" id=\"mkdocs-search-query\" title=\"Please enter search terms here\">\n                    </div>\n                </form>\n                <div id=\"mkdocs-search-results\" data-no-results-text=\"No document matches found\"></div>\n            </div>\n            <div class=\"modal-footer\">\n            </div>\n        </div>\n    </div>\n</div>\n    \n    \n\n\n</body></html>",
  "markdown": "# Extracting JSON (LLM)\nIn some cases, you need to extract **complex or unstructured** information from a webpage that a simple CSS/XPath schema cannot easily parse. Or you want **AI** -driven insights, classification, or summarization. For these scenarios, Crawl4AI provides an **LLM-based extraction strategy** that:\n  1. Works with **any** large language model supported by (Ollama, OpenAI, Claude, and more). \n  2. Automatically splits content into chunks (if desired) to handle token limits, then combines results. \n  3. Lets you define a **schema** (like a Pydantic model) or a simpler “block” extraction approach.\n\n\n**Important** : LLM-based extraction can be slower and costlier than schema-based approaches. If your page data is highly structured, consider using `JsonCssExtractionStrategy`[](https://docs.crawl4ai.com/extraction/<../no-llm-strategies/>) or `JsonXPathExtractionStrategy`[](https://docs.crawl4ai.com/extraction/<../no-llm-strategies/>) first. But if you need AI to interpret or reorganize content, read on!\n## 1. Why Use an LLM?\n  * **Complex Reasoning** : If the site’s data is unstructured, scattered, or full of natural language context. \n  * **Semantic Extraction** : Summaries, knowledge graphs, or relational data that require comprehension. \n  * **Flexible** : You can pass instructions to the model to do more advanced transformations or classification.\n\n\n## 2. Provider-Agnostic via LightLLM\nCrawl4AI uses a “provider string” (e.g., `\"openai/gpt-4o\"`, `\"ollama/llama2.0\"`, `\"aws/titan\"`) to identify your LLM. **Any** model that LightLLM supports is fair game. You just provide:\n  * **`provider`**: The`<provider>/<model_name>` identifier (e.g., `\"openai/gpt-4\"`, `\"ollama/llama2\"`, `\"huggingface/google-flan\"`, etc.). \n  * **`api_token`**: If needed (for OpenAI, HuggingFace, etc.); local models or Ollama might not require it.\n  * **`api_base`**(optional): If your provider has a custom endpoint.\n\n\nThis means you **aren’t locked** into a single LLM vendor. Switch or experiment easily.\n## 3. How LLM Extraction Works\n### 3.1 Flow\n1. **Chunking** (optional): The HTML or markdown is split into smaller segments if it’s very long (based on `chunk_token_threshold`, overlap, etc.). 2. **Prompt Construction** : For each chunk, the library forms a prompt that includes your **`instruction`**(and possibly schema or examples). 3.**LLM Inference** : Each chunk is sent to the model in parallel or sequentially (depending on your concurrency). 4. **Combining** : The results from each chunk are merged and parsed into JSON.\n### 3.2 `extraction_type`\n  * **`\"schema\"`**: The model tries to return JSON conforming to your Pydantic-based schema.\n  * **`\"block\"`**: The model returns freeform text, or smaller JSON structures, which the library collects.\n\n\nFor structured data, `\"schema\"` is recommended. You provide `schema=YourPydanticModel.model_json_schema()`.\n## 4. Key Parameters\nBelow is an overview of important LLM extraction parameters. All are typically set inside `LLMExtractionStrategy(...)`. You then put that strategy in your `CrawlerRunConfig(..., extraction_strategy=...)`.\n1. **`provider`**(str): e.g.,`\"openai/gpt-4\"` , `\"ollama/llama2\"`. 2. **`api_token`**(str): The API key or token for that model. May not be needed for local models. 3.**`schema`**(dict): A JSON schema describing the fields you want. Usually generated by`YourModel.model_json_schema()`. 4. **`extraction_type`**(str):`\"schema\"` or `\"block\"`. 5. **`instruction`**(str): Prompt text telling the LLM what you want extracted. E.g., “Extract these fields as a JSON array.” 6.**`chunk_token_threshold`**(int): Maximum tokens per chunk. If your content is huge, you can break it up for the LLM. 7.**`overlap_rate`**(float): Overlap ratio between adjacent chunks. E.g.,`0.1` means 10% of each chunk is repeated to preserve context continuity. 8. **`apply_chunking`**(bool): Set`True` to chunk automatically. If you want a single pass, set `False`. 9. **`input_format`**(str): Determines**which** crawler result is passed to the LLM. Options include: - `\"markdown\"`: The raw markdown (default). - `\"fit_markdown\"`: The filtered “fit” markdown if you used a content filter. - `\"html\"`: The cleaned or raw HTML. 10. **`extra_args`**(dict): Additional LLM parameters like`temperature` , `max_tokens`, `top_p`, etc. 11. **`show_usage()`**: A method you can call to print out usage info (token usage per chunk, total cost if known).\n**Example** :\n```\nextraction_strategy = LLMExtractionStrategy(\n  provider=\"openai/gpt-4\",\n  api_token=\"YOUR_OPENAI_KEY\",\n  schema=MyModel.model_json_schema(),\n  extraction_type=\"schema\",\n  instruction=\"Extract a list of items from the text with 'name' and 'price' fields.\",\n  chunk_token_threshold=1200,\n  overlap_rate=0.1,\n  apply_chunking=True,\n  input_format=\"html\",\n  extra_args={\"temperature\": 0.1, \"max_tokens\": 1000},\n  verbose=True\n)\n\n```\n\n## 5. Putting It in `CrawlerRunConfig`\n**Important** : In Crawl4AI, all strategy definitions should go inside the `CrawlerRunConfig`, not directly as a param in `arun()`. Here’s a full example:\n```\nimport os\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nclass Product(BaseModel):\n  name: str\n  price: str\nasync def main():\n  # 1. Define the LLM extraction strategy\n  llm_strategy = LLMExtractionStrategy(\n    provider=\"openai/gpt-4o-mini\",      # e.g. \"ollama/llama2\"\n    api_token=os.getenv('OPENAI_API_KEY'),\n    schema=Product.schema_json(),      # Or use model_json_schema()\n    extraction_type=\"schema\",\n    instruction=\"Extract all product objects with 'name' and 'price' from the content.\",\n    chunk_token_threshold=1000,\n    overlap_rate=0.0,\n    apply_chunking=True,\n    input_format=\"markdown\",  # or \"html\", \"fit_markdown\"\n    extra_args={\"temperature\": 0.0, \"max_tokens\": 800}\n  )\n  # 2. Build the crawler config\n  crawl_config = CrawlerRunConfig(\n    extraction_strategy=llm_strategy,\n    cache_mode=CacheMode.BYPASS\n  )\n  # 3. Create a browser config if needed\n  browser_cfg = BrowserConfig(headless=True)\n  async with AsyncWebCrawler(config=browser_cfg) as crawler:\n    # 4. Let's say we want to crawl a single page\n    result = await crawler.arun(\n      url=\"https://example.com/products\",\n      config=crawl_config\n    )\n    if result.success:\n      # 5. The extracted content is presumably JSON\n      data = json.loads(result.extracted_content)\n      print(\"Extracted items:\", data)\n      # 6. Show usage stats\n      llm_strategy.show_usage() # prints token usage\n    else:\n      print(\"Error:\", result.error_message)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n## 6. Chunking Details\n### 6.1 `chunk_token_threshold`\nIf your page is large, you might exceed your LLM’s context window. **`chunk_token_threshold`**sets the approximate max tokens per chunk. The library calculates word→token ratio using`word_token_rate` (often ~0.75 by default). If chunking is enabled (`apply_chunking=True`), the text is split into segments.\n### 6.2 `overlap_rate`\nTo keep context continuous across chunks, we can overlap them. E.g., `overlap_rate=0.1` means each subsequent chunk includes 10% of the previous chunk’s text. This is helpful if your needed info might straddle chunk boundaries.\n### 6.3 Performance & Parallelism\nBy chunking, you can potentially process multiple chunks in parallel (depending on your concurrency settings and the LLM provider). This reduces total time if the site is huge or has many sections.\n## 7. Input Format\nBy default, **LLMExtractionStrategy** uses `input_format=\"markdown\"`, meaning the **crawler’s final markdown** is fed to the LLM. You can change to:\n  * **`html`**: The cleaned HTML or raw HTML (depending on your crawler config) goes into the LLM.\n  * **`fit_markdown`**: If you used, for instance,`PruningContentFilter` , the “fit” version of the markdown is used. This can drastically reduce tokens if you trust the filter. \n  * **`markdown`**: Standard markdown output from the crawler’s`markdown_generator`.\n\n\nThis setting is crucial: if the LLM instructions rely on HTML tags, pick `\"html\"`. If you prefer a text-based approach, pick `\"markdown\"`.\n```\nLLMExtractionStrategy(\n  # ...\n  input_format=\"html\", # Instead of \"markdown\" or \"fit_markdown\"\n)\n\n```\n\n## 8. Token Usage & Show Usage\nTo keep track of tokens and cost, each chunk is processed with an LLM call. We record usage in:\n  * **`usages`**(list): token usage per chunk or call.\n  * **`total_usage`**: sum of all chunk calls.\n  * **`show_usage()`**: prints a usage report (if the provider returns usage data).\n\n\n```\nllm_strategy = LLMExtractionStrategy(...)\n# ...\nllm_strategy.show_usage()\n# e.g. “Total usage: 1241 tokens across 2 chunk calls”\n\n```\n\nIf your model provider doesn’t return usage info, these fields might be partial or empty.\n## 9. Example: Building a Knowledge Graph\nBelow is a snippet combining **`LLMExtractionStrategy`**with a Pydantic schema for a knowledge graph. Notice how we pass an**`instruction`**telling the model what to parse.\n```\nimport os\nimport json\nimport asyncio\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nclass Entity(BaseModel):\n  name: str\n  description: str\nclass Relationship(BaseModel):\n  entity1: Entity\n  entity2: Entity\n  description: str\n  relation_type: str\nclass KnowledgeGraph(BaseModel):\n  entities: List[Entity]\n  relationships: List[Relationship]\nasync def main():\n  # LLM extraction strategy\n  llm_strat = LLMExtractionStrategy(\n    provider=\"openai/gpt-4\",\n    api_token=os.getenv('OPENAI_API_KEY'),\n    schema=KnowledgeGraph.schema_json(),\n    extraction_type=\"schema\",\n    instruction=\"Extract entities and relationships from the content. Return valid JSON.\",\n    chunk_token_threshold=1400,\n    apply_chunking=True,\n    input_format=\"html\",\n    extra_args={\"temperature\": 0.1, \"max_tokens\": 1500}\n  )\n  crawl_config = CrawlerRunConfig(\n    extraction_strategy=llm_strat,\n    cache_mode=CacheMode.BYPASS\n  )\n  async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n    # Example page\n    url = \"https://www.nbcnews.com/business\"\n    result = await crawler.arun(url=url, config=crawl_config)\n    if result.success:\n      with open(\"kb_result.json\", \"w\", encoding=\"utf-8\") as f:\n        f.write(result.extracted_content)\n      llm_strat.show_usage()\n    else:\n      print(\"Crawl failed:\", result.error_message)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Key Observations** :\n  * **`extraction_type=\"schema\"`**ensures we get JSON fitting our`KnowledgeGraph`. \n  * **`input_format=\"html\"`**means we feed HTML to the model.\n  * **`instruction`**guides the model to output a structured knowledge graph.\n\n\n## 10. Best Practices & Caveats\n1. **Cost & Latency**: LLM calls can be slow or expensive. Consider chunking or smaller coverage if you only need partial data. 2. **Model Token Limits** : If your page + instruction exceed the context window, chunking is essential. 3. **Instruction Engineering** : Well-crafted instructions can drastically improve output reliability. 4. **Schema Strictness** : `\"schema\"` extraction tries to parse the model output as JSON. If the model returns invalid JSON, partial extraction might happen, or you might get an error. 5. **Parallel vs. Serial** : The library can process multiple chunks in parallel, but you must watch out for rate limits on certain providers. 6. **Check Output** : Sometimes, an LLM might omit fields or produce extraneous text. You may want to post-validate with Pydantic or do additional cleanup.\n## 11. Conclusion\n**LLM-based extraction** in Crawl4AI is **provider-agnostic** , letting you choose from hundreds of models via LightLLM. It’s perfect for **semantically complex** tasks or generating advanced structures like knowledge graphs. However, it’s **slower** and potentially costlier than schema-based approaches. Keep these tips in mind:\n  * Put your LLM strategy **in`CrawlerRunConfig`**. \n  * Use **`input_format`**to pick which form (markdown, HTML, fit_markdown) the LLM sees.\n  * Tweak **`chunk_token_threshold`**,**`overlap_rate`**, and**`apply_chunking`**to handle large content efficiently.\n  * Monitor token usage with `show_usage()`.\n\n\nIf your site’s data is consistent or repetitive, consider `JsonCssExtractionStrategy`[](https://docs.crawl4ai.com/extraction/<../no-llm-strategies/>) first for speed and simplicity. But if you need an **AI-driven** approach, `LLMExtractionStrategy` offers a flexible, multi-provider solution for extracting structured JSON from any website.\n**Next Steps** :\n1. **Experiment with Different Providers** - Try switching the `provider` (e.g., `\"ollama/llama2\"`, `\"openai/gpt-4o\"`, etc.) to see differences in speed, accuracy, or cost. - Pass different `extra_args` like `temperature`, `top_p`, and `max_tokens` to fine-tune your results.\n2. **Performance Tuning** - If pages are large, tweak `chunk_token_threshold`, `overlap_rate`, or `apply_chunking` to optimize throughput. - Check the usage logs with `show_usage()` to keep an eye on token consumption and identify potential bottlenecks.\n3. **Validate Outputs** - If using `extraction_type=\"schema\"`, parse the LLM’s JSON with a Pydantic model for a final validation step. - Log or handle any parse errors gracefully, especially if the model occasionally returns malformed JSON.\n4. **Explore Hooks & Automation** - Integrate LLM extraction with [hooks](https://docs.crawl4ai.com/extraction/advanced/hooks-auth/>) for complex pre/post-processing. - Use a multi-step pipeline: crawl, filter, LLM-extract, then store or index results for further analysis.\n**Last Updated** : 2025-01-01\nThat’s it for **Extracting JSON (LLM)** —now you can harness AI to parse, classify, or reorganize data on the web. Happy crawling!\n##### Search\nxClose\nType to start searching\n",
  "links": [
    "https://docs.crawl4ai.com",
    "https://docs.crawl4ai.com/advanced/advanced-features",
    "https://docs.crawl4ai.com/advanced/crawl-dispatcher",
    "https://docs.crawl4ai.com/advanced/file-downloading",
    "https://docs.crawl4ai.com/advanced/hooks-auth",
    "https://docs.crawl4ai.com/advanced/identity-based-crawling",
    "https://docs.crawl4ai.com/advanced/lazy-loading",
    "https://docs.crawl4ai.com/advanced/multi-url-crawling",
    "https://docs.crawl4ai.com/advanced/proxy-security",
    "https://docs.crawl4ai.com/advanced/session-management",
    "https://docs.crawl4ai.com/advanced/ssl-certificate",
    "https://docs.crawl4ai.com/api/arun",
    "https://docs.crawl4ai.com/api/arun_many",
    "https://docs.crawl4ai.com/api/async-webcrawler",
    "https://docs.crawl4ai.com/api/crawl-result",
    "https://docs.crawl4ai.com/api/parameters",
    "https://docs.crawl4ai.com/api/strategies",
    "https://docs.crawl4ai.com/blog",
    "https://docs.crawl4ai.com/chunking",
    "https://docs.crawl4ai.com/clustring-strategies",
    "https://docs.crawl4ai.com/core/browser-crawler-config",
    "https://docs.crawl4ai.com/core/cache-modes",
    "https://docs.crawl4ai.com/core/content-selection",
    "https://docs.crawl4ai.com/core/crawler-result",
    "https://docs.crawl4ai.com/core/docker-deploymeny",
    "https://docs.crawl4ai.com/core/fit-markdown",
    "https://docs.crawl4ai.com/core/installation",
    "https://docs.crawl4ai.com/core/link-media",
    "https://docs.crawl4ai.com/core/local-files",
    "https://docs.crawl4ai.com/core/markdown-generation",
    "https://docs.crawl4ai.com/core/page-interaction",
    "https://docs.crawl4ai.com/core/quickstart",
    "https://docs.crawl4ai.com/core/simple-crawling",
    "https://docs.crawl4ai.com/no-llm-strategies"
  ],
  "depth": 1,
  "stats": {
    "processed": 29,
    "total": 0,
    "depth": 1,
    "elapsed": "0:00:36",
    "page_limit": 34
  }
}