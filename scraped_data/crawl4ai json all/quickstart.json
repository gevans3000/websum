{
  "url": "https://docs.crawl4ai.com/core/quickstart",
  "timestamp": "2025-02-06T13:23:47.786574",
  "html": "<!DOCTYPE html><html lang=\"en\" style=\"scroll-padding-top: 50px;\"><head>\n    \n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n    <meta name=\"generator\" content=\"mkdocs-1.6.0, mkdocs-terminal-4.4.0\">\n    \n    <meta name=\"description\" content=\"🚀🤖 Crawl4AI, Open-source LLM-Friendly Web Crawler &amp; Scraper\"> \n     \n    \n    <link rel=\"canonical\" href=\"https://docs.crawl4ai.com/core/quickstart/\"><link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"../../img/android-chrome-192x192.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"512x512\" href=\"../../img/android-chrome-512x512.png\">\n<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"../../img/apple-touch-icon.png\">\n<link rel=\"shortcut icon\" type=\"image/png\" sizes=\"48x48\" href=\"../../img/favicon.ico\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"../../img/favicon-16x16.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"../../img/favicon-32x32.png\">\n\n\n    \n \n<title>Quick Start - Crawl4AI Documentation (v0.4.3bx)</title>\n\n\n<link href=\"../../css/fontawesome/css/fontawesome.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/fontawesome/css/solid.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/normalize.css\" rel=\"stylesheet\">\n<link href=\"../../css/terminal.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.tile_grid.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.footer.css\" rel=\"stylesheet\">\n<!-- dark color palette -->\n<link href=\"../../css/palettes/dark.css\" rel=\"stylesheet\">\n\n<!-- page layout -->\n<style>\n/* initially set page layout to a one column grid */\n.terminal-mkdocs-main-grid {\n    display: grid;\n    grid-column-gap: 1.4em;\n    grid-template-columns: auto;\n    grid-template-rows: auto;\n}\n\n/*  \n*   when side navigation is not hidden, use a two column grid.  \n*   if the screen is too narrow, fall back to the initial one column grid layout.\n*   in this case the main content will be placed under the navigation panel. \n*/\n@media only screen and (min-width: 70em) {\n    .terminal-mkdocs-main-grid {\n        grid-template-columns: 4fr 9fr;\n    }\n}</style>\n\n\n\n    \n    <link href=\"../../assets/styles.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/highlight.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/dmvendor.css\" rel=\"stylesheet\">  \n    \n    \n\n    \n    <!-- search css support -->\n<link href=\"../../css/search/bootstrap-modal.css\" rel=\"stylesheet\">\n<!-- search scripts -->\n<script>\n    var base_url = \"../..\",\n    shortcuts = \"{}\";\n</script>\n<script src=\"../../js/jquery/jquery-1.10.1.min.js\" defer=\"\"></script>\n<script src=\"../../js/bootstrap/bootstrap.min.js\" defer=\"\"></script>\n<script src=\"../../js/mkdocs/base.js\" defer=\"\"></script>\n    \n    \n    \n    \n    <script src=\"../../assets/highlight.min.js\"></script>\n    \n    <script src=\"../../assets/highlight_init.js\"></script>\n    \n    <script src=\"https://buttons.github.io/buttons.js\"></script>\n    \n    <script src=\"../../search/main.js\"></script>\n    \n\n    \n</head>\n\n<body class=\"terminal\" style=\"\"><div class=\"container\">\n    <div class=\"terminal-nav\">\n        <header class=\"terminal-logo\">\n            <div id=\"mkdocs-terminal-site-name\" class=\"logo terminal-prompt\"><a href=\"https://docs.crawl4ai.com/\" class=\"no-style\">Crawl4AI Documentation (v0.4.3bx)</a></div>\n        </header>\n        \n        <nav class=\"terminal-menu\">\n            \n            <ul vocab=\"https://schema.org/\" typeof=\"BreadcrumbList\">\n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../..\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Home</span>\n                    </a>\n                    <meta property=\"position\" content=\"0\">\n                </li>\n                \n                \n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"./\" class=\"menu-item active\" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Quick Start</span>\n                    </a>\n                    <meta property=\"position\" content=\"1\">\n                </li>\n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                    \n                    \n\n\n<li property=\"itemListElement\" typeof=\"ListItem\">\n    <a href=\"#\" class=\"menu-item\" data-toggle=\"modal\" data-target=\"#mkdocs_search_modal\" property=\"item\" typeof=\"SearchAction\">\n        <i aria-hidden=\"true\" class=\"fa fa-search\"></i> <span property=\"name\">Search</span>\n    </a>\n    <meta property=\"position\" content=\"2\">\n</li>\n                    \n            </ul>\n            \n        </nav>\n    </div>\n</div>\n        \n    <div class=\"container\">\n        <div class=\"terminal-mkdocs-main-grid\"><aside id=\"terminal-mkdocs-side-panel\"><nav>\n  \n    <ul class=\"terminal-mkdocs-side-nav-items\">\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../..\">Home</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Setup &amp; Installation</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../installation/\">Installation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../docker-deploymeny/\">Docker Deployment</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        <span class=\"\n\n    terminal-mkdocs-side-nav-item--active\">Quick Start</span>\n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Blog &amp; Changelog</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../blog/\">Blog Home</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md\">Changelog</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Core</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../simple-crawling/\">Simple Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../crawler-result/\">Crawler Result</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../browser-crawler-config/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../markdown-generation/\">Markdown Generation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../fit-markdown/\">Fit Markdown</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../page-interaction/\">Page Interaction</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../content-selection/\">Content Selection</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../cache-modes/\">Cache Modes</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../local-files/\">Local Files &amp; Raw HTML</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../link-media/\">Link &amp; Media</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Advanced</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/advanced-features/\">Overview</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/file-downloading/\">File Downloading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/lazy-loading/\">Lazy Loading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/hooks-auth/\">Hooks &amp; Auth</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/proxy-security/\">Proxy &amp; Security</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/session-management/\">Session Management</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/multi-url-crawling/\">Multi-URL Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/crawl-dispatcher/\">Crawl Dispatcher</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/identity-based-crawling/\">Identity Based Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../advanced/ssl-certificate/\">SSL Certificate</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Extraction</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/no-llm-strategies/\">LLM-Free Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/llm-strategies/\">LLM Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/clustring-strategies/\">Clustering Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/chunking/\">Chunking</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">API Reference</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/async-webcrawler/\">AsyncWebCrawler</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun/\">arun()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun_many/\">arun_many()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/parameters/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/crawl-result/\">CrawlResult</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/strategies/\">Strategies</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n    </ul>\n  \n</nav><hr>\n<nav>\n    <ul>\n        <li><a href=\"#getting-started-with-crawl4ai\">Getting Started with Crawl4AI</a></li>\n        <li><a href=\"#1-introduction\">1. Introduction</a></li><li><a href=\"#2-your-first-crawl\">2. Your First Crawl</a></li><li><a href=\"#3-basic-configuration-light-introduction\">3. Basic Configuration (Light Introduction)</a></li><li><a href=\"#4-generating-markdown-output\">4. Generating Markdown Output</a></li><li><a href=\"#5-simple-data-extraction-css-based\">5. Simple Data Extraction (CSS-based)</a></li><li><a href=\"#6-simple-data-extraction-llm-based\">6. Simple Data Extraction (LLM-based)</a></li><li><a href=\"#7-multi-url-concurrency-preview\">7. Multi-URL Concurrency (Preview)</a></li><li><a href=\"#8-dynamic-content-example\">8. Dynamic Content Example</a></li><li><a href=\"#9-next-steps\">9. Next Steps</a></li>\n    </ul>\n</nav>\n</aside>\n            <main id=\"terminal-mkdocs-main-content\">\n<section id=\"mkdocs-terminal-content\">\n    <h1 id=\"getting-started-with-crawl4ai\">Getting Started with Crawl4AI</h1>\n<p>Welcome to <strong>Crawl4AI</strong>, an open-source LLM-friendly Web Crawler &amp; Scraper. In this tutorial, you’ll:</p>\n<ol>\n<li>Run your <strong>first crawl</strong> using minimal configuration.  </li>\n<li>Generate <strong>Markdown</strong> output (and learn how it’s influenced by content filters).  </li>\n<li>Experiment with a simple <strong>CSS-based extraction</strong> strategy.  </li>\n<li>See a glimpse of <strong>LLM-based extraction</strong> (including open-source and closed-source model options).  </li>\n<li>Crawl a <strong>dynamic</strong> page that loads content via JavaScript.</li>\n</ol>\n<hr>\n<h2 id=\"1-introduction\">1. Introduction</h2>\n<p>Crawl4AI provides:</p>\n<ul>\n<li>An asynchronous crawler, <strong><code>AsyncWebCrawler</code></strong>.  </li>\n<li>Configurable browser and run settings via <strong><code>BrowserConfig</code></strong> and <strong><code>CrawlerRunConfig</code></strong>.  </li>\n<li>Automatic HTML-to-Markdown conversion via <strong><code>DefaultMarkdownGenerator</code></strong> (supports optional filters).  </li>\n<li>Multiple extraction strategies (LLM-based or “traditional” CSS/XPath-based).</li>\n</ul>\n<p>By the end of this guide, you’ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses “Load More” buttons or JavaScript updates.</p>\n<hr>\n<h2 id=\"2-your-first-crawl\">2. Your First Crawl</h2>\n<p>Here’s a minimal Python script that creates an <strong><code>AsyncWebCrawler</code></strong>, fetches a webpage, and prints the first 300 characters of its Markdown output:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(<span class=\"hljs-string\">\"https://example.com\"</span>)\n        <span class=\"hljs-built_in\">print</span>(result.markdown[:<span class=\"hljs-number\">300</span>])  <span class=\"hljs-comment\"># Print first 300 chars</span>\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>What’s happening?</strong>\n- <strong><code>AsyncWebCrawler</code></strong> launches a headless browser (Chromium by default).\n- It fetches <code>https://example.com</code>.\n- Crawl4AI automatically converts the HTML into Markdown.</p>\n<p>You now have a simple, working crawl!</p>\n<hr>\n<h2 id=\"3-basic-configuration-light-introduction\">3. Basic Configuration (Light Introduction)</h2>\n<p>Crawl4AI’s crawler can be heavily customized using two main classes:</p>\n<p>1. <strong><code>BrowserConfig</code></strong>: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.).<br>\n2. <strong><code>CrawlerRunConfig</code></strong>: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.).</p>\n<p>Below is an example with minimal usage:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    browser_conf = BrowserConfig(headless=<span class=\"hljs-literal\">True</span>)  <span class=\"hljs-comment\"># or False to see the browser</span>\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_conf) <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://example.com\"</span>,\n            config=run_conf\n        )\n        <span class=\"hljs-built_in\">print</span>(result.markdown)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<blockquote>\n<p>IMPORTANT: By default cache mode is set to <code>CacheMode.ENABLED</code>. So to have fresh content, you need to set it to <code>CacheMode.BYPASS</code></p>\n</blockquote>\n<p>We’ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling.</p>\n<hr>\n<h2 id=\"4-generating-markdown-output\">4. Generating Markdown Output</h2>\n<p>By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a <strong>markdown generator</strong> or <strong>content filter</strong>.</p>\n<ul>\n<li><strong><code>result.markdown</code></strong>:<br>\n  The direct HTML-to-Markdown conversion.  </li>\n<li><strong><code>result.markdown.fit_markdown</code></strong>:<br>\n  The same content after applying any configured <strong>content filter</strong> (e.g., <code>PruningContentFilter</code>).</li>\n</ul>\n<h3 id=\"example-using-a-filter-with-defaultmarkdowngenerator\">Example: Using a Filter with <code>DefaultMarkdownGenerator</code></h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig\n<span class=\"hljs-keyword\">from</span> crawl4ai.content_filter_strategy <span class=\"hljs-keyword\">import</span> PruningContentFilter\n<span class=\"hljs-keyword\">from</span> crawl4ai.markdown_generation_strategy <span class=\"hljs-keyword\">import</span> DefaultMarkdownGenerator\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=PruningContentFilter(threshold=<span class=\"hljs-number\">0.4</span>, threshold_type=<span class=\"hljs-string\">\"fixed\"</span>)\n)\n\nconfig = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    markdown_generator=md_generator\n)\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n    result = <span class=\"hljs-keyword\">await</span> crawler.arun(<span class=\"hljs-string\">\"https://news.ycombinator.com\"</span>, config=config)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Raw Markdown length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.markdown.raw_markdown))\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Fit Markdown length:\"</span>, <span class=\"hljs-built_in\">len</span>(result.markdown.fit_markdown))\n</code></pre></div>\n<p><strong>Note</strong>: If you do <strong>not</strong> specify a content filter or markdown generator, you’ll typically see only the raw Markdown. <code>PruningContentFilter</code> may adds around <code>50ms</code> in processing time. We’ll dive deeper into these strategies in a dedicated <strong>Markdown Generation</strong> tutorial.</p>\n<hr>\n<h2 id=\"5-simple-data-extraction-css-based\">5. Simple Data Extraction (CSS-based)</h2>\n<p>Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example:</p>\n<blockquote>\n<p><strong>New!</strong> Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions:</p>\n</blockquote>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-makefile\">from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\n<span class=\"hljs-comment\"># Generate a schema (one-time cost)</span>\nhtml = <span class=\"hljs-string\">\"&lt;div class='product'&gt;&lt;h2&gt;Gaming Laptop&lt;/h2&gt;&lt;span class='price'&gt;$999.99&lt;/span&gt;&lt;/div&gt;\"</span>\n\n<span class=\"hljs-comment\"># Using OpenAI (requires API token)</span>\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_provider=<span class=\"hljs-string\">\"openai/gpt-4o\"</span>,  <span class=\"hljs-comment\"># Default provider</span>\n    api_token=<span class=\"hljs-string\">\"your-openai-token\"</span>  <span class=\"hljs-comment\"># Required for OpenAI</span>\n)\n\n<span class=\"hljs-comment\"># Or using Ollama (open source, no token needed)</span>\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_provider=<span class=\"hljs-string\">\"ollama/llama3.3\"</span>,  <span class=\"hljs-comment\"># Open source alternative</span>\n    api_token=None  <span class=\"hljs-comment\"># Not needed for Ollama</span>\n)\n\n<span class=\"hljs-comment\"># Use the schema for fast, repeated extractions</span>\nstrategy = JsonCssExtractionStrategy(schema)\n</code></pre></div>\n<p>For a complete guide on schema generation and advanced usage, see <a href=\"../../extraction/no-llm-strategies/\">No-LLM Extraction Strategies</a>.</p>\n<p>Here's a basic extraction example:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> JsonCssExtractionStrategy\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    schema = {\n        <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"Example Items\"</span>,\n        <span class=\"hljs-string\">\"baseSelector\"</span>: <span class=\"hljs-string\">\"div.item\"</span>,\n        <span class=\"hljs-string\">\"fields\"</span>: [\n            {<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"title\"</span>, <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\"h2\"</span>, <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>},\n            {<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"link\"</span>, <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\"a\"</span>, <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"attribute\"</span>, <span class=\"hljs-string\">\"attribute\"</span>: <span class=\"hljs-string\">\"href\"</span>}\n        ]\n    }\n\n    raw_html = <span class=\"hljs-string\">\"&lt;div class='item'&gt;&lt;h2&gt;Item 1&lt;/h2&gt;&lt;a href='https://example.com/item1'&gt;Link 1&lt;/a&gt;&lt;/div&gt;\"</span>\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"raw://\"</span> + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        <span class=\"hljs-comment\"># The JSON output is stored in 'extracted_content'</span>\n        data = json.loads(result.extracted_content)\n        <span class=\"hljs-built_in\">print</span>(data)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Why is this helpful?</strong>\n- Great for repetitive page structures (e.g., item listings, articles).\n- No AI usage or costs.\n- The crawler returns a JSON string you can parse or store.</p>\n<blockquote>\n<p>Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with <code>raw://</code>.</p>\n</blockquote>\n<hr>\n<h2 id=\"6-simple-data-extraction-llm-based\">6. Simple Data Extraction (LLM-based)</h2>\n<p>For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports <strong>open-source</strong> or <strong>closed-source</strong> providers:</p>\n<ul>\n<li><strong>Open-Source Models</strong> (e.g., <code>ollama/llama3.3</code>, <code>no_token</code>)  </li>\n<li><strong>OpenAI Models</strong> (e.g., <code>openai/gpt-4</code>, requires <code>api_token</code>)  </li>\n<li>Or any provider supported by the underlying library</li>\n</ul>\n<p>Below is an example using <strong>open-source</strong> style (no token) and closed-source:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> pydantic <span class=\"hljs-keyword\">import</span> BaseModel, Field\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> LLMExtractionStrategy\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">OpenAIModelFee</span>(<span class=\"hljs-title class_ inherited__\">BaseModel</span>):\n    model_name: <span class=\"hljs-built_in\">str</span> = Field(..., description=<span class=\"hljs-string\">\"Name of the OpenAI model.\"</span>)\n    input_fee: <span class=\"hljs-built_in\">str</span> = Field(..., description=<span class=\"hljs-string\">\"Fee for input token for the OpenAI model.\"</span>)\n    output_fee: <span class=\"hljs-built_in\">str</span> = Field(\n        ..., description=<span class=\"hljs-string\">\"Fee for output token for the OpenAI model.\"</span>\n    )\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">extract_structured_data_using_llm</span>(<span class=\"hljs-params\">\n    provider: <span class=\"hljs-built_in\">str</span>, api_token: <span class=\"hljs-built_in\">str</span> = <span class=\"hljs-literal\">None</span>, extra_headers: <span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\">str</span>] = <span class=\"hljs-literal\">None</span>\n</span>):\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"\\n--- Extracting Structured Data with <span class=\"hljs-subst\">{provider}</span> ---\"</span>)\n\n    <span class=\"hljs-keyword\">if</span> api_token <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\">and</span> provider != <span class=\"hljs-string\">\"ollama\"</span>:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"API token is required for <span class=\"hljs-subst\">{provider}</span>. Skipping this example.\"</span>)\n        <span class=\"hljs-keyword\">return</span>\n\n    browser_config = BrowserConfig(headless=<span class=\"hljs-literal\">True</span>)\n\n    extra_args = {<span class=\"hljs-string\">\"temperature\"</span>: <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">\"top_p\"</span>: <span class=\"hljs-number\">0.9</span>, <span class=\"hljs-string\">\"max_tokens\"</span>: <span class=\"hljs-number\">2000</span>}\n    <span class=\"hljs-keyword\">if</span> extra_headers:\n        extra_args[<span class=\"hljs-string\">\"extra_headers\"</span>] = extra_headers\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        word_count_threshold=<span class=\"hljs-number\">1</span>,\n        page_timeout=<span class=\"hljs-number\">80000</span>,\n        extraction_strategy=LLMExtractionStrategy(\n            provider=provider,\n            api_token=api_token,\n            schema=OpenAIModelFee.model_json_schema(),\n            extraction_type=<span class=\"hljs-string\">\"schema\"</span>,\n            instruction=<span class=\"hljs-string\">\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content.\"\"\"</span>,\n            extra_args=extra_args,\n        ),\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_config) <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://openai.com/api/pricing/\"</span>, config=crawler_config\n        )\n        <span class=\"hljs-built_in\">print</span>(result.extracted_content)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    <span class=\"hljs-comment\"># Use ollama with llama3.3</span>\n    <span class=\"hljs-comment\"># asyncio.run(</span>\n    <span class=\"hljs-comment\">#     extract_structured_data_using_llm(</span>\n    <span class=\"hljs-comment\">#         provider=\"ollama/llama3.3\", api_token=\"no-token\"</span>\n    <span class=\"hljs-comment\">#     )</span>\n    <span class=\"hljs-comment\"># )</span>\n\n    asyncio.run(\n        extract_structured_data_using_llm(\n            provider=<span class=\"hljs-string\">\"openai/gpt-4o\"</span>, api_token=os.getenv(<span class=\"hljs-string\">\"OPENAI_API_KEY\"</span>)\n        )\n    )\n</code></pre></div>\n<p><strong>What’s happening?</strong>\n- We define a Pydantic schema (<code>PricingInfo</code>) describing the fields we want.\n- The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON.\n- Depending on the <strong>provider</strong> and <strong>api_token</strong>, you can use local models or a remote API.</p>\n<hr>\n<h2 id=\"7-multi-url-concurrency-preview\">7. Multi-URL Concurrency (Preview)</h2>\n<p>If you need to crawl multiple URLs in <strong>parallel</strong>, you can use <code>arun_many()</code>. By default, Crawl4AI employs a <strong>MemoryAdaptiveDispatcher</strong>, automatically adjusting concurrency based on system resources. Here’s a quick glimpse:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">quick_parallel_example</span>():\n    urls = [\n        <span class=\"hljs-string\">\"https://example.com/page1\"</span>,\n        <span class=\"hljs-string\">\"https://example.com/page2\"</span>,\n        <span class=\"hljs-string\">\"https://example.com/page3\"</span>\n    ]\n\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=<span class=\"hljs-literal\">True</span>  <span class=\"hljs-comment\"># Enable streaming mode</span>\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        <span class=\"hljs-comment\"># Stream results as they complete</span>\n        <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> <span class=\"hljs-keyword\">await</span> crawler.arun_many(urls, config=run_conf):\n            <span class=\"hljs-keyword\">if</span> result.success:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"[OK] <span class=\"hljs-subst\">{result.url}</span>, length: <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(result.markdown_v2.raw_markdown)}</span>\"</span>)\n            <span class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"[ERROR] <span class=\"hljs-subst\">{result.url}</span> =&gt; <span class=\"hljs-subst\">{result.error_message}</span>\"</span>)\n\n        <span class=\"hljs-comment\"># Or get all results at once (default behavior)</span>\n        run_conf = run_conf.clone(stream=<span class=\"hljs-literal\">False</span>)\n        results = <span class=\"hljs-keyword\">await</span> crawler.arun_many(urls, config=run_conf)\n        <span class=\"hljs-keyword\">for</span> res <span class=\"hljs-keyword\">in</span> results:\n            <span class=\"hljs-keyword\">if</span> res.success:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"[OK] <span class=\"hljs-subst\">{res.url}</span>, length: <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(res.markdown_v2.raw_markdown)}</span>\"</span>)\n            <span class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"[ERROR] <span class=\"hljs-subst\">{res.url}</span> =&gt; <span class=\"hljs-subst\">{res.error_message}</span>\"</span>)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(quick_parallel_example())\n</code></pre></div>\n<p>The example above shows two ways to handle multiple URLs:\n1. <strong>Streaming mode</strong> (<code>stream=True</code>): Process results as they become available using <code>async for</code>\n2. <strong>Batch mode</strong> (<code>stream=False</code>): Wait for all results to complete</p>\n<p>For more advanced concurrency (e.g., a <strong>semaphore-based</strong> approach, <strong>adaptive memory usage throttling</strong>, or customized rate limiting), see <a href=\"../../advanced/multi-url-crawling/\">Advanced Multi-URL Crawling</a>.</p>\n<hr>\n<h2 id=\"8-dynamic-content-example\">8. Dynamic Content Example</h2>\n<p>Some sites require multiple “page clicks” or dynamic JavaScript updates. Below is an example showing how to <strong>click</strong> a “Next Page” button and wait for new commits to load on GitHub, using <strong><code>BrowserConfig</code></strong> and <strong><code>CrawlerRunConfig</code></strong>:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n<span class=\"hljs-keyword\">from</span> crawl4ai.extraction_strategy <span class=\"hljs-keyword\">import</span> JsonCssExtractionStrategy\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">extract_structured_data_using_css_extractor</span>():\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\"</span>)\n    schema = {\n        <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"KidoCode Courses\"</span>,\n        <span class=\"hljs-string\">\"baseSelector\"</span>: <span class=\"hljs-string\">\"section.charge-methodology .w-tab-content &gt; div\"</span>,\n        <span class=\"hljs-string\">\"fields\"</span>: [\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"section_title\"</span>,\n                <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\"h3.heading-50\"</span>,\n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>,\n            },\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"section_description\"</span>,\n                <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\".charge-content\"</span>,\n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>,\n            },\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"course_name\"</span>,\n                <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\".text-block-93\"</span>,\n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>,\n            },\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"course_description\"</span>,\n                <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\".course-content-text\"</span>,\n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"text\"</span>,\n            },\n            {\n                <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"course_icon\"</span>,\n                <span class=\"hljs-string\">\"selector\"</span>: <span class=\"hljs-string\">\".image-92\"</span>,\n                <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"attribute\"</span>,\n                <span class=\"hljs-string\">\"attribute\"</span>: <span class=\"hljs-string\">\"src\"</span>,\n            },\n        ],\n    }\n\n    browser_config = BrowserConfig(headless=<span class=\"hljs-literal\">True</span>, java_script_enabled=<span class=\"hljs-literal\">True</span>)\n\n    js_click_tabs = <span class=\"hljs-string\">\"\"\"\n    (async () =&gt; {\n        const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 &gt; div\");\n        for(let tab of tabs) {\n            tab.scrollIntoView();\n            tab.click();\n            await new Promise(r =&gt; setTimeout(r, 500));\n        }\n    })();\n    \"\"\"</span>\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        js_code=[js_click_tabs],\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_config) <span class=\"hljs-keyword\">as</span> crawler:\n        result = <span class=\"hljs-keyword\">await</span> crawler.arun(\n            url=<span class=\"hljs-string\">\"https://www.kidocode.com/degrees/technology\"</span>, config=crawler_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Successfully extracted <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(companies)}</span> companies\"</span>)\n        <span class=\"hljs-built_in\">print</span>(json.dumps(companies[<span class=\"hljs-number\">0</span>], indent=<span class=\"hljs-number\">2</span>))\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    <span class=\"hljs-keyword\">await</span> extract_structured_data_using_css_extractor()\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Key Points</strong>:</p>\n<ul>\n<li><strong><code>BrowserConfig(headless=False)</code></strong>: We want to watch it click “Next Page.”  </li>\n<li><strong><code>CrawlerRunConfig(...)</code></strong>: We specify the extraction strategy, pass <code>session_id</code> to reuse the same page.  </li>\n<li><strong><code>js_code</code></strong> and <strong><code>wait_for</code></strong> are used for subsequent pages (<code>page &gt; 0</code>) to click the “Next” button and wait for new commits to load.  </li>\n<li><strong><code>js_only=True</code></strong> indicates we’re not re-navigating but continuing the existing session.  </li>\n<li>Finally, we call <code>kill_session()</code> to clean up the page and browser session.</li>\n</ul>\n<hr>\n<h2 id=\"9-next-steps\">9. Next Steps</h2>\n<p>Congratulations! You have:</p>\n<ol>\n<li>Performed a basic crawl and printed Markdown.  </li>\n<li>Used <strong>content filters</strong> with a markdown generator.  </li>\n<li>Extracted JSON via <strong>CSS</strong> or <strong>LLM</strong> strategies.  </li>\n<li>Handled <strong>dynamic</strong> pages with JavaScript triggers.</li>\n</ol>\n<p>If you’re ready for more, check out:</p>\n<ul>\n<li><strong>Installation</strong>: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies.  </li>\n<li><strong>Hooks &amp; Auth</strong>: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc.  </li>\n<li><strong>Deployment</strong>: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release.  </li>\n<li><strong>Browser Management</strong>: Delve into user simulation, stealth modes, and concurrency best practices.  </li>\n</ul>\n<p>Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!</p>\n</section>\n\n            </main>\n        </div>\n        <hr><footer>\n    <div class=\"terminal-mkdocs-footer-grid\">\n        <div id=\"terminal-mkdocs-footer-copyright-info\">\n             Site built with <a href=\"http://www.mkdocs.org\">MkDocs</a> and <a href=\"https://github.com/ntno/mkdocs-terminal\">Terminal for MkDocs</a>.\n        </div>\n    </div>\n</footer>\n    </div>\n\n    \n    <div class=\"modal\" id=\"mkdocs_search_modal\" tabindex=\"-1\" role=\"alertdialog\" aria-modal=\"true\" aria-labelledby=\"searchModalLabel\">\n    <div class=\"modal-dialog modal-lg\" role=\"search\">\n        <div class=\"modal-content\">\n            <div class=\"modal-header\">\n                <h5 class=\"modal-title\" id=\"searchModalLabel\">Search</h5>\n                <button type=\"button\" class=\"close btn btn-default btn-ghost\" data-dismiss=\"modal\"><span aria-hidden=\"true\">x</span><span class=\"sr-only\">Close</span></button>\n            </div>\n            <div class=\"modal-body\">\n                <p id=\"searchInputLabel\">Type to start searching</p>\n                <form>\n                    <div class=\"form-group\">\n                        <input type=\"search\" class=\"form-control\" aria-labelledby=\"searchInputLabel\" placeholder=\"\" id=\"mkdocs-search-query\" title=\"Please enter search terms here\">\n                    </div>\n                </form>\n                <div id=\"mkdocs-search-results\" data-no-results-text=\"No document matches found\"></div>\n            </div>\n            <div class=\"modal-footer\">\n            </div>\n        </div>\n    </div>\n</div>\n    \n    \n\n\n</body></html>",
  "markdown": "# Getting Started with Crawl4AI\nWelcome to **Crawl4AI** , an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you’ll:\n  1. Run your **first crawl** using minimal configuration. \n  2. Generate **Markdown** output (and learn how it’s influenced by content filters). \n  3. Experiment with a simple **CSS-based extraction** strategy. \n  4. See a glimpse of **LLM-based extraction** (including open-source and closed-source model options). \n  5. Crawl a **dynamic** page that loads content via JavaScript.\n\n\n## 1. Introduction\nCrawl4AI provides:\n  * An asynchronous crawler, **`AsyncWebCrawler`**.\n  * Configurable browser and run settings via **`BrowserConfig`**and**`CrawlerRunConfig`**.\n  * Automatic HTML-to-Markdown conversion via **`DefaultMarkdownGenerator`**(supports optional filters).\n  * Multiple extraction strategies (LLM-based or “traditional” CSS/XPath-based).\n\n\nBy the end of this guide, you’ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses “Load More” buttons or JavaScript updates.\n## 2. Your First Crawl\nHere’s a minimal Python script that creates an **`AsyncWebCrawler`**, fetches a webpage, and prints the first 300 characters of its Markdown output:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    print(result.markdown[:300]) # Print first 300 chars\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**What’s happening?** - **`AsyncWebCrawler`**launches a headless browser (Chromium by default). - It fetches`https://example.com`. - Crawl4AI automatically converts the HTML into Markdown.\nYou now have a simple, working crawl!\n## 3. Basic Configuration (Light Introduction)\nCrawl4AI’s crawler can be heavily customized using two main classes:\n1. **`BrowserConfig`**: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2.**`CrawlerRunConfig`**: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.).\nBelow is an example with minimal usage:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nasync def main():\n  browser_conf = BrowserConfig(headless=True) # or False to see the browser\n  run_conf = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS\n  )\n  async with AsyncWebCrawler(config=browser_conf) as crawler:\n    result = await crawler.arun(\n      url=\"https://example.com\",\n      config=run_conf\n    )\n    print(result.markdown)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n> IMPORTANT: By default cache mode is set to `CacheMode.ENABLED`. So to have fresh content, you need to set it to `CacheMode.BYPASS`\nWe’ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling.\n## 4. Generating Markdown Output\nBy default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a **markdown generator** or **content filter**.\n  * **`result.markdown`**: The direct HTML-to-Markdown conversion.\n  * **`result.markdown.fit_markdown`**: The same content after applying any configured**content filter** (e.g., `PruningContentFilter`).\n\n\n### Example: Using a Filter with `DefaultMarkdownGenerator`\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nmd_generator = DefaultMarkdownGenerator(\n  content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n)\nconfig = CrawlerRunConfig(\n  cache_mode=CacheMode.BYPASS,\n  markdown_generator=md_generator\n)\nasync with AsyncWebCrawler() as crawler:\n  result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n  print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n  print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n\n```\n\n**Note** : If you do **not** specify a content filter or markdown generator, you’ll typically see only the raw Markdown. `PruningContentFilter` may adds around `50ms` in processing time. We’ll dive deeper into these strategies in a dedicated **Markdown Generation** tutorial.\n## 5. Simple Data Extraction (CSS-based)\nCrawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example:\n> **New!** Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n# Generate a schema (one-time cost)\nhtml = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n# Using OpenAI (requires API token)\nschema = JsonCssExtractionStrategy.generate_schema(\n  html,\n  llm_provider=\"openai/gpt-4o\", # Default provider\n  api_token=\"your-openai-token\" # Required for OpenAI\n)\n# Or using Ollama (open source, no token needed)\nschema = JsonCssExtractionStrategy.generate_schema(\n  html,\n  llm_provider=\"ollama/llama3.3\", # Open source alternative\n  api_token=None # Not needed for Ollama\n)\n# Use the schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(schema)\n\n```\n\nFor a complete guide on schema generation and advanced usage, see [No-LLM Extraction Strategies](https://docs.crawl4ai.com/core/extraction/no-llm-strategies/>).\nHere's a basic extraction example:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nasync def main():\n  schema = {\n    \"name\": \"Example Items\",\n    \"baseSelector\": \"div.item\",\n    \"fields\": [\n      {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n      {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n    ]\n  }\n  raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      url=\"raw://\" + raw_html,\n      config=CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema)\n      )\n    )\n    # The JSON output is stored in 'extracted_content'\n    data = json.loads(result.extracted_content)\n    print(data)\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Why is this helpful?** - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store.\n> Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with `raw://`.\n## 6. Simple Data Extraction (LLM-based)\nFor more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports **open-source** or **closed-source** providers:\n  * **Open-Source Models** (e.g., `ollama/llama3.3`, `no_token`) \n  * **OpenAI Models** (e.g., `openai/gpt-4`, requires `api_token`) \n  * Or any provider supported by the underlying library\n\n\nBelow is an example using **open-source** style (no token) and closed-source:\n```\nimport os\nimport json\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nclass OpenAIModelFee(BaseModel):\n  model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n  input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n  output_fee: str = Field(\n    ..., description=\"Fee for output token for the OpenAI model.\"\n  )\nasync def extract_structured_data_using_llm(\n  provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n  print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n  if api_token is None and provider != \"ollama\":\n    print(f\"API token is required for {provider}. Skipping this example.\")\n    return\n  browser_config = BrowserConfig(headless=True)\n  extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n  if extra_headers:\n    extra_args[\"extra_headers\"] = extra_headers\n  crawler_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    word_count_threshold=1,\n    page_timeout=80000,\n    extraction_strategy=LLMExtractionStrategy(\n      provider=provider,\n      api_token=api_token,\n      schema=OpenAIModelFee.model_json_schema(),\n      extraction_type=\"schema\",\n      instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n      Do not miss any models in the entire content.\"\"\",\n      extra_args=extra_args,\n    ),\n  )\n  async with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(\n      url=\"https://openai.com/api/pricing/\", config=crawler_config\n    )\n    print(result.extracted_content)\nif __name__ == \"__main__\":\n  # Use ollama with llama3.3\n  # asyncio.run(\n  #   extract_structured_data_using_llm(\n  #     provider=\"ollama/llama3.3\", api_token=\"no-token\"\n  #   )\n  # )\n  asyncio.run(\n    extract_structured_data_using_llm(\n      provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\")\n    )\n  )\n\n```\n\n**What’s happening?** - We define a Pydantic schema (`PricingInfo`) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the **provider** and **api_token** , you can use local models or a remote API.\n## 7. Multi-URL Concurrency (Preview)\nIf you need to crawl multiple URLs in **parallel** , you can use `arun_many()`. By default, Crawl4AI employs a **MemoryAdaptiveDispatcher** , automatically adjusting concurrency based on system resources. Here’s a quick glimpse:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nasync def quick_parallel_example():\n  urls = [\n    \"https://example.com/page1\",\n    \"https://example.com/page2\",\n    \"https://example.com/page3\"\n  ]\n  run_conf = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    stream=True # Enable streaming mode\n  )\n  async with AsyncWebCrawler() as crawler:\n    # Stream results as they complete\n    async for result in await crawler.arun_many(urls, config=run_conf):\n      if result.success:\n        print(f\"[OK] {result.url}, length: {len(result.markdown_v2.raw_markdown)}\")\n      else:\n        print(f\"[ERROR] {result.url} => {result.error_message}\")\n    # Or get all results at once (default behavior)\n    run_conf = run_conf.clone(stream=False)\n    results = await crawler.arun_many(urls, config=run_conf)\n    for res in results:\n      if res.success:\n        print(f\"[OK] {res.url}, length: {len(res.markdown_v2.raw_markdown)}\")\n      else:\n        print(f\"[ERROR] {res.url} => {res.error_message}\")\nif __name__ == \"__main__\":\n  asyncio.run(quick_parallel_example())\n\n```\n\nThe example above shows two ways to handle multiple URLs: 1. **Streaming mode** (`stream=True`): Process results as they become available using `async for` 2. **Batch mode** (`stream=False`): Wait for all results to complete\nFor more advanced concurrency (e.g., a **semaphore-based** approach, **adaptive memory usage throttling** , or customized rate limiting), see [Advanced Multi-URL Crawling](https://docs.crawl4ai.com/core/advanced/multi-url-crawling/>).\n## 8. Dynamic Content Example\nSome sites require multiple “page clicks” or dynamic JavaScript updates. Below is an example showing how to **click** a “Next Page” button and wait for new commits to load on GitHub, using **`BrowserConfig`**and**`CrawlerRunConfig`**:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nasync def extract_structured_data_using_css_extractor():\n  print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n  schema = {\n    \"name\": \"KidoCode Courses\",\n    \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n    \"fields\": [\n      {\n        \"name\": \"section_title\",\n        \"selector\": \"h3.heading-50\",\n        \"type\": \"text\",\n      },\n      {\n        \"name\": \"section_description\",\n        \"selector\": \".charge-content\",\n        \"type\": \"text\",\n      },\n      {\n        \"name\": \"course_name\",\n        \"selector\": \".text-block-93\",\n        \"type\": \"text\",\n      },\n      {\n        \"name\": \"course_description\",\n        \"selector\": \".course-content-text\",\n        \"type\": \"text\",\n      },\n      {\n        \"name\": \"course_icon\",\n        \"selector\": \".image-92\",\n        \"type\": \"attribute\",\n        \"attribute\": \"src\",\n      },\n    ],\n  }\n  browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n  js_click_tabs = \"\"\"\n  (async () => {\n    const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n    for(let tab of tabs) {\n      tab.scrollIntoView();\n      tab.click();\n      await new Promise(r => setTimeout(r, 500));\n    }\n  })();\n  \"\"\"\n  crawler_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    extraction_strategy=JsonCssExtractionStrategy(schema),\n    js_code=[js_click_tabs],\n  )\n  async with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(\n      url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n    )\n    companies = json.loads(result.extracted_content)\n    print(f\"Successfully extracted {len(companies)} companies\")\n    print(json.dumps(companies[0], indent=2))\nasync def main():\n  await extract_structured_data_using_css_extractor()\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Key Points** :\n  * **`BrowserConfig(headless=False)`**: We want to watch it click “Next Page.”\n  * **`CrawlerRunConfig(...)`**: We specify the extraction strategy, pass`session_id` to reuse the same page. \n  * **`js_code`**and**`wait_for`**are used for subsequent pages (`page > 0`) to click the “Next” button and wait for new commits to load. \n  * **`js_only=True`**indicates we’re not re-navigating but continuing the existing session.\n  * Finally, we call `kill_session()` to clean up the page and browser session.\n\n\n## 9. Next Steps\nCongratulations! You have:\n  1. Performed a basic crawl and printed Markdown. \n  2. Used **content filters** with a markdown generator. \n  3. Extracted JSON via **CSS** or **LLM** strategies. \n  4. Handled **dynamic** pages with JavaScript triggers.\n\n\nIf you’re ready for more, check out:\n  * **Installation** : A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. \n  * **Hooks & Auth**: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. \n  * **Deployment** : Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. \n  * **Browser Management** : Delve into user simulation, stealth modes, and concurrency best practices. \n\n\nCrawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!\n##### Search\nxClose\nType to start searching\n",
  "links": [
    "https://docs.crawl4ai.com",
    "https://docs.crawl4ai.com/advanced/advanced-features",
    "https://docs.crawl4ai.com/advanced/crawl-dispatcher",
    "https://docs.crawl4ai.com/advanced/file-downloading",
    "https://docs.crawl4ai.com/advanced/hooks-auth",
    "https://docs.crawl4ai.com/advanced/identity-based-crawling",
    "https://docs.crawl4ai.com/advanced/lazy-loading",
    "https://docs.crawl4ai.com/advanced/multi-url-crawling",
    "https://docs.crawl4ai.com/advanced/proxy-security",
    "https://docs.crawl4ai.com/advanced/session-management",
    "https://docs.crawl4ai.com/advanced/ssl-certificate",
    "https://docs.crawl4ai.com/api/arun",
    "https://docs.crawl4ai.com/api/arun_many",
    "https://docs.crawl4ai.com/api/async-webcrawler",
    "https://docs.crawl4ai.com/api/crawl-result",
    "https://docs.crawl4ai.com/api/parameters",
    "https://docs.crawl4ai.com/api/strategies",
    "https://docs.crawl4ai.com/blog",
    "https://docs.crawl4ai.com/browser-crawler-config",
    "https://docs.crawl4ai.com/cache-modes",
    "https://docs.crawl4ai.com/content-selection",
    "https://docs.crawl4ai.com/core",
    "https://docs.crawl4ai.com/crawler-result",
    "https://docs.crawl4ai.com/docker-deploymeny",
    "https://docs.crawl4ai.com/extraction/chunking",
    "https://docs.crawl4ai.com/extraction/clustring-strategies",
    "https://docs.crawl4ai.com/extraction/llm-strategies",
    "https://docs.crawl4ai.com/extraction/no-llm-strategies",
    "https://docs.crawl4ai.com/fit-markdown",
    "https://docs.crawl4ai.com/installation",
    "https://docs.crawl4ai.com/link-media",
    "https://docs.crawl4ai.com/local-files",
    "https://docs.crawl4ai.com/markdown-generation",
    "https://docs.crawl4ai.com/page-interaction",
    "https://docs.crawl4ai.com/simple-crawling"
  ],
  "depth": 1,
  "stats": {
    "processed": 25,
    "total": 0,
    "depth": 1,
    "elapsed": "0:00:31",
    "page_limit": 34
  }
}