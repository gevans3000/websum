{
  "url": "https://docs.crawl4ai.com/advanced/multi-url-crawling",
  "timestamp": "2025-02-06T13:23:21.170214",
  "html": "<!DOCTYPE html><html lang=\"en\" style=\"scroll-padding-top: 50px;\"><head>\n    \n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n    <meta name=\"generator\" content=\"mkdocs-1.6.0, mkdocs-terminal-4.4.0\">\n    \n    <meta name=\"description\" content=\"ðŸš€ðŸ¤– Crawl4AI, Open-source LLM-Friendly Web Crawler &amp; Scraper\"> \n     \n    \n    <link rel=\"canonical\" href=\"https://docs.crawl4ai.com/advanced/multi-url-crawling/\"><link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"../../img/android-chrome-192x192.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"512x512\" href=\"../../img/android-chrome-512x512.png\">\n<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"../../img/apple-touch-icon.png\">\n<link rel=\"shortcut icon\" type=\"image/png\" sizes=\"48x48\" href=\"../../img/favicon.ico\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"../../img/favicon-16x16.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"../../img/favicon-32x32.png\">\n\n\n    \n \n<title>Multi-URL Crawling - Crawl4AI Documentation (v0.4.3bx)</title>\n\n\n<link href=\"../../css/fontawesome/css/fontawesome.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/fontawesome/css/solid.min.css\" rel=\"stylesheet\">\n<link href=\"../../css/normalize.css\" rel=\"stylesheet\">\n<link href=\"../../css/terminal.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.tile_grid.css\" rel=\"stylesheet\">\n<link href=\"../../css/theme.footer.css\" rel=\"stylesheet\">\n<!-- dark color palette -->\n<link href=\"../../css/palettes/dark.css\" rel=\"stylesheet\">\n\n<!-- page layout -->\n<style>\n/* initially set page layout to a one column grid */\n.terminal-mkdocs-main-grid {\n    display: grid;\n    grid-column-gap: 1.4em;\n    grid-template-columns: auto;\n    grid-template-rows: auto;\n}\n\n/*  \n*   when side navigation is not hidden, use a two column grid.  \n*   if the screen is too narrow, fall back to the initial one column grid layout.\n*   in this case the main content will be placed under the navigation panel. \n*/\n@media only screen and (min-width: 70em) {\n    .terminal-mkdocs-main-grid {\n        grid-template-columns: 4fr 9fr;\n    }\n}</style>\n\n\n\n    \n    <link href=\"../../assets/styles.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/highlight.css\" rel=\"stylesheet\"> \n    <link href=\"../../assets/dmvendor.css\" rel=\"stylesheet\">  \n    \n    \n\n    \n    <!-- search css support -->\n<link href=\"../../css/search/bootstrap-modal.css\" rel=\"stylesheet\">\n<!-- search scripts -->\n<script>\n    var base_url = \"../..\",\n    shortcuts = \"{}\";\n</script>\n<script src=\"../../js/jquery/jquery-1.10.1.min.js\" defer=\"\"></script>\n<script src=\"../../js/bootstrap/bootstrap.min.js\" defer=\"\"></script>\n<script src=\"../../js/mkdocs/base.js\" defer=\"\"></script>\n    \n    \n    \n    \n    <script src=\"../../assets/highlight.min.js\"></script>\n    \n    <script src=\"../../assets/highlight_init.js\"></script>\n    \n    <script src=\"https://buttons.github.io/buttons.js\"></script>\n    \n    <script src=\"../../search/main.js\"></script>\n    \n\n    \n</head>\n\n<body class=\"terminal\" style=\"\"><div class=\"container\">\n    <div class=\"terminal-nav\">\n        <header class=\"terminal-logo\">\n            <div id=\"mkdocs-terminal-site-name\" class=\"logo terminal-prompt\"><a href=\"https://docs.crawl4ai.com/\" class=\"no-style\">Crawl4AI Documentation (v0.4.3bx)</a></div>\n        </header>\n        \n        <nav class=\"terminal-menu\">\n            \n            <ul vocab=\"https://schema.org/\" typeof=\"BreadcrumbList\">\n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../..\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Home</span>\n                    </a>\n                    <meta property=\"position\" content=\"0\">\n                </li>\n                \n                \n                \n                \n                <li property=\"itemListElement\" typeof=\"ListItem\">\n                    <a href=\"../../core/quickstart/\" class=\"menu-item \" property=\"item\" typeof=\"WebPage\">\n                        <span property=\"name\">Quick Start</span>\n                    </a>\n                    <meta property=\"position\" content=\"1\">\n                </li>\n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                \n                    \n                    \n\n\n<li property=\"itemListElement\" typeof=\"ListItem\">\n    <a href=\"#\" class=\"menu-item\" data-toggle=\"modal\" data-target=\"#mkdocs_search_modal\" property=\"item\" typeof=\"SearchAction\">\n        <i aria-hidden=\"true\" class=\"fa fa-search\"></i> <span property=\"name\">Search</span>\n    </a>\n    <meta property=\"position\" content=\"2\">\n</li>\n                    \n            </ul>\n            \n        </nav>\n    </div>\n</div>\n        \n    <div class=\"container\">\n        <div class=\"terminal-mkdocs-main-grid\"><aside id=\"terminal-mkdocs-side-panel\"><nav>\n  \n    <ul class=\"terminal-mkdocs-side-nav-items\">\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../..\">Home</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Setup &amp; Installation</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/installation/\">Installation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/docker-deploymeny/\">Docker Deployment</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/quickstart/\">Quick Start</a>\n        \n    \n    \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Blog &amp; Changelog</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../blog/\">Blog Home</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md\">Changelog</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Core</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/simple-crawling/\">Simple Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/crawler-result/\">Crawler Result</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/browser-crawler-config/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/markdown-generation/\">Markdown Generation</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/fit-markdown/\">Fit Markdown</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/page-interaction/\">Page Interaction</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/content-selection/\">Content Selection</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/cache-modes/\">Cache Modes</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/local-files/\">Local Files &amp; Raw HTML</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../core/link-media/\">Link &amp; Media</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item--active terminal-mkdocs-side-nav-section-no-index\">Advanced</span>\n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../advanced-features/\">Overview</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../file-downloading/\">File Downloading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../lazy-loading/\">Lazy Loading</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../hooks-auth/\">Hooks &amp; Auth</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../proxy-security/\">Proxy &amp; Security</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../session-management/\">Session Management</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        <span class=\"\n\n    terminal-mkdocs-side-nav-item--active\">Multi-URL Crawling</span>\n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../crawl-dispatcher/\">Crawl Dispatcher</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../identity-based-crawling/\">Identity Based Crawling</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../ssl-certificate/\">SSL Certificate</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">Extraction</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/no-llm-strategies/\">LLM-Free Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/llm-strategies/\">LLM Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/clustring-strategies/\">Clustering Strategies</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../extraction/chunking/\">Chunking</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n          \n\n\n\n<li class=\"terminal-mkdocs-side-nav-li\">\n    \n    \n        \n        \n\n        \n            \n    \n        \n        \n            \n            \n            <span class=\"\n        \n    \n\n    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index\">API Reference</span>\n        \n    \n    \n        \n      \n        \n            <ul class=\"terminal-mkdocs-side-nav-li-ul\">\n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/async-webcrawler/\">AsyncWebCrawler</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun/\">arun()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/arun_many/\">arun_many()</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/parameters/\">Browser &amp; Crawler Config</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/crawl-result/\">CrawlResult</a>\n        \n    \n    </li>\n            \n        \n            \n            \n\n             \n                <li class=\"terminal-mkdocs-side-nav-li-ul-li\">\n    \n        \n        \n            <a class=\"\n\n    terminal-mkdocs-side-nav-item\" href=\"../../api/strategies/\">Strategies</a>\n        \n    \n    </li>\n            \n            \n    </ul>\n        \n    \n  </li>\n        \n    </ul>\n  \n</nav><hr>\n<nav>\n    <ul>\n        <li><a href=\"#advanced-multi-url-crawling-with-dispatchers\">Advanced Multi-URL Crawling with Dispatchers</a></li>\n        <li><a href=\"#1-introduction\">1. Introduction</a></li><li><a href=\"#2-core-components\">2. Core Components</a></li><li><a href=\"#3-available-dispatchers\">3. Available Dispatchers</a></li><li><a href=\"#4-usage-examples\">4. Usage Examples</a></li><li><a href=\"#5-dispatch-results\">5. Dispatch Results</a></li><li><a href=\"#6-summary\">6. Summary</a></li>\n    </ul>\n</nav>\n</aside>\n            <main id=\"terminal-mkdocs-main-content\">\n<section id=\"mkdocs-terminal-content\">\n    <h1 id=\"advanced-multi-url-crawling-with-dispatchers\">Advanced Multi-URL Crawling with Dispatchers</h1>\n<blockquote>\n<p><strong>Heads Up</strong>: Crawl4AI supports advanced dispatchers for <strong>parallel</strong> or <strong>throttled</strong> crawling, providing dynamic rate limiting and memory usage checks. The built-in <code>arun_many()</code> function uses these dispatchers to handle concurrency efficiently.</p>\n</blockquote>\n<h2 id=\"1-introduction\">1. Introduction</h2>\n<p>When crawling many URLs:</p>\n<ul>\n<li><strong>Basic</strong>: Use <code>arun()</code> in a loop (simple but less efficient)</li>\n<li><strong>Better</strong>: Use <code>arun_many()</code>, which efficiently handles multiple URLs with proper concurrency control</li>\n<li><strong>Best</strong>: Customize dispatcher behavior for your specific needs (memory management, rate limits, etc.)</li>\n</ul>\n<p><strong>Why Dispatchers?</strong>  </p>\n<ul>\n<li><strong>Adaptive</strong>: Memory-based dispatchers can pause or slow down based on system resources</li>\n<li><strong>Rate-limiting</strong>: Built-in rate limiting with exponential backoff for 429/503 responses</li>\n<li><strong>Real-time Monitoring</strong>: Live dashboard of ongoing tasks, memory usage, and performance</li>\n<li><strong>Flexibility</strong>: Choose between memory-adaptive or semaphore-based concurrency</li>\n</ul>\n<hr>\n<h2 id=\"2-core-components\">2. Core Components</h2>\n<h3 id=\"21-rate-limiter\">2.1 Rate Limiter</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">RateLimiter</span>:\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">\n        <span class=\"hljs-comment\"># Random delay range between requests</span>\n        base_delay: <span class=\"hljs-type\">Tuple</span>[<span class=\"hljs-built_in\">float</span>, <span class=\"hljs-built_in\">float</span>] = (<span class=\"hljs-params\"><span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">3.0</span></span>),  \n\n        <span class=\"hljs-comment\"># Maximum backoff delay</span>\n        max_delay: <span class=\"hljs-built_in\">float</span> = <span class=\"hljs-number\">60.0</span>,                        \n\n        <span class=\"hljs-comment\"># Retries before giving up</span>\n        max_retries: <span class=\"hljs-built_in\">int</span> = <span class=\"hljs-number\">3</span>,                          \n\n        <span class=\"hljs-comment\"># Status codes triggering backoff</span>\n        rate_limit_codes: <span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\">int</span>] = [<span class=\"hljs-number\">429</span>, <span class=\"hljs-number\">503</span>]        \n    </span>)\n</code></pre></div>\n<p>Hereâ€™s the revised and simplified explanation of the <strong>RateLimiter</strong>, focusing on constructor parameters and adhering to your markdown style and mkDocs guidelines.</p>\n<h4 id=\"ratelimiter-constructor-parameters\">RateLimiter Constructor Parameters</h4>\n<p>The <strong>RateLimiter</strong> is a utility that helps manage the pace of requests to avoid overloading servers or getting blocked due to rate limits. It operates internally to delay requests and handle retries but can be configured using its constructor parameters.</p>\n<p><strong>Parameters of the <code>RateLimiter</code> constructor:</strong></p>\n<p>1.â€‚<strong><code>base_delay</code></strong> (<code>Tuple[float, float]</code>, default: <code>(1.0, 3.0)</code>)<br>\nâ€‚â€‚The range for a random delay (in seconds) between consecutive requests to the same domain.</p>\n<ul>\n<li>A random delay is chosen between <code>base_delay[0]</code> and <code>base_delay[1]</code> for each request.  </li>\n<li>This prevents sending requests at a predictable frequency, reducing the chances of triggering rate limits.</li>\n</ul>\n<p><strong>Example:</strong><br>\nIf <code>base_delay = (2.0, 5.0)</code>, delays could be randomly chosen as <code>2.3s</code>, <code>4.1s</code>, etc.</p>\n<hr>\n<p>2.â€‚<strong><code>max_delay</code></strong> (<code>float</code>, default: <code>60.0</code>)<br>\nâ€‚â€‚The maximum allowable delay when rate-limiting errors occur.</p>\n<ul>\n<li>When servers return rate-limit responses (e.g., 429 or 503), the delay increases exponentially with jitter.  </li>\n<li>The <code>max_delay</code> ensures the delay doesnâ€™t grow unreasonably high, capping it at this value.</li>\n</ul>\n<p><strong>Example:</strong><br>\nFor a <code>max_delay = 30.0</code>, even if backoff calculations suggest a delay of <code>45s</code>, it will cap at <code>30s</code>.</p>\n<hr>\n<p>3.â€‚<strong><code>max_retries</code></strong> (<code>int</code>, default: <code>3</code>)<br>\nâ€‚â€‚The maximum number of retries for a request if rate-limiting errors occur.</p>\n<ul>\n<li>After encountering a rate-limit response, the <code>RateLimiter</code> retries the request up to this number of times.  </li>\n<li>If all retries fail, the request is marked as failed, and the process continues.</li>\n</ul>\n<p><strong>Example:</strong><br>\nIf <code>max_retries = 3</code>, the system retries a failed request three times before giving up.</p>\n<hr>\n<p>4.â€‚<strong><code>rate_limit_codes</code></strong> (<code>List[int]</code>, default: <code>[429, 503]</code>)<br>\nâ€‚â€‚A list of HTTP status codes that trigger the rate-limiting logic.</p>\n<ul>\n<li>These status codes indicate the server is overwhelmed or actively limiting requests.  </li>\n<li>You can customize this list to include other codes based on specific server behavior.</li>\n</ul>\n<p><strong>Example:</strong><br>\nIf <code>rate_limit_codes = [429, 503, 504]</code>, the crawler will back off on these three error codes.</p>\n<hr>\n<p><strong>How to Use the <code>RateLimiter</code>:</strong></p>\n<p>Hereâ€™s an example of initializing and using a <code>RateLimiter</code> in your project:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-graphql\">from crawl4ai import RateLimiter\n\n<span class=\"hljs-comment\"># Create a RateLimiter with custom settings</span>\nrate_limiter <span class=\"hljs-punctuation\">=</span> RateLimiter<span class=\"hljs-punctuation\">(</span>\n    base_delay<span class=\"hljs-punctuation\">=</span><span class=\"hljs-punctuation\">(</span><span class=\"hljs-number\">2.0</span>, <span class=\"hljs-number\">4.0</span><span class=\"hljs-punctuation\">)</span>,  <span class=\"hljs-comment\"># Random delay between 2-4 seconds</span>\n    max_delay<span class=\"hljs-punctuation\">=</span><span class=\"hljs-number\">30.0</span>,         <span class=\"hljs-comment\"># Cap delay at 30 seconds</span>\n    max_retries<span class=\"hljs-punctuation\">=</span><span class=\"hljs-number\">5</span>,          <span class=\"hljs-comment\"># Retry up to 5 times on rate-limiting errors</span>\n    rate_limit_codes<span class=\"hljs-punctuation\">=</span><span class=\"hljs-punctuation\">[</span><span class=\"hljs-number\">429</span>, <span class=\"hljs-number\">503</span><span class=\"hljs-punctuation\">]</span>  <span class=\"hljs-comment\"># Handle these HTTP status codes</span>\n<span class=\"hljs-punctuation\">)</span>\n\n<span class=\"hljs-comment\"># RateLimiter will handle delays and retries internally</span>\n<span class=\"hljs-comment\"># No additional setup is required for its operation</span>\n</code></pre></div>\n<p>The <code>RateLimiter</code> integrates seamlessly with dispatchers like <code>MemoryAdaptiveDispatcher</code> and <code>SemaphoreDispatcher</code>, ensuring requests are paced correctly without user intervention. Its internal mechanisms manage delays and retries to avoid overwhelming servers while maximizing efficiency.</p>\n<h3 id=\"22-crawler-monitor\">2.2 Crawler Monitor</h3>\n<p>The CrawlerMonitor provides real-time visibility into crawling operations:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> CrawlerMonitor, DisplayMode\nmonitor = CrawlerMonitor(\n    <span class=\"hljs-comment\"># Maximum rows in live display</span>\n    max_visible_rows=<span class=\"hljs-number\">15</span>,          \n\n    <span class=\"hljs-comment\"># DETAILED or AGGREGATED view</span>\n    display_mode=DisplayMode.DETAILED  \n)\n</code></pre></div>\n<p><strong>Display Modes</strong>:</p>\n<ol>\n<li><strong>DETAILED</strong>: Shows individual task status, memory usage, and timing</li>\n<li><strong>AGGREGATED</strong>: Displays summary statistics and overall progress</li>\n</ol>\n<hr>\n<h2 id=\"3-available-dispatchers\">3. Available Dispatchers</h2>\n<h3 id=\"31-memoryadaptivedispatcher-default\">3.1 MemoryAdaptiveDispatcher (Default)</h3>\n<p>Automatically manages concurrency based on system memory usage:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> crawl4ai.async_dispatcher <span class=\"hljs-keyword\">import</span> MemoryAdaptiveDispatcher\n\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=<span class=\"hljs-number\">90.0</span>,  <span class=\"hljs-comment\"># Pause if memory exceeds this</span>\n    check_interval=<span class=\"hljs-number\">1.0</span>,             <span class=\"hljs-comment\"># How often to check memory</span>\n    max_session_permit=<span class=\"hljs-number\">10</span>,          <span class=\"hljs-comment\"># Maximum concurrent tasks</span>\n    rate_limiter=RateLimiter(       <span class=\"hljs-comment\"># Optional rate limiting</span>\n        base_delay=(<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">2.0</span>),\n        max_delay=<span class=\"hljs-number\">30.0</span>,\n        max_retries=<span class=\"hljs-number\">2</span>\n    ),\n    monitor=CrawlerMonitor(         <span class=\"hljs-comment\"># Optional monitoring</span>\n        max_visible_rows=<span class=\"hljs-number\">15</span>,\n        display_mode=DisplayMode.DETAILED\n    )\n)\n</code></pre></div>\n<p><strong>Constructor Parameters:</strong></p>\n<p>1.â€‚<strong><code>memory_threshold_percent</code></strong> (<code>float</code>, default: <code>90.0</code>)<br>\nâ€‚â€‚Specifies the memory usage threshold (as a percentage). If system memory usage exceeds this value, the dispatcher pauses crawling to prevent system overload.</p>\n<p>2.â€‚<strong><code>check_interval</code></strong> (<code>float</code>, default: <code>1.0</code>)<br>\nâ€‚â€‚The interval (in seconds) at which the dispatcher checks system memory usage.</p>\n<p>3.â€‚<strong><code>max_session_permit</code></strong> (<code>int</code>, default: <code>10</code>)<br>\nâ€‚â€‚The maximum number of concurrent crawling tasks allowed. This ensures resource limits are respected while maintaining concurrency.</p>\n<p>4.â€‚<strong><code>memory_wait_timeout</code></strong> (<code>float</code>, default: <code>300.0</code>)<br>\nâ€‚â€‚Optional timeout (in seconds). If memory usage exceeds <code>memory_threshold_percent</code> for longer than this duration, a <code>MemoryError</code> is raised.</p>\n<p>5.â€‚<strong><code>rate_limiter</code></strong> (<code>RateLimiter</code>, default: <code>None</code>)<br>\nâ€‚â€‚Optional rate-limiting logic to avoid server-side blocking (e.g., for handling 429 or 503 errors). See <strong>RateLimiter</strong> for details.</p>\n<p>6.â€‚<strong><code>monitor</code></strong> (<code>CrawlerMonitor</code>, default: <code>None</code>)<br>\nâ€‚â€‚Optional monitoring for real-time task tracking and performance insights. See <strong>CrawlerMonitor</strong> for details.</p>\n<hr>\n<h3 id=\"32-semaphoredispatcher\">3.2 SemaphoreDispatcher</h3>\n<p>Provides simple concurrency control with a fixed limit:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> crawl4ai.async_dispatcher <span class=\"hljs-keyword\">import</span> SemaphoreDispatcher\n\ndispatcher = SemaphoreDispatcher(\n    max_session_permit=<span class=\"hljs-number\">20</span>,         <span class=\"hljs-comment\"># Maximum concurrent tasks</span>\n    rate_limiter=RateLimiter(      <span class=\"hljs-comment\"># Optional rate limiting</span>\n        base_delay=(<span class=\"hljs-number\">0.5</span>, <span class=\"hljs-number\">1.0</span>),\n        max_delay=<span class=\"hljs-number\">10.0</span>\n    ),\n    monitor=CrawlerMonitor(        <span class=\"hljs-comment\"># Optional monitoring</span>\n        max_visible_rows=<span class=\"hljs-number\">15</span>,\n        display_mode=DisplayMode.DETAILED\n    )\n)\n</code></pre></div>\n<p><strong>Constructor Parameters:</strong></p>\n<p>1.â€‚<strong><code>max_session_permit</code></strong> (<code>int</code>, default: <code>20</code>)<br>\nâ€‚â€‚The maximum number of concurrent crawling tasks allowed, irrespective of semaphore slots.</p>\n<p>2.â€‚<strong><code>rate_limiter</code></strong> (<code>RateLimiter</code>, default: <code>None</code>)<br>\nâ€‚â€‚Optional rate-limiting logic to avoid overwhelming servers. See <strong>RateLimiter</strong> for details.</p>\n<p>3.â€‚<strong><code>monitor</code></strong> (<code>CrawlerMonitor</code>, default: <code>None</code>)<br>\nâ€‚â€‚Optional monitoring for tracking task progress and resource usage. See <strong>CrawlerMonitor</strong> for details.</p>\n<hr>\n<h2 id=\"4-usage-examples\">4. Usage Examples</h2>\n<h3 id=\"41-batch-processing-default\">4.1 Batch Processing (Default)</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">crawl_batch</span>():\n    browser_config = BrowserConfig(headless=<span class=\"hljs-literal\">True</span>, verbose=<span class=\"hljs-literal\">False</span>)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=<span class=\"hljs-literal\">False</span>  <span class=\"hljs-comment\"># Default: get all results at once</span>\n    )\n\n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=<span class=\"hljs-number\">70.0</span>,\n        check_interval=<span class=\"hljs-number\">1.0</span>,\n        max_session_permit=<span class=\"hljs-number\">10</span>,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_config) <span class=\"hljs-keyword\">as</span> crawler:\n        <span class=\"hljs-comment\"># Get all results at once</span>\n        results = <span class=\"hljs-keyword\">await</span> crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        )\n\n        <span class=\"hljs-comment\"># Process all results after completion</span>\n        <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> results:\n            <span class=\"hljs-keyword\">if</span> result.success:\n                <span class=\"hljs-keyword\">await</span> process_result(result)\n            <span class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Failed to crawl <span class=\"hljs-subst\">{result.url}</span>: <span class=\"hljs-subst\">{result.error_message}</span>\"</span>)\n</code></pre></div>\n<p><strong>Review:</strong><br>\n- <strong>Purpose:</strong> Executes a batch crawl with all URLs processed together after crawling is complete.<br>\n- <strong>Dispatcher:</strong> Uses <code>MemoryAdaptiveDispatcher</code> to manage concurrency and system memory.<br>\n- <strong>Stream:</strong> Disabled (<code>stream=False</code>), so all results are collected at once for post-processing.<br>\n- <strong>Best Use Case:</strong> When you need to analyze results in bulk rather than individually during the crawl.</p>\n<hr>\n<h3 id=\"42-streaming-mode\">4.2 Streaming Mode</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">crawl_streaming</span>():\n    browser_config = BrowserConfig(headless=<span class=\"hljs-literal\">True</span>, verbose=<span class=\"hljs-literal\">False</span>)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=<span class=\"hljs-literal\">True</span>  <span class=\"hljs-comment\"># Enable streaming mode</span>\n    )\n\n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=<span class=\"hljs-number\">70.0</span>,\n        check_interval=<span class=\"hljs-number\">1.0</span>,\n        max_session_permit=<span class=\"hljs-number\">10</span>,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_config) <span class=\"hljs-keyword\">as</span> crawler:\n        <span class=\"hljs-comment\"># Process results as they become available</span>\n        <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> <span class=\"hljs-keyword\">await</span> crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        ):\n            <span class=\"hljs-keyword\">if</span> result.success:\n                <span class=\"hljs-comment\"># Process each result immediately</span>\n                <span class=\"hljs-keyword\">await</span> process_result(result)\n            <span class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Failed to crawl <span class=\"hljs-subst\">{result.url}</span>: <span class=\"hljs-subst\">{result.error_message}</span>\"</span>)\n</code></pre></div>\n<p><strong>Review:</strong><br>\n- <strong>Purpose:</strong> Enables streaming to process results as soon as theyâ€™re available.<br>\n- <strong>Dispatcher:</strong> Uses <code>MemoryAdaptiveDispatcher</code> for concurrency and memory management.<br>\n- <strong>Stream:</strong> Enabled (<code>stream=True</code>), allowing real-time processing during crawling.<br>\n- <strong>Best Use Case:</strong> When you need to act on results immediately, such as for real-time analytics or progressive data storage.</p>\n<hr>\n<h3 id=\"43-semaphore-based-crawling\">4.3 Semaphore-based Crawling</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">crawl_with_semaphore</span>(<span class=\"hljs-params\">urls</span>):\n    browser_config = BrowserConfig(headless=<span class=\"hljs-literal\">True</span>, verbose=<span class=\"hljs-literal\">False</span>)\n    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n    dispatcher = SemaphoreDispatcher(\n        semaphore_count=<span class=\"hljs-number\">5</span>,\n        rate_limiter=RateLimiter(\n            base_delay=(<span class=\"hljs-number\">0.5</span>, <span class=\"hljs-number\">1.0</span>),\n            max_delay=<span class=\"hljs-number\">10.0</span>\n        ),\n        monitor=CrawlerMonitor(\n            max_visible_rows=<span class=\"hljs-number\">15</span>,\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler(config=browser_config) <span class=\"hljs-keyword\">as</span> crawler:\n        results = <span class=\"hljs-keyword\">await</span> crawler.arun_many(\n            urls, \n            config=run_config,\n            dispatcher=dispatcher\n        )\n        <span class=\"hljs-keyword\">return</span> results\n</code></pre></div>\n<p><strong>Review:</strong><br>\n- <strong>Purpose:</strong> Uses <code>SemaphoreDispatcher</code> to limit concurrency with a fixed number of slots.<br>\n- <strong>Dispatcher:</strong> Configured with a semaphore to control parallel crawling tasks.<br>\n- <strong>Rate Limiter:</strong> Prevents servers from being overwhelmed by pacing requests.<br>\n- <strong>Best Use Case:</strong> When you want precise control over the number of concurrent requests, independent of system memory.</p>\n<hr>\n<h3 id=\"44-robotstxt-consideration\">4.4 Robots.txt Consideration</h3>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> crawl4ai <span class=\"hljs-keyword\">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    urls = [\n        <span class=\"hljs-string\">\"https://example1.com\"</span>,\n        <span class=\"hljs-string\">\"https://example2.com\"</span>,\n        <span class=\"hljs-string\">\"https://example3.com\"</span>\n    ]\n\n    config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=<span class=\"hljs-literal\">True</span>,  <span class=\"hljs-comment\"># Will respect robots.txt for each URL</span>\n        semaphore_count=<span class=\"hljs-number\">3</span>      <span class=\"hljs-comment\"># Max concurrent requests</span>\n    )\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">with</span> AsyncWebCrawler() <span class=\"hljs-keyword\">as</span> crawler:\n        <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> crawler.arun_many(urls, config=config):\n            <span class=\"hljs-keyword\">if</span> result.success:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Successfully crawled <span class=\"hljs-subst\">{result.url}</span>\"</span>)\n            <span class=\"hljs-keyword\">elif</span> result.status_code == <span class=\"hljs-number\">403</span> <span class=\"hljs-keyword\">and</span> <span class=\"hljs-string\">\"robots.txt\"</span> <span class=\"hljs-keyword\">in</span> result.error_message:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Skipped <span class=\"hljs-subst\">{result.url}</span> - blocked by robots.txt\"</span>)\n            <span class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Failed to crawl <span class=\"hljs-subst\">{result.url}</span>: <span class=\"hljs-subst\">{result.error_message}</span>\"</span>)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    asyncio.run(main())\n</code></pre></div>\n<p><strong>Review:</strong><br>\n- <strong>Purpose:</strong> Ensures compliance with <code>robots.txt</code> rules for ethical and legal web crawling.<br>\n- <strong>Configuration:</strong> Set <code>check_robots_txt=True</code> to validate each URL against <code>robots.txt</code> before crawling.<br>\n- <strong>Dispatcher:</strong> Handles requests with concurrency limits (<code>semaphore_count=3</code>).<br>\n- <strong>Best Use Case:</strong> When crawling websites that strictly enforce robots.txt policies or for responsible crawling practices.</p>\n<hr>\n<h2 id=\"5-dispatch-results\">5. Dispatch Results</h2>\n<p>Each crawl result includes dispatch information:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-css\"><span class=\"hljs-keyword\">@dataclass</span>\nclass <span class=\"hljs-attribute\">DispatchResult</span>:\n    task_<span class=\"hljs-attribute\">id</span>: str\n    memory_<span class=\"hljs-attribute\">usage</span>: float\n    peak_<span class=\"hljs-attribute\">memory</span>: float\n    start_<span class=\"hljs-attribute\">time</span>: datetime\n    end_<span class=\"hljs-attribute\">time</span>: datetime\n    error_<span class=\"hljs-attribute\">message</span>: str = <span class=\"hljs-string\">\"\"</span>\n</code></pre></div>\n<p>Access via <code>result.dispatch_result</code>:</p>\n<div class=\"highlight\"><pre><span></span><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">for</span> result <span class=\"hljs-keyword\">in</span> results:\n    <span class=\"hljs-keyword\">if</span> result.success:\n        dr = result.dispatch_result\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"URL: <span class=\"hljs-subst\">{result.url}</span>\"</span>)\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Memory: <span class=\"hljs-subst\">{dr.memory_usage:<span class=\"hljs-number\">.1</span>f}</span>MB\"</span>)\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Duration: <span class=\"hljs-subst\">{dr.end_time - dr.start_time}</span>\"</span>)\n</code></pre></div>\n<h2 id=\"6-summary\">6. Summary</h2>\n<p>1.â€‚<strong>Two Dispatcher Types</strong>:</p>\n<ul>\n<li>MemoryAdaptiveDispatcher (default): Dynamic concurrency based on memory</li>\n<li>SemaphoreDispatcher: Fixed concurrency limit</li>\n</ul>\n<p>2.â€‚<strong>Optional Components</strong>:</p>\n<ul>\n<li>RateLimiter: Smart request pacing and backoff</li>\n<li>CrawlerMonitor: Real-time progress visualization</li>\n</ul>\n<p>3.â€‚<strong>Key Benefits</strong>:</p>\n<ul>\n<li>Automatic memory management</li>\n<li>Built-in rate limiting</li>\n<li>Live progress monitoring</li>\n<li>Flexible concurrency control</li>\n</ul>\n<p>Choose the dispatcher that best fits your needs:</p>\n<ul>\n<li><strong>MemoryAdaptiveDispatcher</strong>: For large crawls or limited resources</li>\n<li><strong>SemaphoreDispatcher</strong>: For simple, fixed-concurrency scenarios</li>\n</ul>\n</section>\n\n            </main>\n        </div>\n        <hr><footer>\n    <div class=\"terminal-mkdocs-footer-grid\">\n        <div id=\"terminal-mkdocs-footer-copyright-info\">\n             Site built with <a href=\"http://www.mkdocs.org\">MkDocs</a> and <a href=\"https://github.com/ntno/mkdocs-terminal\">Terminal for MkDocs</a>.\n        </div>\n    </div>\n</footer>\n    </div>\n\n    \n    <div class=\"modal\" id=\"mkdocs_search_modal\" tabindex=\"-1\" role=\"alertdialog\" aria-modal=\"true\" aria-labelledby=\"searchModalLabel\">\n    <div class=\"modal-dialog modal-lg\" role=\"search\">\n        <div class=\"modal-content\">\n            <div class=\"modal-header\">\n                <h5 class=\"modal-title\" id=\"searchModalLabel\">Search</h5>\n                <button type=\"button\" class=\"close btn btn-default btn-ghost\" data-dismiss=\"modal\"><span aria-hidden=\"true\">x</span><span class=\"sr-only\">Close</span></button>\n            </div>\n            <div class=\"modal-body\">\n                <p id=\"searchInputLabel\">Type to start searching</p>\n                <form>\n                    <div class=\"form-group\">\n                        <input type=\"search\" class=\"form-control\" aria-labelledby=\"searchInputLabel\" placeholder=\"\" id=\"mkdocs-search-query\" title=\"Please enter search terms here\">\n                    </div>\n                </form>\n                <div id=\"mkdocs-search-results\" data-no-results-text=\"No document matches found\"></div>\n            </div>\n            <div class=\"modal-footer\">\n            </div>\n        </div>\n    </div>\n</div>\n    \n    \n\n\n</body></html>",
  "markdown": "# Advanced Multi-URL Crawling with Dispatchers\n> **Heads Up** : Crawl4AI supports advanced dispatchers for **parallel** or **throttled** crawling, providing dynamic rate limiting and memory usage checks. The built-in `arun_many()` function uses these dispatchers to handle concurrency efficiently.\n## 1. Introduction\nWhen crawling many URLs:\n  * **Basic** : Use `arun()` in a loop (simple but less efficient)\n  * **Better** : Use `arun_many()`, which efficiently handles multiple URLs with proper concurrency control\n  * **Best** : Customize dispatcher behavior for your specific needs (memory management, rate limits, etc.)\n\n\n**Why Dispatchers?**\n  * **Adaptive** : Memory-based dispatchers can pause or slow down based on system resources\n  * **Rate-limiting** : Built-in rate limiting with exponential backoff for 429/503 responses\n  * **Real-time Monitoring** : Live dashboard of ongoing tasks, memory usage, and performance\n  * **Flexibility** : Choose between memory-adaptive or semaphore-based concurrency\n\n\n## 2. Core Components\n### 2.1 Rate Limiter\n```\nclass RateLimiter:\n  def __init__(\n    # Random delay range between requests\n    base_delay: Tuple[float, float] = (1.0, 3.0), \n    # Maximum backoff delay\n    max_delay: float = 60.0,            \n    # Retries before giving up\n    max_retries: int = 3,             \n    # Status codes triggering backoff\n    rate_limit_codes: List[int] = [429, 503]    \n  )\n\n```\n\nHereâ€™s the revised and simplified explanation of the **RateLimiter** , focusing on constructor parameters and adhering to your markdown style and mkDocs guidelines.\n#### RateLimiter Constructor Parameters\nThe **RateLimiter** is a utility that helps manage the pace of requests to avoid overloading servers or getting blocked due to rate limits. It operates internally to delay requests and handle retries but can be configured using its constructor parameters.\n**Parameters of the`RateLimiter` constructor:**\n1. **`base_delay`**(`Tuple[float, float]` , default: `(1.0, 3.0)`) The range for a random delay (in seconds) between consecutive requests to the same domain.\n  * A random delay is chosen between `base_delay[0]` and `base_delay[1]` for each request. \n  * This prevents sending requests at a predictable frequency, reducing the chances of triggering rate limits.\n\n\n**Example:** If `base_delay = (2.0, 5.0)`, delays could be randomly chosen as `2.3s`, `4.1s`, etc.\n2. **`max_delay`**(`float` , default: `60.0`) The maximum allowable delay when rate-limiting errors occur.\n  * When servers return rate-limit responses (e.g., 429 or 503), the delay increases exponentially with jitter. \n  * The `max_delay` ensures the delay doesnâ€™t grow unreasonably high, capping it at this value.\n\n\n**Example:** For a `max_delay = 30.0`, even if backoff calculations suggest a delay of `45s`, it will cap at `30s`.\n3. **`max_retries`**(`int` , default: `3`) The maximum number of retries for a request if rate-limiting errors occur.\n  * After encountering a rate-limit response, the `RateLimiter` retries the request up to this number of times. \n  * If all retries fail, the request is marked as failed, and the process continues.\n\n\n**Example:** If `max_retries = 3`, the system retries a failed request three times before giving up.\n4. **`rate_limit_codes`**(`List[int]` , default: `[429, 503]`) A list of HTTP status codes that trigger the rate-limiting logic.\n  * These status codes indicate the server is overwhelmed or actively limiting requests. \n  * You can customize this list to include other codes based on specific server behavior.\n\n\n**Example:** If `rate_limit_codes = [429, 503, 504]`, the crawler will back off on these three error codes.\n**How to Use the`RateLimiter` :**\nHereâ€™s an example of initializing and using a `RateLimiter` in your project:\n```\nfrom crawl4ai import RateLimiter\n# Create a RateLimiter with custom settings\nrate_limiter = RateLimiter(\n  base_delay=(2.0, 4.0), # Random delay between 2-4 seconds\n  max_delay=30.0,     # Cap delay at 30 seconds\n  max_retries=5,     # Retry up to 5 times on rate-limiting errors\n  rate_limit_codes=[429, 503] # Handle these HTTP status codes\n)\n# RateLimiter will handle delays and retries internally\n# No additional setup is required for its operation\n\n```\n\nThe `RateLimiter` integrates seamlessly with dispatchers like `MemoryAdaptiveDispatcher` and `SemaphoreDispatcher`, ensuring requests are paced correctly without user intervention. Its internal mechanisms manage delays and retries to avoid overwhelming servers while maximizing efficiency.\n### 2.2 Crawler Monitor\nThe CrawlerMonitor provides real-time visibility into crawling operations:\n```\nfrom crawl4ai import CrawlerMonitor, DisplayMode\nmonitor = CrawlerMonitor(\n  # Maximum rows in live display\n  max_visible_rows=15,     \n  # DETAILED or AGGREGATED view\n  display_mode=DisplayMode.DETAILED \n)\n\n```\n\n**Display Modes** :\n  1. **DETAILED** : Shows individual task status, memory usage, and timing\n  2. **AGGREGATED** : Displays summary statistics and overall progress\n\n\n## 3. Available Dispatchers\n### 3.1 MemoryAdaptiveDispatcher (Default)\nAutomatically manages concurrency based on system memory usage:\n```\nfrom crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher\ndispatcher = MemoryAdaptiveDispatcher(\n  memory_threshold_percent=90.0, # Pause if memory exceeds this\n  check_interval=1.0,       # How often to check memory\n  max_session_permit=10,     # Maximum concurrent tasks\n  rate_limiter=RateLimiter(    # Optional rate limiting\n    base_delay=(1.0, 2.0),\n    max_delay=30.0,\n    max_retries=2\n  ),\n  monitor=CrawlerMonitor(     # Optional monitoring\n    max_visible_rows=15,\n    display_mode=DisplayMode.DETAILED\n  )\n)\n\n```\n\n**Constructor Parameters:**\n1. **`memory_threshold_percent`**(`float` , default: `90.0`) Specifies the memory usage threshold (as a percentage). If system memory usage exceeds this value, the dispatcher pauses crawling to prevent system overload.\n2. **`check_interval`**(`float` , default: `1.0`) The interval (in seconds) at which the dispatcher checks system memory usage.\n3. **`max_session_permit`**(`int` , default: `10`) The maximum number of concurrent crawling tasks allowed. This ensures resource limits are respected while maintaining concurrency.\n4. **`memory_wait_timeout`**(`float` , default: `300.0`) Optional timeout (in seconds). If memory usage exceeds `memory_threshold_percent` for longer than this duration, a `MemoryError` is raised.\n5. **`rate_limiter`**(`RateLimiter` , default: `None`) Optional rate-limiting logic to avoid server-side blocking (e.g., for handling 429 or 503 errors). See **RateLimiter** for details.\n6. **`monitor`**(`CrawlerMonitor` , default: `None`) Optional monitoring for real-time task tracking and performance insights. See **CrawlerMonitor** for details.\n### 3.2 SemaphoreDispatcher\nProvides simple concurrency control with a fixed limit:\n```\nfrom crawl4ai.async_dispatcher import SemaphoreDispatcher\ndispatcher = SemaphoreDispatcher(\n  max_session_permit=20,     # Maximum concurrent tasks\n  rate_limiter=RateLimiter(   # Optional rate limiting\n    base_delay=(0.5, 1.0),\n    max_delay=10.0\n  ),\n  monitor=CrawlerMonitor(    # Optional monitoring\n    max_visible_rows=15,\n    display_mode=DisplayMode.DETAILED\n  )\n)\n\n```\n\n**Constructor Parameters:**\n1. **`max_session_permit`**(`int` , default: `20`) The maximum number of concurrent crawling tasks allowed, irrespective of semaphore slots.\n2. **`rate_limiter`**(`RateLimiter` , default: `None`) Optional rate-limiting logic to avoid overwhelming servers. See **RateLimiter** for details.\n3. **`monitor`**(`CrawlerMonitor` , default: `None`) Optional monitoring for tracking task progress and resource usage. See **CrawlerMonitor** for details.\n## 4. Usage Examples\n### 4.1 Batch Processing (Default)\n```\nasync def crawl_batch():\n  browser_config = BrowserConfig(headless=True, verbose=False)\n  run_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    stream=False # Default: get all results at once\n  )\n  dispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=70.0,\n    check_interval=1.0,\n    max_session_permit=10,\n    monitor=CrawlerMonitor(\n      display_mode=DisplayMode.DETAILED\n    )\n  )\n  async with AsyncWebCrawler(config=browser_config) as crawler:\n    # Get all results at once\n    results = await crawler.arun_many(\n      urls=urls,\n      config=run_config,\n      dispatcher=dispatcher\n    )\n    # Process all results after completion\n    for result in results:\n      if result.success:\n        await process_result(result)\n      else:\n        print(f\"Failed to crawl {result.url}: {result.error_message}\")\n\n```\n\n**Review:** - **Purpose:** Executes a batch crawl with all URLs processed together after crawling is complete. - **Dispatcher:** Uses `MemoryAdaptiveDispatcher` to manage concurrency and system memory. - **Stream:** Disabled (`stream=False`), so all results are collected at once for post-processing. - **Best Use Case:** When you need to analyze results in bulk rather than individually during the crawl.\n### 4.2 Streaming Mode\n```\nasync def crawl_streaming():\n  browser_config = BrowserConfig(headless=True, verbose=False)\n  run_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    stream=True # Enable streaming mode\n  )\n  dispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=70.0,\n    check_interval=1.0,\n    max_session_permit=10,\n    monitor=CrawlerMonitor(\n      display_mode=DisplayMode.DETAILED\n    )\n  )\n  async with AsyncWebCrawler(config=browser_config) as crawler:\n    # Process results as they become available\n    async for result in await crawler.arun_many(\n      urls=urls,\n      config=run_config,\n      dispatcher=dispatcher\n    ):\n      if result.success:\n        # Process each result immediately\n        await process_result(result)\n      else:\n        print(f\"Failed to crawl {result.url}: {result.error_message}\")\n\n```\n\n**Review:** - **Purpose:** Enables streaming to process results as soon as theyâ€™re available. - **Dispatcher:** Uses `MemoryAdaptiveDispatcher` for concurrency and memory management. - **Stream:** Enabled (`stream=True`), allowing real-time processing during crawling. - **Best Use Case:** When you need to act on results immediately, such as for real-time analytics or progressive data storage.\n### 4.3 Semaphore-based Crawling\n```\nasync def crawl_with_semaphore(urls):\n  browser_config = BrowserConfig(headless=True, verbose=False)\n  run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n  dispatcher = SemaphoreDispatcher(\n    semaphore_count=5,\n    rate_limiter=RateLimiter(\n      base_delay=(0.5, 1.0),\n      max_delay=10.0\n    ),\n    monitor=CrawlerMonitor(\n      max_visible_rows=15,\n      display_mode=DisplayMode.DETAILED\n    )\n  )\n  async with AsyncWebCrawler(config=browser_config) as crawler:\n    results = await crawler.arun_many(\n      urls, \n      config=run_config,\n      dispatcher=dispatcher\n    )\n    return results\n\n```\n\n**Review:** - **Purpose:** Uses `SemaphoreDispatcher` to limit concurrency with a fixed number of slots. - **Dispatcher:** Configured with a semaphore to control parallel crawling tasks. - **Rate Limiter:** Prevents servers from being overwhelmed by pacing requests. - **Best Use Case:** When you want precise control over the number of concurrent requests, independent of system memory.\n### 4.4 Robots.txt Consideration\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nasync def main():\n  urls = [\n    \"https://example1.com\",\n    \"https://example2.com\",\n    \"https://example3.com\"\n  ]\n  config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    check_robots_txt=True, # Will respect robots.txt for each URL\n    semaphore_count=3   # Max concurrent requests\n  )\n  async with AsyncWebCrawler() as crawler:\n    async for result in crawler.arun_many(urls, config=config):\n      if result.success:\n        print(f\"Successfully crawled {result.url}\")\n      elif result.status_code == 403 and \"robots.txt\" in result.error_message:\n        print(f\"Skipped {result.url} - blocked by robots.txt\")\n      else:\n        print(f\"Failed to crawl {result.url}: {result.error_message}\")\nif __name__ == \"__main__\":\n  asyncio.run(main())\n\n```\n\n**Review:** - **Purpose:** Ensures compliance with `robots.txt` rules for ethical and legal web crawling. - **Configuration:** Set `check_robots_txt=True` to validate each URL against `robots.txt` before crawling. - **Dispatcher:** Handles requests with concurrency limits (`semaphore_count=3`). - **Best Use Case:** When crawling websites that strictly enforce robots.txt policies or for responsible crawling practices.\n## 5. Dispatch Results\nEach crawl result includes dispatch information:\n```\n@dataclass\nclass DispatchResult:\n  task_id: str\n  memory_usage: float\n  peak_memory: float\n  start_time: datetime\n  end_time: datetime\n  error_message: str = \"\"\n\n```\n\nAccess via `result.dispatch_result`:\n```\nfor result in results:\n  if result.success:\n    dr = result.dispatch_result\n    print(f\"URL: {result.url}\")\n    print(f\"Memory: {dr.memory_usage:.1f}MB\")\n    print(f\"Duration: {dr.end_time - dr.start_time}\")\n\n```\n\n## 6. Summary\n1. **Two Dispatcher Types** :\n  * MemoryAdaptiveDispatcher (default): Dynamic concurrency based on memory\n  * SemaphoreDispatcher: Fixed concurrency limit\n\n\n2. **Optional Components** :\n  * RateLimiter: Smart request pacing and backoff\n  * CrawlerMonitor: Real-time progress visualization\n\n\n3. **Key Benefits** :\n  * Automatic memory management\n  * Built-in rate limiting\n  * Live progress monitoring\n  * Flexible concurrency control\n\n\nChoose the dispatcher that best fits your needs:\n  * **MemoryAdaptiveDispatcher** : For large crawls or limited resources\n  * **SemaphoreDispatcher** : For simple, fixed-concurrency scenarios\n\n\n##### Search\nxClose\nType to start searching\n",
  "links": [
    "https://docs.crawl4ai.com",
    "https://docs.crawl4ai.com/advanced-features",
    "https://docs.crawl4ai.com/api/arun",
    "https://docs.crawl4ai.com/api/arun_many",
    "https://docs.crawl4ai.com/api/async-webcrawler",
    "https://docs.crawl4ai.com/api/crawl-result",
    "https://docs.crawl4ai.com/api/parameters",
    "https://docs.crawl4ai.com/api/strategies",
    "https://docs.crawl4ai.com/blog",
    "https://docs.crawl4ai.com/core/browser-crawler-config",
    "https://docs.crawl4ai.com/core/cache-modes",
    "https://docs.crawl4ai.com/core/content-selection",
    "https://docs.crawl4ai.com/core/crawler-result",
    "https://docs.crawl4ai.com/core/docker-deploymeny",
    "https://docs.crawl4ai.com/core/fit-markdown",
    "https://docs.crawl4ai.com/core/installation",
    "https://docs.crawl4ai.com/core/link-media",
    "https://docs.crawl4ai.com/core/local-files",
    "https://docs.crawl4ai.com/core/markdown-generation",
    "https://docs.crawl4ai.com/core/page-interaction",
    "https://docs.crawl4ai.com/core/quickstart",
    "https://docs.crawl4ai.com/core/simple-crawling",
    "https://docs.crawl4ai.com/crawl-dispatcher",
    "https://docs.crawl4ai.com/extraction/chunking",
    "https://docs.crawl4ai.com/extraction/clustring-strategies",
    "https://docs.crawl4ai.com/extraction/llm-strategies",
    "https://docs.crawl4ai.com/extraction/no-llm-strategies",
    "https://docs.crawl4ai.com/file-downloading",
    "https://docs.crawl4ai.com/hooks-auth",
    "https://docs.crawl4ai.com/identity-based-crawling",
    "https://docs.crawl4ai.com/lazy-loading",
    "https://docs.crawl4ai.com/proxy-security",
    "https://docs.crawl4ai.com/session-management",
    "https://docs.crawl4ai.com/ssl-certificate"
  ],
  "depth": 1,
  "stats": {
    "processed": 3,
    "total": 0,
    "depth": 1,
    "elapsed": "0:00:04",
    "page_limit": 34
  }
}